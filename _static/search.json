[
    {
        "objectID": "abstractions/app-interface",
        "href": "abstractions/app-interface.html#app-interface",
        "title": "App interface",
        "section": "App interface",
        "text": "Many Ansys apps are designed around user interaction within a\ndesktop GUI-based environment. Consequently, scripts are recorded\ndirectly from user sessions and are in the context of manipulating the\ndesktop app. Instead, scripts should be written for an API\nthat is structured around data represented as classes and methods.\nPyAnsys seeks to make the API a “first class citizen” in regard to\ninteracting with an Ansys product by presenting the product as a\nstateful data model. Consider the following comparison between using a\nrecorded script from AEDT versus using the PyAEDT library to create an\nopen region in the active editor:\nUsing a Recorded Script from AEDT (MS COM Methods)\nUsing the PyAEDT Library\nBesides length and readability, the biggest difference between the two\napproaches is how the methods and attributes from the Hfss class\nare encapsulated. For example, AEDT no longer needs to be\nexplicitly instantiated and is hidden as a protected attribute\n_desktop. The connection to the app takes place\nautomatically when Hfss is instantiated, and the active AEDT\nproject, editor, and module are automatically used to create the\nopen region.\nFurthermore, the create_open_region method within the Hfss\nclass contains a full Python documentation string with keyword arguments,\nclear numpydoc parameters and returns, and a basic example.\nThese are unavailable when directly using COM methods, preventing\nthe use of contextual help from within a Python IDE.\nThe source of the hfss.py method within PyAEDT follows.\nNote how calls to the COM object are all encapsulated\nwithin this method.\nHere, the COM CreateOpenRegion method is abstracted, encapsulating\nthe model setup object. There’s no reason why a user needs direct\naccess to _omodelsetup, which is why it’s protected in the\nHfss class. Additionally, calling the method is simplified by\nproviding (and documenting) the defaults using keyword arguments and\nplacing them into the vars list, all while following the Style\nGuide for Python Code (PEP8).\nApp interface\nHfss\n_desktop\nHfss\ncreate_open_region\nHfss\nnumpydoc\nhfss.py\nCreateOpenRegion\n_omodelsetup\nHfss\nvars"
    },
    {
        "objectID": "content-writing/content-how-tos/add-sphinx-extensions",
        "href": "content-writing/content-how-tos/add-sphinx-extensions.html#add-sphinx-extensions",
        "title": "Add Sphinx extensions",
        "section": "Add Sphinx extensions",
        "text": "The Sphinx configuration (doc/source/conf.py) file contains an extensions\nvariable that specifies the list of extensions that are configured for use by\nSphinx when generating documentation. When the Ansys templates\ntool is used to create a PyAnsys project from the pyansys or pyansys-advanced\ntemplate, the extensions variable lists these extensions by default:\nExtensions with names beginning with sphinx_ext are native (built-in) and are\navailable for Sphinx’s use without any additional installation. Extensions with names\nthat do not begin with sphinx_ext are external extensions and require installation.\nIf a non-native extension is not installed but added to the conf.py file in the\ndoc/source directory, attempts to build the documentation fail because Sphinx cannot\nfind the needed extension.\nAdd Sphinx extensions\ndoc/source/conf.py\nextensions\npyansys\npyansys-advanced\nextensions\nsphinx_ext\nsphinx_ext\nconf.py\ndoc/source"
    },
    {
        "objectID": "content-writing/content-how-tos/add-sphinx-extensions",
        "href": "content-writing/content-how-tos/add-sphinx-extensions.html#link-to-python-objects-in-other-sphinx-documentation",
        "title": "Add Sphinx extensions > Link to Python objects in other Sphinx documentation",
        "section": "Link to Python objects in other Sphinx documentation",
        "text": "The sphinx.ext.intersphinx extension is configured by default in your project’s\nconf.py file so that you can link to Python objects in other Sphinx documentation.\nThe intersphinx_mapping variable specifies the URIs for the Sphinx documentation with\nthe objects that you want to link to.\nFor example, this intersphinx_mapping variable provides mappings to the Sphinx documentation\nfor several other projects:\nTo be able to link to a Python object in other Sphinx documentation, the object must be part\nof that documentation’s inventory (object.inv) file. You map the target (base URI of\nthe external documentation or the local path) to the documentation’s object.inv\nfile. The keyword None indicates that this file is found at the same location. If it is not,\nyou must supply the appropriate URI.\nGiven that a target for a Python object exists in the Sphinx documentation that you want to\nlink to, you can link to it using the same roles that you use to link to a Python object in\nyour documentation. Here are some examples:\nSciPy odeint() function:\n:func:`odeint() <scipy.integrate.odeint>` odeint()\n:func:`scipy.integrate.odeint` scipy.integrate.odeint()\nSciPy quad() function:\n:func:`quad() <scipy.integrate.quad>` quad()\n:func:`scipy.integrate.quad` scipy.integrate.quad()\nMatplotlib hist() method:\n:meth:`hist() <matplotlib.axes.Axes.hist>` hist()\n:meth:`matplotlib.axes.Axes.hist` matplotlib.axes.Axes.hist()\nNumPy module:\n:mod:`numpy <numpy>` numpy\n:mod:`numpy` numpy\nNumPy matrix class:\n:class:`matrix <numpy.matrix>` matrix\n:class:`numpy.matrix.` numpy.matrix\nNumPy matrix attribute shape:\n:attr:`shape() <numpy.ndarray.shape>` shape()\n:attr:`numpy.ndarray.shape` numpy.ndarray.shape\nFor more information, see API_object_links.\nLink to Python objects in other Sphinx documentation\nsphinx.ext.intersphinx\nconf.py\nintersphinx_mapping\nintersphinx_mapping\nobject.inv\nobject.inv\nNone\nodeint()\n:func:`odeint() <scipy.integrate.odeint>` odeint()\n:func:`scipy.integrate.odeint` scipy.integrate.odeint()\nquad()\n:func:`quad() <scipy.integrate.quad>` quad()\n:func:`scipy.integrate.quad` scipy.integrate.quad()\nhist()\n:meth:`hist() <matplotlib.axes.Axes.hist>` hist()\n:meth:`matplotlib.axes.Axes.hist` matplotlib.axes.Axes.hist()\n:mod:`numpy <numpy>` numpy\n:mod:`numpy` numpy\nmatrix\n:class:`matrix <numpy.matrix>` matrix\n:class:`numpy.matrix.` numpy.matrix\n:attr:`shape() <numpy.ndarray.shape>` shape()\n:attr:`numpy.ndarray.shape` numpy.ndarray.shape"
    },
    {
        "objectID": "content-writing/content-how-tos/add-sphinx-extensions",
        "href": "content-writing/content-how-tos/add-sphinx-extensions.html#add-a-native-extension",
        "title": "Add Sphinx extensions > Add a native extension",
        "section": "Add a native extension",
        "text": "To use the special features provided by a native Sphinx extension, you only need to add\nthe extension to the extensions variable in the project’s conf.py\nfile.\nFor example, the native sphinx.ext.todo extension was added to the extensions\nvariable in the conf.py file for this guide. This extension supports\nuse of the todo directive to create a specially formatted block of text for\na task that must still be done. The blocks for todo directives do not render\nin the documentation by default. However, to render them in the documentation, you\ncan add a todo_include_todos variable to the conf.py file and then set\nthis variable to True.\nAdd a native extension\nextensions\nconf.py\nsphinx.ext.todo\nextensions\nconf.py\ntodo\ntodo\ntodo_include_todos\nconf.py\nTrue"
    },
    {
        "objectID": "content-writing/content-how-tos/add-sphinx-extensions",
        "href": "content-writing/content-how-tos/add-sphinx-extensions.html#add-an-external-extension",
        "title": "Add Sphinx extensions > Add an external extension",
        "section": "Add an external extension",
        "text": "To use the special features provided by an external Sphinx extension is a bit\nmore complicated. You must install the extension in your development environment and then\nadd it to both the project’s conf.py and its list of documentation requirements.\nFor example, to use cards and tab sets in your documentation, you must install and configure\nthe external sphinx-design extension for use:\nIf the Ansys Python Manager and Administrator window are not still\nopen, open them.\nFrom the Administrator window’s command prompt, run the command\nfor installing the external extension in your development environment:\nExamples follow for some of the external extensions mentioned in this\ndocumentation.\nTo install the external sphinx-design extension, run this command:\nTo install the external sphinx_toolbox.collapse extension, run this command:\nAdd the external extension to the extensions variable in your project’s\nconf.py file.\nAdd the external extension to your project’s documentation requirements as indicated\nin the next topic.\nAdd an external extension\nconf.py\nsphinx-design\nsphinx_toolbox.collapse\nextensions\nconf.py"
    },
    {
        "objectID": "content-writing/content-how-tos/add-sphinx-extensions",
        "href": "content-writing/content-how-tos/add-sphinx-extensions.html#add-the-extension-to-the-documentation-requirements",
        "title": "Add Sphinx extensions > Add the extension to the documentation requirements",
        "section": "Add the extension to the documentation requirements",
        "text": "Documentation requirements list the pip packages that Sphinx requires for\nbuilding the documentation. Depending on the project’s configuration, you list these\npackages in either the pyproject.toml file or the requirements_doc_txt\nfile.\nAdd the extension to the documentation requirements\npip\npyproject.toml\nrequirements_doc_txt"
    },
    {
        "objectID": "content-writing/content-how-tos/add-sphinx-extensions",
        "href": "content-writing/content-how-tos/add-sphinx-extensions.html#the-pyprojecttoml-file",
        "title": "Add Sphinx extensions > The pyproject.toml file",
        "section": "The pyproject.toml file",
        "text": "Most projects specify documentation requirements in a pyproject.toml file, which\nresides in the root folder. In this file, the doc variable defines the required pip\npackages and their versions like this.\nThe pyproject.toml file\npyproject.toml\npyproject.toml\ndoc\npip"
    },
    {
        "objectID": "content-writing/content-how-tos/add-sphinx-extensions",
        "href": "content-writing/content-how-tos/add-sphinx-extensions.html#the-requirements_doc_txt-file",
        "title": "Add Sphinx extensions > The requirements_doc_txt file",
        "section": "The requirements_doc_txt file",
        "text": "Some projects specify documentation requirements in a requirements_doc_txt\nfile. The root folder of such a project typically has a requirements directory\nthat contains this TXT file, which defines the required pip packages and their\nversions like this:\nThe requirements_doc_txt file\nrequirements_doc_txt\nrequirements_doc_txt\nrequirements\npip"
    },
    {
        "objectID": "content-writing/content-how-tos/add-sphinx-extensions",
        "href": "content-writing/content-how-tos/add-sphinx-extensions.html#learn-more-about-extensions",
        "title": "Add Sphinx extensions > Learn more about extensions",
        "section": "Learn more about extensions",
        "text": "As you can see, PyAnsys projects add many extensions to their conf.py files\nand documentation requirements. Here are some other native and non-native extensions\nthat you might see:\nsphinx.ext.coverage\nsphinx.ext.doctest\nsphinx.ext.extlinks\nsphinx.ext.graphviz\nsphinx.ext.napoleon\nsphinx.ext.viewcode\nsphinx_toolbox.collapse\nFor more information on extensions, see Extensions in the\nSphinx documentation. In addition to the external (third-party) extensions collected\nin the sphinx-contrib organization, you can search the internet\nto find other Sphinx extensions or learn more about the ones in the preceding list.\nLearn more about extensions\nconf.py\nsphinx.ext.coverage\nsphinx.ext.doctest\nsphinx.ext.extlinks\nsphinx.ext.graphviz\nsphinx.ext.napoleon\nsphinx.ext.viewcode\nsphinx_toolbox.collapse"
    },
    {
        "objectID": "sg_execution_times",
        "href": "sg_execution_times.html#computation-times",
        "title": "Computation times",
        "section": "Computation times",
        "text": "00:01.596 total execution time for 1 file from all galleries:\nExample\nTime\nMem (MB)\nsphx_glr_examples_pyvista_example.py (../../examples/pyvista_example.py)\n00:01.596\n0.0\nComputation times\n../../examples/pyvista_example.py"
    },
    {
        "objectID": "content-writing/rst-files-writers/collapsible-sections",
        "href": "content-writing/rst-files-writers/collapsible-sections.html#collapsible-sections",
        "title": "Collapsible sections",
        "section": "Collapsible sections",
        "text": "When information is applicable only to a subset of your users or provides\nmore comprehensive information than most users need, you can use collapsible\nsections. Then, only users who either need or want to view this information\ncan expand these sections.\nTo use collapsible sections in your PyAnsys documentation, you must install\nthe sphinx_toolbox.collapse extension and then add it to the conf.py\nfile in the doc/source directory and to your list of documentation requirements.\nFor more information, see add_sphinx_extensions.\nTo see and use the collapsible sections that are shown only as an image on this page,\nsee artifact_download. To see how the collapsible sections on this\npage are formatted, click the Show Source link in the page’s right pane.\nAs described in rst_file_formatting, you can copy content from the\nTXT version of this file and then paste it directly into one of your RST files\nfor reuse, modifying it as needed.\nHere is an image of the three collapsible sections. Clicking a collapsible section expands it\nso that additional information is shown.\nCollapsible sections\nsphinx_toolbox.collapse\nconf.py\ndoc/source"
    },
    {
        "objectID": "content-writing/rst-files-writers/cards",
        "href": "content-writing/rst-files-writers/cards.html#cards",
        "title": "Cards",
        "section": "Cards",
        "text": "Cards are special content blocks or components that visually stand out from\nregular text. You use cards to emphasize or showcase specific pieces of\ninformation. While notices are simple cards that you use\nadmonition directives to create, you can manually create more sophisticated\ncustom cards.\nTo use custom cards in your PyAnsys documentation, you must install\nthe sphinx-design extension and then\nadd it to the conf.py file in the doc/source directory and to\nyour list of documentation requirements. For more information, see\nadd_sphinx_extensions.\nTo see and use the cards that are shown only as images on this page,\nclick the links to their respective documentation pages. To see how these\ncards are formatted, click the Show Source link in the documentation page’s\nright pane. As described in rst_file_formatting, you can copy content\nfrom the TXT version of this file and then paste it directly into one of your\nRST files for reuse, modifying it as needed.\nHere is an image of the cards on the documentation landing page for this guide.\nThese cards describe and link to various sections of this guide.\nHere is an image of cards on the Docker containers\npage in the PyAnsys Geometry documentation. These cards link to instructions for building\neither a Windows Docker container or a Linux Docker container for the Ansys Geometry service.\nYou only need to click the card for your operating system. This page and the other pages\nin the PyAnsys Geometry Getting Started section also have a Go to Getting started\nbutton for returning to this section’s first page.\nCards\nconf.py\ndoc/source"
    },
    {
        "objectID": "content-writing/rst-files-writers/rst-file-formatting",
        "href": "content-writing/rst-files-writers/rst-file-formatting.html#rst-file-formatting",
        "title": "RST file formatting",
        "section": "RST file formatting",
        "text": "The easiest and best way to understand how to format a RST file is to use the Show Source\nlink in the right navigation pane of a PyAnsys documentation page. Clicking this link\ndisplays a text (TXT) version of the page’s source file, where you can see the formatting.\nTo reuse any content, copy it from the TXT version and paste it directly into one of your RST files,\nmodifying it as necessary.\nYou do not want to copy content from the TXT version of an API documentation page\nbecause this content is not manually authored in a RST file but rather automatically\ngenerated from a PY file.\nTo view the TXT version for this documentation page, click the Show Source link.\nWhen you finish viewing the TXT version, click your browser’s back arrow to return to\nthe documentation page.\nIf you want to view the TXT version of a richly formatted page in the PyFluent documentation,\nread through and then perform these steps:\nGo to the Frequently asked questions page in the PyFluent documentation.\nIn the right navigation pane, click the Show Source link to open the TXT version.\nView the formatting for this page.\nWhen finished, click your browser’s back arrow to return to the PyFluent documentation page.\nClick your browser’s back arrow once again to return to the developer guide’s documentation page.\nRST file formatting"
    },
    {
        "objectID": "content-writing/rst-files-writers/doc-links",
        "href": "content-writing/rst-files-writers/doc-links.html#links",
        "title": "Links",
        "section": "Links",
        "text": "A link consists of some explicit markup with a named target that is either\ninternal or external to your project. You can also create a link for downloading\na file or for going to the description of a Python object in your API reference\ndocumentation.\nLinks"
    },
    {
        "objectID": "content-writing/rst-files-writers/doc-links",
        "href": "content-writing/rst-files-writers/doc-links.html#internal-links",
        "title": "Links > Internal links",
        "section": "Internal links",
        "text": "You use the ref role to link to a target that you place before a heading.\nThis allows Sphinx to use the heading as the display text for the link, thereby\nreducing maintenance if the heading later changes.\nYou first insert the target in the RST file before the heading that you want\nto link to. The target consists of these parts:\nAn explicit markup start (..) followed by a space\nAn underscore followed by the target name and a colon\nAlways insert blanks lines both before and after the target.\nHere is the target for Ansys_Python_Manager in this guide:\nTo link to this target from any RST or PY file in this guide, use the\nref role and the target name within a sentence, surrounding the target name\nin single backticks:\nIn the documentation, the display text for the link is the heading that follows this target.\nIf the target is not placed before a heading, or if you want to customize the display text,\nyou must provide the display text:\nInternal links\nref\n..\nref"
    },
    {
        "objectID": "content-writing/rst-files-writers/doc-links",
        "href": "content-writing/rst-files-writers/doc-links.html#external-links",
        "title": "Links > External links",
        "section": "External links",
        "text": "There are several methods that you can use to link to external targets. The\nsimplest way is to specify the display text for the link and a URL directly in\na sentence:\nThe display text for a link should either exactly match the title or heading\nthat you are referencing or provide a description of the destination page, as\nspecified in Link text in the\nGoogle developer documentation style guide. As shown in the example sentence,\nyou should follow a link with a descriptor that indicates where the\nlinked content resides, such as in a product’s documentation or on an\norganization’s website.\nThe problem with specifying a URL as a target is that if the URL changes or the\npage is later removed, you must update the URL in every place where it is used. This can\nbecome quite burdensome if you have many links to the same URL.\nTo reduce maintenance of external links, you can use one of these methods to link to\nnamed targets:\nInsert named targets at the bottom of individual RST files. For more information,\nsee named_targets_bottom_rst_files.\nInsert named targets in a central list file, which is usually the list.rst\nfile, in the project’s doc/source directory. For more information, see\nnamed_targets_link_rst_file.\nYou should choose only one of these methods. If you mix them, you might use\nthe same target name in both an individual RST file and the list.rst file,\nwhich causes Sphinx to raises errors about duplicated targets when building the\ndocumentation.\nExternal links\nlist.rst\ndoc/source\nlist.rst"
    },
    {
        "objectID": "content-writing/rst-files-writers/doc-links",
        "href": "content-writing/rst-files-writers/doc-links.html#guidelines-for-target-names",
        "title": "Links > Guidelines for target names",
        "section": "Guidelines for target names",
        "text": "When naming targets, you must adhere to these guidelines to avoid errors\nthat Sphinx might otherwise raise when building the documentation:\nDo not use spaces in target names. Instead, use underscores to separate words.\nFor example, _numpy is the name of the target that links\nto the NumPy website.\nAbbreviate target names however you want. Keep in mind that longer, more\ndescriptive names are easier to find if you ever need to replace them.\nFor example, _python_main is a better target name for the Python organization\nthan _python.\nDo not use the same names for both internal and external targets. Varying how the\nnames are capitalized does not cause Sphinx to see them as unique. For example, having an\ninternal target named _RST_files and an external target named _rst_files causes\nSphinx to raise errors when building the documentation because it sees them as\nduplicate targets.\nGuidelines for target names\n_numpy\n_python_main\n_python\n_RST_files\n_rst_files"
    },
    {
        "objectID": "content-writing/rst-files-writers/doc-links",
        "href": "content-writing/rst-files-writers/doc-links.html#named-targets-at-the-bottom-of-rst-files",
        "title": "Links > Named targets at the bottom of RST files",
        "section": "Named targets at the bottom of RST files",
        "text": "Assume that you want to add links in a RST file to the websites for NumPy, SciPy,\nand pandas, the  “big three” data science packages. At the bottom of\nthe RST file, you insert named targets to these websites like this:\nTo insert links to these targets in a sentence in the RST file, you surround\neach target name in single backticks and follow it with an underscore:\nWhen using this named target method, you must add targets to the bottom of every RST file\nwhere you want to insert links to these targets, which requires much more effort and\nmaintenance than if you insert all targets in a central list.rst file.\nNamed targets at the bottom of RST files\nlist.rst"
    },
    {
        "objectID": "content-writing/rst-files-writers/doc-links",
        "href": "content-writing/rst-files-writers/doc-links.html#named-targets-in-a-listrst-file",
        "title": "Links > Named targets in a list.rst file",
        "section": "Named targets in a list.rst file",
        "text": "You can add a central RST file in your project’s doc/source directory and have the\nSphinx configuration file read this file to load all named targets, allowing you to insert\nlinks to any of these targets in any RST file. Consolidating all targets in one file,\nwhich is usually the list.rst file, can significantly reduce maintenance over\ntime.\nNamed targets in a list.rst file\nlist.rst\ndoc/source\nlist.rst"
    },
    {
        "objectID": "content-writing/rst-files-writers/doc-links",
        "href": "content-writing/rst-files-writers/doc-links.html#set-up-the-sphinx-configuration-file",
        "title": "Links > Set up the Sphinx configuration file",
        "section": "Set up the Sphinx configuration file",
        "text": "To use a list.rst file, you must set it up in the Sphinx configuration file\n(doc/source/conf.py):\nOpen the conf.py file.\nSearch to see if the exclude_patterns variable is defined.\nIf this variable is defined, add this line to the list of files and\ndirectories that Sphinx is to ignore  when looking for source files:\nIf this variable is not defined, after the lines configuring\nnumpydoc and enabling numpydoc validation, define this variable and add the\nlinks.rst file:\nIf you do not add the links.rst file to the exclude_patterns variable,\nSphinx raises this warning during documentation generation: document isn't included in any toctree.\nBeneath the exclude_patterns variable, add these lines to enable the rst_epilog\nreStructuredText string:\nYou might want to look at the config.py file for PyMAPDL because\nthe exclude_patterns variable for this project lists many files and directories for\nSphinx to ignore when looking for source files. Additionally, the rst_epilog\nstring contains a replace step that look for a VERSION variable\nto easily update targets with the MAPDL version for the latest MAPDL release.\nWhen building the documentation, Sphinx dynamically includes the content in the rst_epilog\nstring at the bottom of every RST file. Because this string tells Sphinx to read all targets\nin the list.rst file, you are able to link to any of these targets from any RST or PY\nfile in the project.\nYou can use another name for the list.rst file or use multiple RST files for organizing\nexternal links as long as you make the appropriate changes to the lines enabling the\nrst_epilog string.\nWhen documentation checks run on a PR, Sphinx might indicate some links or link anchors are broken,\nwhich results in errors. For more information on resolving these errors, see\nresolve_too_long_lines_broken_links.\nSet up the Sphinx configuration file\nlist.rst\ndoc/source/conf.py\nconf.py\nexclude_patterns\nnumpydoc\nnumpydoc\nlinks.rst\nlinks.rst\nexclude_patterns\ndocument isn't included in any toctree\nexclude_patterns\nrst_epilog\nexclude_patterns\nrst_epilog\nreplace\nVERSION\nrst_epilog\nlist.rst\nlist.rst\nrst_epilog"
    },
    {
        "objectID": "content-writing/rst-files-writers/doc-links",
        "href": "content-writing/rst-files-writers/doc-links.html#link-to-named-targets-in-the-listrst-file",
        "title": "Links > Link to named targets in the list.rst file",
        "section": "Link to named targets in the list.rst file",
        "text": "To link to a named target in the list.rst file, you drop the leading underscore in the\ntarget name and add a trailing underscore after the target name. For example, the list.rst\nfile for this project includes a target named _Style_guide_Sphinx_doc. To link to this target\nfrom any RST or PY file in this guide, here is how you insert the link:\nWhile the link still begins with a single backtick and the display text for the link,\ncarats surround the target name and an underscore. Following the closing carat, you must\nhave a closing single backtick and another underscore.\nFor examples of named targets and how to organize them by categories, see the\nlist.rst file in the doc/source directory for this project or the\nPyMAPDL project.\nLink to named targets in the list.rst file\nlist.rst\nlist.rst\nlist.rst\n_Style_guide_Sphinx_doc\nlist.rst\ndoc/source"
    },
    {
        "objectID": "content-writing/rst-files-writers/doc-links",
        "href": "content-writing/rst-files-writers/doc-links.html#download-links",
        "title": "Links > Download links",
        "section": "Download links",
        "text": "To create a link that downloads a file, you can use either a download link that specifies\na URL for the file or the download role and a named target in the list.rst file.\nThis sentence uses a download link that specifies the URL to download the PDF file for the\nPyFluent cheat sheet:\nThis next sentence uses the download role and a named target in the list.rst file\nfor the PyMAPDL project to download the PDF file for the PyMAPDL cheat sheet:\nDownload links\ndownload\ndownload\nlist.rst\ndownload\ndownload\nlist.rst"
    },
    {
        "objectID": "content-writing/rst-files-writers/doc-links",
        "href": "content-writing/rst-files-writers/doc-links.html#python-object-links",
        "title": "Links > Python object links",
        "section": "Python object links",
        "text": "To link to Python objects in your API reference documentation, you use Python-specific roles.\nFor a list of these roles, see Cross-referencing Python objects in\nthe Sphinx documentation. For descriptions of fundamental Python objects, see py_file_format.\nIf the role links to a Python object in the same module, you only need to use the object name\nin the role (as shown in the first of the following three examples). If the role links to a\nPython object in a different module, you must use the module name and object name in the role\n(as shown in the second and third of the following three examples).\nPython uses a period (.) to denote submodules. If you need to see where a Python object is\ndefined in your API, use the GitHub search function. For example, to see where the\nPrimitives3DLayout class is defined in the PyAEDT API, search its repository for this string:\nclass Primitives3DLayout\nSearch results indicate that this class is defined here: pyaedt.modeler.Primitives3DLayout.Primitives3DLayout.\nBecause using the full module name and object name in the role always works, when you perform a\nsearch to see where a Python object is defined, consider collecting the result in a TXT file. When\nyou next need to link to this object, you can easily search this text file for the object name\nand then copy its full module name and object name into your RST file.\nPython object links\n.\nPrimitives3DLayout\nclass Primitives3DLayout\npyaedt.modeler.Primitives3DLayout.Primitives3DLayout"
    },
    {
        "objectID": "content-writing/rst-files-writers/doc-links",
        "href": "content-writing/rst-files-writers/doc-links.html#examples-of-python-object-links",
        "title": "Links > Examples of Python object links",
        "section": "Examples of Python object links",
        "text": "Here are some examples of using Python-specific roles to link to Python objects.\nExample 1\nAssuming that your project is PyAEDT, you can use the class role to link to the\nDesktop class in the PyAEDT API reference documentation:\nExample 2\nAssuming that your project is PyMAPDL, you can use the func role to link to the\nrun_batch() function in the PyMAPDL API reference documentation:\nExample 3\nAlso assuming that your project is PyMAPDL, you can use both the func and\nattr roles to link to the nodal_displacement() function and then the\nselected_nodes attribute in the PyMAPDL API reference documentation:\nExamples of Python object links\nclass\nDesktop\nfunc\nrun_batch()\nfunc\nattr\nnodal_displacement()\nselected_nodes"
    },
    {
        "objectID": "content-writing/rst-files-writers/doc-links",
        "href": "content-writing/rst-files-writers/doc-links.html#more-examples-of-python-object-links",
        "title": "Links > More examples of Python object links",
        "section": "More examples of Python object links",
        "text": "To see more examples of how to use Python-specific roles to link to Python objects\nin your API reference documentation, use the GitHub search feature to find the following\nstrings in the repositories for PyAnsys libraries, keeping in mind that only some subset\nof these roles is likely used in any library:\nmod\nfunc\ndata\nconst\nclass\nmeth\nattr\nexc\nobj\nTo learn how you can also use Python-specific roles to link to Python objects in the\nSphinx documentation for other projects, see links_to_objects_in_other_doc.\nMore examples of Python object links\nmod\nfunc\ndata\nconst\nclass\nmeth\nattr\nexc\nobj"
    },
    {
        "objectID": "content-writing/examples-writers/index",
        "href": "content-writing/examples-writers/index.html#content-in-examples",
        "title": "Content in examples",
        "section": "Content in examples",
        "text": "Many PyAnsys libraries have an “Example” section. The Sphinx-Gallery\nextension (sphinx_gallery) is typically used to generate standalone, downloadable Python scripts\n(PY files) or Jupyter notebooks (IPYNB files) that you can run. However, some\nPyAnsys libraries use the nbsphinx extension,\nwhich is specifically designed for working with and integrating Jupyter notebooks\ninto your Sphinx documentation.\nWhen setting up a PyAnsys library, developers choose between Sphinx-Gallery or nbsphinx\nbased on their specific documentation needs and the format of their code examples.\nSphinx-Gallery is chosen if a gallery of examples is to be created from Python scripts and Jupyter\nnotebooks. This extension automatically generates thumbnail images and links to the source code\nfor each example. It also provides a number of configuration options for customizing the\nappearance of the gallery, providing a great way to showcase how to use your library\nin a structured manner with embedded code examples, explanations, and narrative text.\nThe nbsphinx extension is chosen if developers want to integrate Jupyter notebooks into their Sphinx documentation. This\nextension processes Jupyter notebooks as inputs and offers custom Sphinx directives for rendering\nthese notebooks as interactive HTML pages or LaTeX output. Within these formats, you can execute\nand modify code cells. Furthermore, nbsphinx can execute unevaluated notebooks automatically\nduring the Sphinx build process. While this extension allows for some customization on the\nappearance of Jupyter notebooks in your Sphinx documentation, its scope is narrower than that\nof Sphinx Gallery.\nFor implementation information, see sphinx-gallery and nbsphinx.\nContent in examples\nsphinx_gallery\nnbsphinx\nnbsphinx\nnbsphinx"
    },
    {
        "objectID": "content-writing/content-contrib-setup/client-libraries",
        "href": "content-writing/content-contrib-setup/client-libraries.html#client-libraries",
        "title": "Client libraries",
        "section": "Client libraries",
        "text": "A client library is a collection of code specific to one programming language\nthat makes it easier to write scripts to access APIs programmatically. A\nclient library provides support for underlying services, including connection\nmanagement, asynchronous request processing, and exception handling. Its\nhigh-level API abstractions are easier to understand and can be more\neasily integrated with your codebase.\nPyAnsys client libraries are collections of Python code that break apart\nlarge monolithic Ansys desktop products into subsets by features, with the\nexpectation of compatibility and reusability across the entire Ansys\nportfolio. For more information, see componentizing. In addition\nto Ansys product wrappers, the PyAnsys ecosystem provides Python tools and\nsoftware utilities.\nWhile installing a Python library depends on both the library and the\nunderlying services that it supports, most are installed in only a few steps\nfrom the Python Package Index (PyPI) using pip,\na package manager for installing Python packages:\nOpen a terminal or command prompt.\nInstall pip if it is not already installed on your system.\nIf you are using a virtual environment, activate it.\nRun the command to install the library, pip install <library-name>, where\n<library-name> is the name of the library.\nTo easily set up your PyAnsys development environment and install PyAnsys libraries\nfrom one central source, you should use the Ansys Python Manager. From this app,\nyou can install a selected Python version (which includes pip), create and\nautomatically activate a virtual environment, and install all or selected PyAnsys libraries.\nYou can also launch a console in your virtual environment to run commands from. For more\ninformation, see Ansys_Python_Manager, which explains how to set up your content\ndevelopment environment.\nClient libraries\npip\npip install <library-name>\n<library-name>\npip"
    },
    {
        "objectID": "content-writing/content-how-tos/create-PR",
        "href": "content-writing/content-how-tos/create-PR.html#create-a-pr",
        "title": "Create a PR",
        "section": "Create a PR",
        "text": "This page describes how to create a PR to update or enhance PyAnsys documentation.\nIf you have not yet cloned the repository and created a local branch to work in,\nsee clone_branch.\nCreate a PR"
    },
    {
        "objectID": "content-writing/content-how-tos/create-PR",
        "href": "content-writing/content-how-tos/create-PR.html#make-changes-in-your-local-branch",
        "title": "Create a PR > Make changes in your local branch",
        "section": "Make changes in your local branch",
        "text": "In your local branch, make changes to RST and PY files to improve the\nPyAnsys documentation. As you work, periodically run the tools described in the\nnext few topics to check your changes. You should also periodically build the\ndocumentation locally so that you can verify if new or changed content is\nrendered correctly.\nAs you work, remember to use your preferred Git tool to periodically commit your changes\nand pull merged changes from the remote main branch into your local branch. Because\nresolving conflicts is always painful, you want to do all that you can to minimize\nthe possible occurrence of conflicts and resolve any that do arise locally.\nMake changes in your local branch"
    },
    {
        "objectID": "content-writing/content-how-tos/create-PR",
        "href": "content-writing/content-how-tos/create-PR.html#run-pre-commit-locally",
        "title": "Create a PR > Run pre-commit locally",
        "section": "Run pre-commit locally",
        "text": "pre-commit is a tool for ensuring that all the changes that you make to\nfiles in your project successfully pass all checks run by the code style tools that are\nconfigured as part of the CI/CD process. These tools, which typically include Black,\nisort, and Flake8, analyze, format, review, and improve\ncode quality and security. For more information on the code style tools most commonly\nconfigured for use in PyAnsys projects, see code_style_tools.\nBecause you do not want the Code style check for your PyAnsys project to fail\nwhen you create or push changes to a PR, you want to periodically run pre-commit\nlocally to proactively check if your changes are compliant. This tool resolves most\ncode style issues automatically and adds any files it changes to Git’s commit list.\nTo run pre-commit locally:\nIf the Ansys Python Manager and Administrator window are not still\nopen, then open them. For more information, see Ansys_Python_Manager.\nIn the Administrator window, run this command:\npre-commit initializes the code style tools configured for your PyAnsys project\nif this is the first time that you are running it in this session. It then shows\nstatus information for the various checks.\nIn the Windows Administrator window, press the up arrow to display\nthe command for running pre-commit again and then run it to see if\nall issues have been automatically resolved.\nFor issues that pre-commit could not automatically resolve, such as those\nfound by the codespell check, it tells you where the issues are so that you\ncan manually resolve them. Here are some issues that you might have to resolve:\nAn occurrence of a spelling error or an unrecognized word\nA line of code that exceeds the maximum line length\nA failure when creating a cross reference to find the target title or caption\nA formatting error such as this one:\nYou can configure codespell to ignore words that it flags as errors in PY files by adding one\nof two arguments to the codespell hook in the pre-commit-config.yaml file in the\nrepository’s root directory.\nTo have codespell ignore words that you place in a TXT file named ignore_words.txt\nin the repositority’s root directory, add this argument to the codespell hook:\nTo have codespell accept words that you place in the accept.txt file for\nVale, which is described in the next topic, add this argument to the codespell hook:\nManually resolve any issues that pre-commit could not resolve. For more information,\nsee resolve_failing_checks.\nIn the Windows Administrator window, press the up arrow to display\nthe command for running pre-commit again and then run it.\nResolve any issues, repeating these last few steps until pre-commit\nshows that all code style checks have passed.\nRun pre-commit locally\npre-commit\npre-commit\npre-commit\npre-commit\ncodespell\ncodespell\ncodespell\npre-commit-config.yaml\ncodespell\nignore_words.txt\ncodespell\ncodespell\naccept.txt\ncodespell\npre-commit\npre-commit\npre-commit"
    },
    {
        "objectID": "content-writing/content-how-tos/create-PR",
        "href": "content-writing/content-how-tos/create-PR.html#run-vale-locally",
        "title": "Create a PR > Run Vale locally",
        "section": "Run Vale locally",
        "text": "Vale is a tool for maintaining a consistent style and voice in your\ndocumentation based on a given style guide. For PyAnsys projects, Vale\nuses rules from the Google developer documentation style guide\nto check the content in RST and MD files. In addition to checking for style guide violations,\nVale checks for other common problems, such as extra spaces, repeated words, excessive use of\njargon, sexist language, and incorrect capitalization. Vale does not check the content surrounded\nin double backticks or in code blocks.\nWhen Vale detects an issue, it displays a suggestion, warning, or error. While you can choose\nto ignore a suggestion or warning, you must resolve all errors. Otherwise, when you\ncreate or push changes to a PR, the Vale check fails, which also causes the\nDocumentation style check to fail.\nBecause you do not want these checks to fail on the PR, you want to periodically run Vale\nlocally to proactively check that your changes are compliant.\nTo run Vale locally:\nIf the Ansys Python Manager and Administrator window are not still\nopen, open them. For more information, see Ansys_Python_Manager.\nFrom the Windows Administrator window, use the cd command to go to the\nlibrary’s doc directory.\nFor example, to go to the doc directory for this guide, you might\nrun this cd command:\nTo ensure that the latest rules for the Google style guidelines are installed\nlocally, run this command:\nThe preceding command downloads the latest rules for the\nGoogle developer’s documentation style guide to the doc/styles/Google\nfolder. If the .gitignore file in your project’s root directory is not\nset up to ignore these YML style files, add the following lines to the end of\nthis file:\nThe next several steps show how to use various Vale commands to check RST and MD\nfiles in different directories.\nTo check all RST and MD files in the doc directory, run this command:\nIf Vale displays errors in build\\html\\_static\\404.rst, it is checking files generated\nby a previous local build of the documentation. Delete the html directory and\nthen run the preceding command again.\nTo check all RST and MD files in the repository, go to the root directory and\nrun this command:\nTo check all RST and MD files in only a particular directory, start the command\nwith vale followed a space and then the name of the directory.\nTo resolve errors and other issues that Vale raises, either edit files or\nadd words or phrases to the accept.txt file in doc\\styles\\config\\vocabularies\\ANSYS.\nHere is an explanation of how Vale is set up in a PyAnsys project. The doc directory\ncontains the .vale.ini configuration file and the styles directory.\nThe .vale.ini configuration file specifies that Vale is to check content\nin RST and MD files to ensure that it is compliant with the\nGoogle developer documentation style guide, along with any custom Ansys rules.\nIn the styles directory, the config/vocabularies/ANSYS directory contains accept.txt\nand reject.txt files. You can add words or phrases that Vale does not recognize\nto the accept.txt file, and you can add recognized words or phrases that you want\nVale to raise as issues to the reject.txt file.\nVale is case-aware. When adding words to the accept.txt file, use an appropriate\nregular express if an entry should be case-insensitive.\nTo make an entire word case-insensitive, use a (?i) prefix. For example,\nadd (?i)Ansys to have Vale accept “Ansys,” “ANSYS,” and “ansys.”\nTo make it acceptable for a word to start in either uppercase or lowercase, place\nboth cases of the first letter in brackets. For example, add [Dd]ocstrings.\nOccasionally Vale raises issues that are not considered errors in PyAnsys\nprojects. For more information, see work_around_Vale_issues.\nIf Vale raises a runtime error when running locally, the error indicates the\nfile where the issue exists. In all likelihood, the file has two links that use\nthe same display text to the same named target. Modifying the display text for one\nof the two links should resolve the issue.\nRun Vale locally\ncd\ndoc\ndoc\ncd\ndoc/styles/Google\n.gitignore\ndoc\nbuild\\html\\_static\\404.rst\nhtml\nvale\naccept.txt\ndoc\\styles\\config\\vocabularies\\ANSYS\ndoc\n.vale.ini\nstyles\n.vale.ini\nstyles\nconfig/vocabularies/ANSYS\naccept.txt\nreject.txt\naccept.txt\nreject.txt\naccept.txt\n(?i)\n(?i)Ansys\n[Dd]ocstrings"
    },
    {
        "objectID": "content-writing/content-how-tos/create-PR",
        "href": "content-writing/content-how-tos/create-PR.html#build-documentation-locally",
        "title": "Create a PR > Build documentation locally",
        "section": "Build documentation locally",
        "text": "Sphinx is a tool for generating documentation. While designed primarily for\ngenerating documentation for Python projects, it can be used for documenting other programming\nlanguages and projects.\nThe build process for a project’s documentation is specified in a configuration (conf.py)\nfile in the doc/source directory. This directory also contains a Makefile file and a\nmake.bat file for automating the building process. Different builders render different\ndocumentation output, such as HTML, LaTeX, or PDF.\nWhen making lots of changes, consider building the HTML documentation locally so that you can\nsee if your changes are rendered correctly. Otherwise, you can download and view the documentation\nartifacts that are built by the pipeline’s ci-build.yml file once the Documentation building\ncheck on the PR passes. For more information, see view_artifacts.\nTo build documentation locally:\nIf the Ansys Python Manager and Administrator window are not still\nopen, open them. For more information, see Ansys_Python_Manager.\nFrom the Windows Administrator window, use the cd command to go to the\nlibrary’s doc directory.\nFor example, to go to the doc directory for this guide, you might\nrun this cd command:\nRun the make command for you operating system.\nOn Windows, run make.bat html.\nOn Linux or macOS, run make html.\nThe resulting HTML files are created in the project’s doc/_build/html directory by default.\nTo view the HTML documentation, use your favorite browser to open the index.html file:\nIf a project’s CI/CD process uses tox for testing and task automation, rather than\nusing the Sphinx documentation-building method, check the integrity of the documentation\nby running this command locally:\ntox places the resulting HTML files in the project’s .tox/doc_out directory by default.\nTo view the HTML documentation, use your favorite browser to open the index.html file in\nthis directory:\nIf you would like tox to place the resulting HTML files in the project’s doc/_build/html directory,\nyou can replace the last two lines in the tox.ini file with these two lines:\nBuild documentation locally\nconf.py\ndoc/source\nMakefile\nmake.bat\nHTML\nLaTeX\nPDF\nci-build.yml\ncd\ndoc\ndoc\ncd\nmake\nmake.bat html\nmake html\ndoc/_build/html\nindex.html\ntox\n.tox/doc_out\nindex.html\ntox\ndoc/_build/html\ntox.ini"
    },
    {
        "objectID": "content-writing/content-how-tos/create-PR",
        "href": "content-writing/content-how-tos/create-PR.html#create-a-pr",
        "title": "Create a PR",
        "section": "Create a PR",
        "text": "Before you use your Git tool to push your changes to a PR, first use it to pull merged changes\nfrom the remote main branch to your local branch. If there are any conflicts, resolve\nthem in your local branch.\nTo create a PR:\nUse your Git tool to push your changes to the remote main branch.\nOn the main page of the repository, a notification indicates that a branch had recent pushes.\nClick Compare & pull request.\nThe Open a pull request window opens.\nSupply a commit message and an optional extended description.\nAssign the PR to yourself by clicking assign yourself on the right side of the window.\nIf the PR is meant to fix an issue, add Closes #issue_number to the PR description.\nClick Create pull request. Or, if you want to create a draft pull request,\nuse the dropdown to select Create draft pull request.\nA draft pull request cannot be merged until you mark it as ready for review,\nThe Ansys Review Bot can perform a review of your changes. For more information,\nsee bot_reviews.\nCreate a PR"
    },
    {
        "objectID": "content-writing/content-how-tos/create-PR",
        "href": "content-writing/content-how-tos/create-PR.html#resolve-failed-checks",
        "title": "Create a PR > Resolve failed checks",
        "section": "Resolve failed checks",
        "text": "GitHub integrates with tools that automate code and documentation style\nchecking, testing, and deployment, which makes it far easier to streamline the development\nprocess and maintain high code quality. When you create or submit changes to a PR, all checks that\nare configured in the project’s CI/CD process run. When a check fails, you must resolve the issues causing\nthe failure. For more information, see resolve_failing_checks.\nResolve failed checks"
    },
    {
        "objectID": "content-writing/content-how-tos/create-PR",
        "href": "content-writing/content-how-tos/create-PR.html#download-and-view-documentation-artifacts",
        "title": "Create a PR > Download and view documentation artifacts",
        "section": "Download and view documentation artifacts",
        "text": "If the Documentation building check on the PR completes successfully, both HTML and PDF\ndocumentation artifacts are generated:\ndocumentation-html\ndocumentation-pdf\nYou can download and unzip these artifacts to see how the documentation for this PR is\nrendered. While you generally do not need to download the PDF artifact, you should\ndownload and extract the HTML artifact so that you can confirm that the changes in the PR\nrender correctly in the documentation.\nTo download and view documentation artifacts:\nOn the repository’s Actions page, click the GitHub CI workflow run for your pull request.\nUnder Artifacts, which is at the bottom of the page, click the artifacts to download.\nAs mentioned earlier, you likely only want to download the documentation-html artifact.\nGo to your Downloads folder and use a tool like 7-Zip to extract the HTML\nartifact (and PDF artifact if you also downloaded it).\nTo view the generated HTML documentation, in the folder that you extracted this artifact’s\nfiles to, double-click the index.html file to open the HTML documentation.\nTo view the generated PDF documentation, in the folder that you extracted this artifact’s file\nto, double-click the PDF file to open it.\nThe artifacts for a PR are only available if the Documentation building check completed\nsuccessfully. If you click the GitHub CI workflow run for a PR where this check failed,\nno artifacts are shown under Artifacts. Artifacts remain available after PRs are\nmerged.\nDownload and view documentation artifacts\ndocumentation-html\ndocumentation-pdf\ndocumentation-html\nDownloads\nindex.html"
    },
    {
        "objectID": "content-writing/content-how-tos/create-PR",
        "href": "content-writing/content-how-tos/create-PR.html#tag-reviewers",
        "title": "Create a PR > Tag reviewers",
        "section": "Tag reviewers",
        "text": "In the PR, the right pane of the Conversation page displays a Reviewers area. In many\nPyAnsys projects, maintainers are automatically assigned as reviewers. You can manually\nassign any number of reviewers. You can also see how many approving reviewers are\nrequired before the PR can be merged. This number varies from one PyAnsys project to another.\nManually add reviewers to your PR:\nClick the gear icon on the right side of this area.\nChoose the reviewer to add.\nIf a specified number of maintainers must review and approve the PR, the PR displays a\nCode owner review required area with status information.\nYou can add comments to your own PR as indicated in add_comments and then\nresolve them as indicated in the next topic. If you intend to keep working in\nyour local branch, remember to always use your Git tool to pull all changes made\nin the remote branch for your PR into your local branch.\nTag reviewers"
    },
    {
        "objectID": "content-writing/content-how-tos/create-PR",
        "href": "content-writing/content-how-tos/create-PR.html#resolve-reviewer-comments",
        "title": "Create a PR > Resolve reviewer comments",
        "section": "Resolve reviewer comments",
        "text": "A reviewer can make a general comment on your overall PR and both general and specific\ncomments on a single changed line or multiple consecutive changed lines in\nyour PR. For more information, see add_comments in the information for reviewing\na PR.\nThe Conversation page of your PR shows all overall comments on your PR and all\nunresolved comments on changed lines in your PR. Because overall comments are informational,\nthey do not display Resolve conversation buttons. However, all unresolved comments on\nchanged lines do display this button.\nHere is how to review and resolve comments on changed lines:\nOn the Conversation page, determine whether the comment requires you to\nmake changes to one or more files in your local branch.\nIf the comment is merely informational, click Resolve conversation.\nIf you must make changes in your local branch, do not click Resolve conversation.\nInstead, make these changes and push them to the PR.\nIf the comment makes changes to one or more lines, determine if you want\nto commit the suggestion.\nIf you click Commit suggestion on a reviewer’s suggestion and then\nclick Commit changes in the window that opens, the suggestion is\nimmediately committed to the PR, which triggers a build. Because the CI/CD\nprocess is resource intensive, best practice is to commit suggestions in batch,\nwhich triggers the build process only once. Subsequent steps assume that you\nare following this best practice.\nTo commit the suggestion, click Add suggestion to batch.\nRepeat the preceding steps to review and resolve comments or commit suggestions.\nAt the top of the PR, the Commit suggestions option indicates\nthe number of suggestions that are waiting to be committed in batch.\nWhen you are ready to commit all suggestions in batch, click\nCommit suggestions at the top of the PR and, in the window that opens,\nclick Commit Changes.\nIf you intend to keep working in your local branch, use your Git tool\nto pull all changes made in the remote branch for your PR into your local branch.\nGitHub notifies you if there are any merge conflicts. You can use the GitHub editor\nto find and edit the conflicts.\nResolve reviewer comments\nResolve conversation\nResolve conversation\nResolve conversation"
    },
    {
        "objectID": "content-writing/content-how-tos/create-PR",
        "href": "content-writing/content-how-tos/create-PR.html#merge-your-pr",
        "title": "Create a PR > Merge your PR",
        "section": "Merge your PR",
        "text": "You can merge your PR only after these criteria are met:\nAll required reviewers have approved the PR.\nAll conversations in the PR are resolved.\nAll checks configured in the CI/CD process have passed.\nThe branch has no conflicts with the base branch.\nIf the branch is out of date with the remote main branch, merging is blocked.\nTo the right of the This branch is out-of-date with the base branch\nnotification is an Update branch option. Clicking it merges the latest\nchanges in the main branch into this branch with a merge commit. While the\ndropdown provides an option for rebasing this PR on top of the latest changes\nand then force pushing the PR, choosing this option is not advised.\nWhen these criteria are met, a required reviewer might merge the PR for you.\nIf not, you can merge the PR:\nClick the Squash and merge option.\nIn the window that opens, provide an optional extended description and\nclick Confirm squash and merge.\nWhen you finish resolving comments, you can click the Enable auto-merge (squash) option\nto have PR be automatically merged when all criteria are met.\nMerge your PR"
    },
    {
        "objectID": "content-writing/content-how-tos/create-PR",
        "href": "content-writing/content-how-tos/create-PR.html#pull-changes-and-delete-the-merged-branch",
        "title": "Create a PR > Pull changes and delete the merged branch",
        "section": "Pull changes and delete the merged branch",
        "text": "One the PR is merged, use your GitHub tool to pull all changes from the remote main\nbranch on GitHub into the main branch of your locally cloned repository. Also delete\nthe local branch with the changes that have now been merged. For additional changes,\ncreate another local branch to work in.\nPull changes and delete the merged branch"
    },
    {
        "objectID": "content-writing/content-how-tos/create-PR",
        "href": "content-writing/content-how-tos/create-PR.html#remove-untracked-files-and-directories",
        "title": "Create a PR > Remove untracked files and directories",
        "section": "Remove untracked files and directories",
        "text": "To remove untracked files and directories from your working directory, from the\ndoc folder, periodically run this command:\ngit clean -fdx .\nFor more information on this Git command, see git_clean.\nWhen you next run pre-commit, the code style tools configured for\nyour PyAnsys project must be initialized once again. For more information,\nsee run_precommit.\nBefore you can run Vale again locally, you must download the latest rules for the\nGoogle developer’s documentation style guide to the doc/styles/Google folder\nby running this command:\nYou can then run Vale with this command:\nFor more information, see run_Vale_locally.\nRemove untracked files and directories\ndoc\ngit clean -fdx .\npre-commit\ndoc/styles/Google"
    },
    {
        "objectID": "abstractions/data-transfer",
        "href": "abstractions/data-transfer.html#data-transfer",
        "title": "Data transfer",
        "section": "Data transfer",
        "text": "Abstracted APIs should attempt to hide the implementation details of\nthe remote or local API in a well organized data model. This data\nmodel should avoid returning raw JSON files, gRPC messages, SWIG objects,\nor .NET objects. It should preferably return standard Python objects\nlike lists, strings, dictionaries when useful, and numpy\narrays or pandas dataframes for more complex data.\nFor example, consider a simple mesh in MAPDL:\nAt this point, there are only two ways within MAPDL to access the nodes and\nconnectivity of the mesh, You can either print it using the NLIST\ncommand or write it to disk using the CDWRITE command. Both of these\nmethods are remarkably inefficient, requiring:\nSerializing the data to ASCII on the server\nTransferring the data\nDeserializing the data within Python\nConverting the data to an array\nThis example prints node coordinates using the NLIST command:\nIt’s more efficient to transfer the node array as either a\nseries of repeated Node messages or, better yet, to serialize\nthe entire array into bytes and then deserialize it on the client\nside. For a concrete and standalone example of this in C++ and Python,\nsee grpc_chunk_stream_demo.\nWhile raw byte streams are vastly more efficient, one major disadvantage\nis that the structure of the data is lost when serializing the array.\nThis should be considered when deciding how to write your API.\nRegardless of the serialization or message format, users\nexpect Python native types (or a common type for a common library like\npandas.DataFrame or numpy.ndarray).  Here, within PyMAPDL,\nthe nodes of the mesh are accessible as the nodes attribute within\nthe mesh attribute, which provides an encapsulation of the mesh\nwithin the MAPDL database.\nData transfer\nNLIST\nCDWRITE\nNLIST\nNode\npandas.DataFrame\nnumpy.ndarray\nnodes\nmesh"
    },
    {
        "objectID": "abstractions/data-transfer",
        "href": "abstractions/data-transfer.html#rest-versus-rpc-data-and-model-abstraction",
        "title": "Data transfer > REST versus RPC data and model abstraction",
        "section": "REST versus RPC data and model abstraction",
        "text": "Because of the nature of most Ansys products, apps and\nservices can either fit into the Remote Procedure Call (RPC) interface,\nwhere the API is centered around operations and commands, or the\nREST model, where the API is centered around resources. Regardless of\nthe interface style, there are several items to consider.\nREST versus RPC data and model abstraction"
    },
    {
        "objectID": "abstractions/data-transfer",
        "href": "abstractions/data-transfer.html#api-chattiness",
        "title": "Data transfer > API chattiness",
        "section": "API chattiness",
        "text": "APIs must be efficient to avoid creating chatty input and output.\nBecause many Ansys products fit well with the RPC API implementation,\nthere is a temptation to design APIs that require constant communication\nwith the remote or local service. However, just because RPC interfaces\ncan do this, it does not mean that they should. The assumption should be\nthat each RPC call takes significant time to execute.\nOne way to avoid constant communication with the service is to serialize\nseveral “commands” for services that employ an internal “command pattern”\nfor data exchange. Another approach is to encapsulate the data model\nentirely on the server to avoid data transfer whenever possible and\nexpose only a limited number of RPC methods in the front-facing API.\nAPI chattiness"
    },
    {
        "objectID": "abstractions/data-transfer",
        "href": "abstractions/data-transfer.html#compatibility-and-efficiency",
        "title": "Data transfer > Compatibility and efficiency",
        "section": "Compatibility and efficiency",
        "text": "APIs should be designed to be compatible with as many languages and\nplatforms as possible.  gRPC for RPC-like interfaces should be one\nof the first choices due to its compatibility with nearly all popular\nlanguages and its efficiency over REST in terms of speed, memory, and\npayload size.\nTypically, REST data exchanges should be limited to short messages\nthat are transferred using JSON files, and gRPC should be used for large data\ntransfers and bidirectional streaming.\nChoosing gRPC over REST is generally preferable due to the performance\nbenefits of a binary compatible protocol. While REST provides a variety of\nbenefits, for complex client/server interactions, it is best to have an\ninterface that can efficiently exchange a wide variety of data formats and\nmessages.\nCompatibility and efficiency"
    },
    {
        "objectID": "how-to/grpc-api-packages",
        "href": "how-to/grpc-api-packages.html#grpc-api-packages",
        "title": "gRPC API packages",
        "section": "gRPC API packages",
        "text": "Protobuf service definitions provide the API specification for underlying\nserver implementations so that each consuming client library has a clear\ncontract for gRPC data messages. Ideally, the Protobuf (.proto) files\nhave a single repository established as the source of truth, organized by\nAPI version increment as the API definition expands and changes. Because\nmost client libraries are custom implementations enhancing the developer\nexperience when consuming the service, releasing the Protobuf definitions\npublicly gives full flexibility to developers to operate at the abstraction\nlayer they choose.\ngRPC API packages\n.proto"
    },
    {
        "objectID": "how-to/grpc-api-packages",
        "href": "how-to/grpc-api-packages.html#maintain-api-definition-repository",
        "title": "gRPC API packages > Maintain API definition repository",
        "section": "Maintain API definition repository",
        "text": "Because the Protobuf definition of the service is language agnostic, the repository\ncontaining the PROTO files can be created within the top-level\nAnsys GitHub organization.\nEvery update of the PROTO files follows a standard pull request process as a\nsanity check for API definition accuracy. Language-specific packages can be\ngenerated for each merge or on a set cadence.\nMaintain API definition repository"
    },
    {
        "objectID": "how-to/grpc-api-packages",
        "href": "how-to/grpc-api-packages.html#manage-protobuf-definitions-for-python-clients",
        "title": "gRPC API packages > Manage Protobuf definitions for Python clients",
        "section": "Manage Protobuf definitions for Python clients",
        "text": "Within Ansys, and more specifically in the PyAnsys environment, most client libraries\nhave a dedicated Python package containing the needed PROTO files compiled as\nPython source code. These are typically consumed by the PyAnsys client libraries\nfor communicating with their respective services.\nFor example, PyMAPDL consumes the ansys-api-mapdl package, which is built in the\nansys-api-mapdl repository.\nManage Protobuf definitions for Python clients\nansys-api-mapdl"
    },
    {
        "objectID": "how-to/grpc-api-packages",
        "href": "how-to/grpc-api-packages.html#build-an-ansys-api-service-repository",
        "title": "gRPC API packages > Build an ansys-api-<service> repository",
        "section": "Build an ansys-api-<service> repository",
        "text": "The Ansys GitHub organization has a dedicated template repository for creating\nPROTO file repositories and the needed files to generate the Python API\npackages to be consumed by the PyAnsys clients.\nTo set up an API repository like the ansys-api-mapdl one,\nselect the ansys-api-template repository\nwhen creating a repository within the Ansys GitHub organization.\nTo understand how to use the ansys-api-template repository, see\nExpected usage\nin this repository’s README.\nBuild an ansys-api-<service> repository\nansys-api-<service>\nansys-api-mapdl\nansys-api-template"
    },
    {
        "objectID": "how-to/grpc-api-packages",
        "href": "how-to/grpc-api-packages.html#build-python-stub-classes",
        "title": "gRPC API packages > Build Python stub classes",
        "section": "Build Python stub classes",
        "text": "The ansys-api-template repository uses the ansys-tools-protoc-helper\nutility to auto-generate Python wheels that can be consumed by downstream Python client libraries.\nTo use the ansys-tools-protoc-helper utility, include it in the pyproject.toml file as a build dependency:\nThen generate a Python wheel containing the autogenerated Python source with\nthese commands:\nBuild Python stub classes\nansys-api-template\nansys-tools-protoc-helper\npyproject.toml"
    },
    {
        "objectID": "how-to/grpc-api-packages",
        "href": "how-to/grpc-api-packages.html#publish-the-api-package",
        "title": "gRPC API packages > Publish the API package",
        "section": "Publish the API package",
        "text": "PyPI is the common package manager where API packages are released.\nHere is an example of a workflow pipeline for building and publishing the Python stub package.\nIn this example, the ansys-api-geometry workflow is shown. However, you can easily copy\nand adapt this workflow. Only the PYTHON_PACKAGE_IMPORT environment variable would have\nto be changed:\nPublish the API package\nansys-api-geometry\nPYTHON_PACKAGE_IMPORT"
    },
    {
        "objectID": "how-to/grpc-api-packages",
        "href": "how-to/grpc-api-packages.html#version-the-api-package",
        "title": "gRPC API packages > Version the API package",
        "section": "Version the API package",
        "text": "PyPI packages follow semantic versioning while gRPC Protobuf API versions\ntypically follow a simplified v* versioning pattern. The PyPI package\nversion is not expected to synchronize with the Protobuf API version, and\nmultiple public APIs can be exposed simultaneously. For example, if you have a\nv0 for MAPDL exposed, you can access it with this code:\nWhile if the API has a v1 API exposed, a different library could also use:\nAnsys follows Microsoft’s gRPC versioning\nrecommendations, which stipulate that incrementing the gRPC Protobuf version is\nonly necessary when making a backwards breaking change. Non-breaking changes\ninclude:\nAdding a service\nAdding a method to a service\nAdding a field to a request message\nHowever, this only applies to the vN gRPC Protobuf API. Python packages\ntend to follow semantic versioning, and PyAnsys packages follow this\napproach. Therefore, these Python gRPC API packages should also follow semantic\nversioning.\nPlan on releasing a new minor version when adding or removing features, messages,\nand services.\nPlan on releasing a patch release when fixing bugs that do not change the behavior\nof the API.\nOnly plan on releasing a major release once the API is stable and no\nmajor release is scheduled in the near future.\nThis way, you can expose a v0 and/or v1 gRPC Protobuf API and release\nfrequent updates using semantic versioning.\nVersion the API package\nv*\nv0\nv1\nvN\nv0\nv1"
    },
    {
        "objectID": "how-to/grpc-api-packages",
        "href": "how-to/grpc-api-packages.html#release-the-api-package",
        "title": "gRPC API packages > Release the API package",
        "section": "Release the API package",
        "text": "As shown in the release section of the previous GitHub workflow, once the Python\nAPI package is compiled it is then uploaded to the public PyPI. In order to do\nso, it is necessary to have access to the PYPI_TOKEN within the GitHub\nrepository. To get the needed credentials, contact the\nPyAnsy core team.\nIf the repository cannot be uploaded to the public PyPI yet but your Python\nclient library needs to consume this Python API package, it can be\nuploaded to the private PyAnsys PyPI. For the required PYANSYS_PYPI_PRIVATE_PAT\npassword, contact the PyAnsy core team.\nIn this last case, the Upload to Public PyPi workflow section should be\nreplaced with the Upload to Private PyPi workflow section:\nRelease the API package\nrelease\nPYPI_TOKEN\nPYANSYS_PYPI_PRIVATE_PAT\nUpload to Public PyPi\nUpload to Private PyPi"
    },
    {
        "objectID": "how-to/grpc-api-packages",
        "href": "how-to/grpc-api-packages.html#consume-the-api-package-within-python",
        "title": "gRPC API packages > Consume the API package within Python",
        "section": "Consume the API package within Python",
        "text": "Once the API package has been published to PyPI, you can include a reference\nwithin the client library build dependencies. For information on how to specify\na project’s required dependencies, see Required Dependencies.\nConsume the API package within Python"
    },
    {
        "objectID": "how-to/grpc-api-packages",
        "href": "how-to/grpc-api-packages.html#use-the-api-package-within-the-python-client",
        "title": "gRPC API packages > Use the API package within the Python client",
        "section": "Use the API package within the Python client",
        "text": "The stub imports follow a standard pattern. For each API service, there is a *_pb2\nmodule that defines all messages within a specific service file and\na *_pb2_grpc module that defines a Stub class that encapsulates all service methods.\nUse the API package within the Python client\n*_pb2\n*_pb2_grpc\nStub"
    },
    {
        "objectID": "how-to/grpc-api-packages",
        "href": "how-to/grpc-api-packages.html#example-grpc-imports-within-the-wrapping-client-library",
        "title": "gRPC API packages > Example gRPC imports within the wrapping client library",
        "section": "Example gRPC imports within the wrapping client library",
        "text": "The best practice is to create a Pythonic client library that organizes the service methods\nin a user-friendly manner. At a minimum, this library should act as a facade layer wrapping the\nservice calls so that the Pythonic API can have a consistent abstraction, independent of\nunderlying implementations.\nFor each client library release, only a single gRPC API version should be wrapped\nto maintain a consistent API abstraction expectation for the supporting server instances.\nExample gRPC imports within the wrapping client library"
    },
    {
        "objectID": "how-to/grpc-api-packages",
        "href": "how-to/grpc-api-packages.html#public-versus-private-python-api-package",
        "title": "gRPC API packages > Public versus private Python API package",
        "section": "Public versus private Python API package",
        "text": "Making the PROTO files for a public or private repository is up to the owner of each repository.\nIn terms of intellectual property (IP) concerns, the PROTO files are typically not an\nissue because they do not expose any critical service logic or knowledge. In most cases,\nthe APIs being exposed through the PROTO files are already exposed publicly through other\nmechanisms.\nThus, the general recommendation is to make these repositories public as soon as possible. The\nmain reasons for doing so follow:\nPrivate Python package dependencies usually involve workarounds when setting up the\nworkflow. It is best to keep the workflows as standard and simple as possible. That\nimplies making all its dependencies public, including this API Python package.\nThe API Python package generated eventually must be uploaded to the public PyPI so\nthat it can be consumed by its corresponding Python client library (when it is publicly released).\nSo, if there are no issues with making it public, it is better to do so sooner rather than later.\nOnce the Python API package is publicly released to PyPI, there is no reason to keep the\nrepository private because all users who consume the Python API package have direct access\nto the PROTO files that are in the repository.\nHowever, before making any repository public in the Ansys GitHub organization, review\nthe Ansys Open Source Developer’s Guide\nto verify that the repository is compliant with all the needed requirements.\nPublic versus private Python API package"
    },
    {
        "objectID": "how-to/contributing",
        "href": "how-to/contributing.html#contributing",
        "title": "Contributing",
        "section": "Contributing",
        "text": "Before contributing to a PyAnsys library, you must understand the general\ncoding paradigms used for PyAnsys development.\nFollow the Zen of Python.\nAs silly as core Python developers are sometimes, there’s much to be\ngained by following the basic guidelines listed in PEP 20. As suggested\nin these guidelines, focus on making your additions intuitive, novel,\nand helpful for users. When in doubt, use import this.\nFor Ansys code quality standards, see Coding style.\nDocument your contributions. Include a docstring for any added function,\nclass, or method, following Numpydoc docstrings as specified by\nPyAnsys Documentation style. Always provide at least one simple use\ncase for a new feature.\nTest your contribution. Because Python is an interpreted language, if\nit’s not tested, it’s probably broken. At the minimum, include a unit\ntest for each new feature within the tests directory. Ensure that\neach new function, class, or method has a reasonable coverage (greater\nthan 80%). For information on automated testing, see Testing.\nDo not include any datasets for which a license is not available\nor commercial use is prohibited.\nReview the Ansys Contributor Code of Conduct.\nAll PyAnsys projects are hosted on GitHub in the form of Git\nrepositories. GitHub is a platform that not only provides storage for\nprojects but also additional features like code reviews or issue boards.\nContributing\nimport this\ntests"
    },
    {
        "objectID": "how-to/contributing",
        "href": "how-to/contributing.html#create-a-github-account",
        "title": "Contributing > Create a GitHub account",
        "section": "Create a GitHub account",
        "text": "To use GitHub, start by creating an account for the platform. Follow the\nGitHub Join Process.\nFor Ansys employees who would like to join the Ansys GitHub organization,\nvisit Join Ansys GitHub Organization.\nCreate a GitHub account"
    },
    {
        "objectID": "how-to/contributing",
        "href": "how-to/contributing.html#interact-with-github-repository-sections",
        "title": "Contributing > Interact with GitHub repository sections",
        "section": "Interact with GitHub repository sections",
        "text": "Once you have a GitHub account and access to the Ansys GitHub organization,\nyou are able to interact with the different repositories. While each\nrepository contains all tabbed sections in the following list,\nyour access level determines which tabbed sections you can see.\nCode: Tree view of the project’s structure\nIssues: Posts noting issues or requesting new features\nPull requests: Code changes either awaiting review or merging or already closed\nDiscussions: Exchanges about development practices with project maintainers\nActions: Available CI/CD workflows\nProjects: Plans for organizing and developing the repository\nWiki: Basic project information\nSecurity: Configurations related to security issues, vulnerabilities, and alerts\nInsights: General information about the repository and its contributors\nSettings: Configurations for access and integration with third-party tools\nInteract with GitHub repository sections\nCode\nIssues\nPull requests\nDiscussions\nActions\nProjects\nWiki\nSecurity\nInsights\nSettings"
    },
    {
        "objectID": "how-to/contributing",
        "href": "how-to/contributing.html#create-an-issue",
        "title": "Contributing > Create an issue",
        "section": "Create an issue",
        "text": "You create an issue to report a bug, request a new feature, or ask for library-specific\nhelp. You can comment on an issue to interact with other users, developers, and project\nmaintainers.\nTo open an issue, select the Issues tab in the github_repo_sections and click\nNew Issue. Then, select a template for the type of issue to open.\nGitHub issues require the usage of Markdown files instead of ReStructuredText (RST)\nfiles. For more information, see Basic writing and formatting syntax\nin the GitHub documentation.\nCreate an issue\nIssues\nNew Issue"
    },
    {
        "objectID": "how-to/contributing",
        "href": "how-to/contributing.html#report-bugs",
        "title": "Contributing > Report bugs",
        "section": "Report bugs",
        "text": "If you encounter a bug in the code, open a new issue and select the template\nfor creating a bug report. In the bug report, take these actions:\nIndicate the operating system, Python version, and library version that you are using.\nInclude a small piece of code to allow others to reproduce the bug that you found.\nAdd any additional information that you consider useful for fixing the bug.\nReport bugs"
    },
    {
        "objectID": "how-to/contributing",
        "href": "how-to/contributing.html#request-new-features",
        "title": "Contributing > Request new features",
        "section": "Request new features",
        "text": "If you would like a new feature to be added to a PyAnsys library, open a\nnew issue and select either the template for code enhancements or a\nfeature idea. In the issue, take these actions:\nDescribe the main goal of the feature that you’d like to have added and why it is beneficial\nto the project.\nDescribe how this feature might possibly be implemented and the steps that should be\nfollowed.\nAdd any references that could help during the development process.\nRequest new features"
    },
    {
        "objectID": "how-to/contributing",
        "href": "how-to/contributing.html#fork-a-repository",
        "title": "Contributing > Fork a repository",
        "section": "Fork a repository",
        "text": "Forking a repository is like copying and pasting a project into your own GitHub\nprofile. Notice that only repositories labeled as public can be forked. You\ncannot fork a repository labeled as internal or private.\nTo fork a repository, click the Fork button at the top of the project’s\nCode page.\nFork a repository\npublic\ninternal\nprivate"
    },
    {
        "objectID": "how-to/contributing",
        "href": "how-to/contributing.html#clone-a-repository",
        "title": "Contributing > Clone a repository",
        "section": "Clone a repository",
        "text": "Cloning a repository means downloading it to your local machine. While there are two ways of\ndoing this (HTTPS or SSH), to force the usage of SSH, only this method is explained.\nClone a repository\nHTTPS\nSSH\nSSH"
    },
    {
        "objectID": "how-to/contributing",
        "href": "how-to/contributing.html#clone-using-ssh",
        "title": "Contributing > Clone using SSH",
        "section": "Clone using SSH",
        "text": "Cloning using SSH requires that SSH be enabled. For more information, see Enable SSH.\nTo clone a repository using SSH, run this command:\nFor example, clone the PyMAPDL project with this command:\nClone using SSH\nSSH"
    },
    {
        "objectID": "how-to/contributing",
        "href": "how-to/contributing.html#install-a-library-in-editable-mode",
        "title": "Contributing > Install a library in editable mode",
        "section": "Install a library in editable mode",
        "text": "You can install a Python library in editable mode, which\nlets you modify the source code and have these new changes\nreflected in your Python environment.\nTo install a Python library in editable mode:\nEnsure that you Create and Activate a Python virtual environment,\nas explained in the Virtual environments section.\nUpdate pip with this command:\nInstall the library with this command:\nInstall a library in editable mode\npip"
    },
    {
        "objectID": "how-to/contributing",
        "href": "how-to/contributing.html#create-a-branch",
        "title": "Contributing > Create a branch",
        "section": "Create a branch",
        "text": "It is likely that the repository’s default branch name is main or master. This is the\ndevelopment branch for PyAnsys projects. For more information, see Branching model.\nYou must implement new contributions in a different branch and then Create a pull request\nso that these changes can later be merged into the repository’s main branch.\nTo create a branch, run this command:\nCreate a branch\nmain\nmaster\nmain"
    },
    {
        "objectID": "how-to/contributing",
        "href": "how-to/contributing.html#branch-naming-conventions",
        "title": "Contributing > Branch-naming conventions",
        "section": "Branch-naming conventions",
        "text": "The following requirements for naming branches helps to streamline\ndevelopment. They help core developers know what kind of\nchanges any given branch is introducing before looking at the code.\nfix/: Bug fixes, patches, or experimental changes that are\nminor.\nfeat/: Changes that introduce a new feature or significant\naddition.\njunk/: Experimental changes that can be deleted if they go\nstale.\nmaint/: General maintenance of the repository or CI routines.\ndocs/: Changes pertaining only to documentation.\nno-ci/: Low-impact activity that should not trigger CI\nroutines.\ntesting/ or test/: Improvements or changes to testing.\nrelease/: Releases (see below).\nchore/: Other changes that don’t modify the code.\nstyle/: Changes that only affect code style.\nrefactor/: A code change that neither fixes a bug nor adds a feature.\nperf/: A code change that improves performance.\nci/: Changes to the CICD configuration files and scripts.\nbuild/: Changes that affect the build system or external dependencies.\nBranch-naming conventions\nfix/\nfeat/\njunk/\nmaint/\ndocs/\nno-ci/\ntesting/\ntest/\nrelease/\nchore/\nstyle/\nrefactor/\nperf/\nci/\nbuild/"
    },
    {
        "objectID": "how-to/contributing",
        "href": "how-to/contributing.html#push-your-branch",
        "title": "Contributing > Push your branch",
        "section": "Push your branch",
        "text": "Once you have implemented new changes and committed them, push your\nbranch with this command:\nYour changes are upload to the repository, but they are only visible in the branch\nthat you just pushed.\nPush your branch"
    },
    {
        "objectID": "how-to/contributing",
        "href": "how-to/contributing.html#create-a-pull-request",
        "title": "Contributing > Create a pull request",
        "section": "Create a pull request",
        "text": "Once you have tested your branch locally, create a pull request (PR) and target your merge to\nthe repository’s main branch. This automatically runs CI testing and verifies that your changes\nwork across all supported platforms. For procedural information, see Creating a pull request\nin the GitHub documentation.\nAfter you submit your PR, a project maintainer reviews your code to verify that it meets\nthe Packaging style, Coding style, and Documentation style.\nOnce your code is approved, if you have write permission, you can merge the PR\nand then delete the PR branch. If you don’t have write permission, the reviewer\nor someone else with write permission must merge your PR and then delete your PR branch.\nYou can set up automatic deletion of branches in Settings > General > Pull Requests.\nCreate a pull request\nAlways delete your PR branch after merging it into the main branch.\nmain"
    },
    {
        "objectID": "how-to/contributing",
        "href": "how-to/contributing.html#commit-naming-conventions",
        "title": "Contributing > Commit-naming conventions",
        "section": "Commit-naming conventions",
        "text": "Following the conventional commits standards,\nhelps to streamline development and improve the Changelog\nquality. Here is a list of the most common commit types:\nfix:: Bug fixes.\nfeat:: Changes that introduce a new feature or significant addition.\ndocs:: Changes pertaining only to documentation.\nstyle:: Changes that do not affect the meaning of the code (white-space,\nformatting, missing semi-colons, etc.).\nrefactor:: A code change that neither fixes a bug nor adds a feature.\nperf:: A code change that improves performance.\ntest:: Improvements or changes to testing.\nbuild:: Changes that affect the build system or external dependencies\n(example scopes: pip, npm, make).\nci:: Changes to the CICD configuration files and scripts.\nchore:: Other changes that don’t modify the code (example scopes: release,\nversioning, etc.).\nrevert:: Reverts a previous commit.\nCommit-naming conventions\nfix:\nfeat:\ndocs:\nstyle:\nrefactor:\nperf:\ntest:\nbuild:\nci:\nchore:\nrevert:"
    },
    {
        "objectID": "how-to/contributing",
        "href": "how-to/contributing.html#use-github-cli",
        "title": "Contributing > Use GitHub CLI",
        "section": "Use GitHub CLI",
        "text": "Because developers do not like leaving their terminals when working in projects,\nGitHub offers a command-line interface (CLI).\nThis program lets you interact with most of the features available in the\nweb version of GitHub. For available commands, see the\nGitHub CLI documentation.\nUse GitHub CLI"
    },
    {
        "objectID": "getting-started/administration",
        "href": "getting-started/administration.html#project-approval-and-public-release",
        "title": "Project approval and public release",
        "section": "Project approval and public release",
        "text": "Most of the projects in the Ansys organization\nexpose the functionality of Ansys products. Due to intellectual property reasons,\nthe public release of a PyAnsys library must go through a project approval process.\nProject approval and public release"
    },
    {
        "objectID": "getting-started/administration",
        "href": "getting-started/administration.html#first-step",
        "title": "Project approval and public release > First step",
        "section": "First step",
        "text": "To trigger the public release process, project leads must first complete the\nRelease request workflow initiation form.\nThis form lets the different parties involved in the public release process know that\nthere is a request to release a project. If your intent is to release an Ansys open\nsource project, then continue to the next section.\nFirst step"
    },
    {
        "objectID": "getting-started/administration",
        "href": "getting-started/administration.html#approval-process",
        "title": "Project approval and public release > Approval process",
        "section": "Approval process",
        "text": "The approval process is divided into three parts:\nEnsures that direct managers, general managers, and the chief technology\nofficer are aware of the project and approve it.\nEnsures that the project adheres to a legal framework that protects\nAnsys intellectual property.\nEnsures that the project complies with Ansys software guidelines and best practices.\nAn approval from each of these three parts is required to release a project to the public.\nOnce all approvals are received, a project can be published to the Public PyPI.\nWhen releasing a project to the public, you must perform these tasks:\nCoordinate with the product line development team, if applicable.\nMaintain the project by means of fixing bugs and providing support for new releases.\nUphold Ansys’ reputation in the open source community.\nOnce all three approvals have been awarded, project leads must then complete\nthe OSS (Open Source Software) approval request form.\nThis form serves as a final checklist to verify that all approvals have been processed\nand to formalize the OSS approval process as a final record. For more information, see\nQuestions asked on the OSS approval request form.\nApproval process"
    },
    {
        "objectID": "getting-started/administration",
        "href": "getting-started/administration.html#managerial",
        "title": "Project approval and public release > Managerial",
        "section": "Managerial",
        "text": "The managerial part of the approval process ensures that the direct manager,\ngeneral managers, vice presidents, or the chief technology officer are aware of\nthe project’s existence and status.\nA project must be classified into one of these categories, determining the\nnumber of administrative reviews and approvals needed:\nDocumentation projects must have direct manager approval, legal review, and\ndocumentation proofing. No source code other than documentation files\nis allowed.\nTool projects require direct manager and business unit’s general\nmanager approval. No product source code is allowed.\nLibrary projects interfacing and wrapping any Ansys products require\napproval from the direct manager, product line, and the chief\ntechnology officer. No product source code is allowed.\nFor multi-physics and tools impacting several products, you must have approval from\nthe chief technology officer for Ansys. For flagship-related projects, you must have\ngeneral manager or vice president approval.\nManagerial"
    },
    {
        "objectID": "getting-started/administration",
        "href": "getting-started/administration.html#legal",
        "title": "Project approval and public release > Legal",
        "section": "Legal",
        "text": "Legal review approval ensures that the entire project complies with Ansys’\nlegal policies.\nClick the following button to complete the legal review request form for open sourcing the code:\nOpen source code release request form\nThese checks are required when performing the legal review of the project:\nOpen source dependencies not distributed as part of the project do not need\ntheir licenses included in the Ansys repository. Examples include dependent\nNode Package Manager (npm) modules or Python packages from PyPI.\nLegal\nCopyright (C) YYYY ANSYS, Inc. and/or its affiliates.\nnpm"
    },
    {
        "objectID": "getting-started/administration",
        "href": "getting-started/administration.html#technical",
        "title": "Project approval and public release > Technical",
        "section": "Technical",
        "text": "Technical approval ensures that the project follows the best and latest\nsoftware development practices. Request a technical review by sending an email\nto pyansys.core@ansys.com.\nThe PyAnsys core team performs these checks when performing the technical review of the project:\nTechnical"
    },
    {
        "objectID": "getting-started/administration",
        "href": "getting-started/administration.html#questions-asked-on-the-oss-approval-request-form",
        "title": "Project approval and public release > Questions asked on the OSS approval request form",
        "section": "Questions asked on the OSS approval request form",
        "text": "When completing the OSS approval request form, project leads must\nsupply responses to several types of questions:\nWhat is the name of your project?\nWho is the project maintainer?\nWho is the lead from the product team?\nWho is the Product Management contact?\nWho is the ACE/AFT owner?\nWho validated your legal readiness?\nProvided there are no issues with the MIT license, have you correctly applied\nit to the GitHub Repository for your project?\nIs the copyright header correctly applied to your files in GitHub?\nHave you confirmed that any intellectual property is removed from the code, docs,\nand examples?\nI and my legal reviewer, as well as my product and PM reviewer, have confirmed that\nthere is no business interest in keeping this code confidential.\nI and my legal reviewer confirm there is no business interest in enforcing copyright\nprotection for this code.\nI and my legal reviewer confirm that the code does not contain any third-party material\n(open source, proprietary, partner, customer, or otherwise).\nI and my legal reviewer confirm that the code does not include any invention on which\nthe company has, or might want to seek, a patent.\nHave you cleaned up comments, issues, and pull requests to remove any potentially bad content?\nMy legal reviewer and I have checked the dependencies and validated that they do not\nimpose any licensing difficulties.\nI and my legal reviewer confirm there is NO encryption present in the code.\nThe repository that hosts the code is generally accessible to the public with no\ntime limits or access restrictions.\nThis tool or library is not meant for use in any specific industry, platform, or\nprocess but rather for use by general customers.\nWho verified your technical review?\nHas your library documentation been reviewed by a documentation team member?\nHas your source code documentation been reviewed by a developer team member?\nHas end user testing been completed?\nHas CI/CD testing been implemented?\nHas a minimum test coverage of 80% been achieved?\nAre usage and installation examples included and tested?\nIs the package definition ready and PyPi packaging completed?\nDoes the GitHub repository supply contribution guidance and have CLA set up?\nWho on the Product Marketing Manager (PMM) or Developer Ecosystem (DevEco)\nteam checked your project for readiness?\nDid you tell ACE and your Business Unit lead that you are ready for release?\nIs there something public that already has the same name as your project?\nDid you get PMM signoff?\nDid you ask the DevEco team to update links from the Developer Portal to your\nnew OSS project?\nDid you let the PMM team know that your library is nearing release?\nQuestions asked on the OSS approval request form"
    },
    {
        "objectID": "content-writing/rst-files-writers/index",
        "href": "content-writing/rst-files-writers/index.html#content-in-rst-files",
        "title": "Content in RST files",
        "section": "Content in RST files",
        "text": "reStructuredText, a plaintext markup language, uses simple and intuitive\nconstructs to indicate the structure of a document. You use reStructuredText (RST)\nfiles to define the hierarchy of your documentation and provide manually authored\ncontent.\nThis page describes the setup of RST files for PyAnsys documentation. It also\ndescribes the option for using either a RST or Markdown (MD) file as the README\nfile for the GitHub repository of a PyAnsys project.\nFor resources related to RST and MD files, see style_format_resources.\nTo learn how RST files for PyAnsys libraries are formatted, see rst_file_formatting.\nContent in RST files"
    },
    {
        "objectID": "content-writing/rst-files-writers/index",
        "href": "content-writing/rst-files-writers/index.html#rst-file-setup",
        "title": "Content in RST files > RST file setup",
        "section": "RST file setup",
        "text": "In a repository, the doc/source directory contains an index.rst file that defines\nthe overall hierarchy (major sections) of the documentation. The child directory for each\nsection has its own index file.\nWhile most PyAnsys libraries use index.rst as the name for a section’s index\nfile, to optimize web searches of the generated HTML documentation, this file\nshould have a short and descriptive name that contains keywords and uses hyphens\n(-) to separate words. All directory names should also use hyphens to separate\nwords. For more information, see SEO.\nIn the index file for a documentation section, the first-level heading is the name\nof the section. Documentation for a PyAnsys client library generally has\nfive sections with these headings:\nGetting started\nUser guide\nAPI reference\nExamples\nContribute\nAfter the section heading and any additional content, the index file includes the\ntoctree directive to specify the pages that this section is to display.\nThe toctree directive for this Content in RST files page\n(doc/_build/html/content-writing/rst-files-writers/index.html), looks like this:\nThe maxdepth attribute of the toctree directive specifies the maximum number of\nheading levels to show in the documentation’s right pane, labeled On this page. The\nmaxdepth attribute is set to 3 for all sections in this guide.\nThe toctree directive also includes an ordered list of the RST files to show in the\ndocumentation’s left pane, labeled Section Navigation. You omit the RST extensions\nfrom this list of files.\nTo see the toctree directives for the other sections in this guide,\nin the project’s repository, go to the doc/source\ndirectory and look at the index.rst files in the child directories.\nFor more information on RST file setup, see rst_files_developers and\nGetting Started in the Sphinx documentation.\nRST file setup\ndoc/source\nindex.rst\nindex.rst\n-\ntoctree\ntoctree\ndoc/_build/html/content-writing/rst-files-writers/index.html\nmaxdepth\ntoctree\nmaxdepth\n3\ntoctree\ntoctree\ndoc/source\nindex.rst"
    },
    {
        "objectID": "content-writing/rst-files-writers/index",
        "href": "content-writing/rst-files-writers/index.html#readmerst-files",
        "title": "Content in RST files > README.rst files",
        "section": "README.rst files",
        "text": "Each PyAnsys repository has a README file in its root directory that explains the\nproject and points readers to the documentation. The README file can be an RST file\nor a GitHub Flavored Markdown (MD) file. While RST and MD files are similar, the syntax\ndiffers.\nIf your README file is an RST file, see rst_file_formatting for syntax information.\nIf your README file is an MD file, see GitHub Flavored Markdown Spec\nand Using Markdown and Liquid in GitHub Docs for\nsyntax information.\nYou can reuse content in a README.rst file in the main index.rst\nfile for your documentation or in the index file for its “Getting started”\nsection. However, you cannot reuse content in a README.md file. Thus, the\ndisadvantages of having to use a different syntax in the MD file and the\ninability to reuse content in it in your documentation might influence you to use a\nREADME.rst file.\nTo reuse all content in a README.rst file in the main index.rst file for your\ndocumentation, use the include directive:\nTo reuse only a portion of the content in a README.rst file, use this directive’s start-line\nand end-line attributes:\nBecause using the preceding attributes necessitates having to change the line numbers\nif content is later added to or removed from the README.rst file, you\nmight want to use this directive’s start-after attribute instead. It allows\nyou to reuse content from a given point to the end of the file.\nYou first insert a target in the README.rst file where you want to start the reuse.\nFor example, assume that the README.rst file has an “Overview” section where you want the reuse\nto begin. Before this section, insert an explicit target name like this, followed by a blank line:\nIn the main index.rst file for your documentation, now insert an include\ndirective with a start-after attribute that specifies this explicit target name:\nIf your README.rst file has links to sections or pages in the documentation, you must\nuse either URLs or insert explicit targets at the bottom of the README.rst file that you can then\nuse in this file. If your project has a central links.rst file in the doc/source directory,\nyou might be tempted to simply use the explicit target names named defined in it in the README.rst\nfile. However, the GitHub renderer is unaware of the links.rst file. For more information, see\ndoc_links_external.\nREADME.rst files\nREADME.rst\nREADME.rst\nindex.rst\nREADME.md\nREADME.rst\nREADME.rst\nindex.rst\ninclude\nREADME.rst\nstart-line\nend-line\nREADME.rst\nstart-after\nREADME.rst\nREADME.rst\nindex.rst\ninclude\nstart-after\nREADME.rst\nREADME.rst\nlinks.rst\ndoc/source\nREADME.rst\nlinks.rst"
    },
    {
        "objectID": "doc-style/doc-configuration",
        "href": "doc-style/doc-configuration.html#required-sphinx-configuration",
        "title": "Required Sphinx configuration",
        "section": "Required Sphinx configuration",
        "text": "This page explains the minimum required Sphinx configuration for\nbuilding the documentation of a PyAnsys library.\nWhen installing Sphinx, a program named sphinx-build also gets installed.\nThis program is in charge of collecting, parsing, and rendering all\nReStructuredText (RST) files in The ``doc`` directory.\nThe behavior of the sphinx-build program is controlled through either\na Makefile (for POSIX systems) or a make.bat file (for Windows systems).\nOnce the sphinx-build command is triggered, the configuration declared in the\nconf.py file is applied when rendering the documentation pages.\nRequired Sphinx configuration\nsphinx-build\nsphinx-build\nMakefile\nmake.bat\nsphinx-build\nconf.py"
    },
    {
        "objectID": "doc-style/doc-configuration",
        "href": "doc-style/doc-configuration.html#the-confpy-file",
        "title": "Required Sphinx configuration > The conf.py file",
        "section": "The conf.py file",
        "text": "The following conf.py file provides the minimum required configuration for a\nPyAnsys library. To guarantee consistency across PyAnsys libraries, you should\nonly make custom additions on top of this configuration.\nThe conf.py file\nconf.py\nconf.py"
    },
    {
        "objectID": "doc-style/doc-configuration",
        "href": "doc-style/doc-configuration.html#automation-files",
        "title": "Required Sphinx configuration > Automation files",
        "section": "Automation files",
        "text": "As indicated earlier on this page, the sphinx-build program and\nall its options and arguments can be automated by using a\nMakefile file or a make.bat file. These files should be placed at the\nfirst level of the doc directory, next to the source directory.\nNotice that both files contain a SPHINXOPTS variable with these options: -j,\n-W, and --keep-going.\n-j: Indicates the number of jobs (number of cores) to use.\nThe default value is auto, which means that the number of cores in\nthe CPU is to be automatically detected.\n-W: turns warnings into errors. This guarantees that documentation\nhealth is maximized.\n--keep-going: Specifies whether to render the whole documentation,\neven if a warning is found. This option enables developers to be aware of the\nfull set of warnings.\nA special rule named pdf is also included. This rule is in charge of\ngenerating a PDF file for the library’s documentation.\nAutomation files\nsphinx-build\nMakefile\nmake.bat\ndoc\nsource\nSPHINXOPTS\n-j\n-W\n--keep-going\n-j\nauto\n-W\n--keep-going\npdf"
    },
    {
        "objectID": "doc-style/doc-configuration",
        "href": "doc-style/doc-configuration.html#example-for-makefile",
        "title": "Required Sphinx configuration > Example for Makefile",
        "section": "Example for Makefile",
        "text": "The following code collects the required options and automation rules for the\nMakefile program to use when building documentation for a PyAnsys library:\nExample for Makefile\nMakefile\nMakefile"
    },
    {
        "objectID": "doc-style/doc-configuration",
        "href": "doc-style/doc-configuration.html#example-for-makebat",
        "title": "Required Sphinx configuration > Example for make.bat",
        "section": "Example for make.bat",
        "text": "The following code collects the required options and automation rules for the\nmake.bat program to use when building documentation for a PyAnsys project:\nExample for make.bat\nmake.bat\nmake.bat"
    },
    {
        "objectID": "how-to/releasing",
        "href": "how-to/releasing.html#releasing-and-publishing",
        "title": "Releasing and publishing",
        "section": "Releasing and publishing",
        "text": "Releasing a new version is a critical procedure. It should be automated as much\nas possible to avoid human error.\nThis sections explains the Git workflow and steps that you must follow\nto create a successful release.\nA project must be authorized to be publicly released. For an explanation\nof the process, see Project approval and public release.\nReleasing and publishing"
    },
    {
        "objectID": "how-to/releasing",
        "href": "how-to/releasing.html#semantic-versioning",
        "title": "Releasing and publishing > Semantic versioning",
        "section": "Semantic versioning",
        "text": "PyAnsys library releases are managed through both automated and manual review\nprocesses.\nPyAnsys follows Semantic Versioning, which produces release names in the\nform of X.Y.Z, where each letter corresponds to an integer value. This\nnotation can also be understand as MAJOR.MINOR.PATCH:\nA MAJOR version is when you make incompatible API changes.\nA MINOR version is when you add a feature in a backwards-compatible manner.\nA PATCH version is when you make backwards-compatible bug fixes.\nTo match the versioning methodology used by the “big three” data science Python\npackages, NumPy, SciPy, and pandas, MAJOR versions of PyAnsys\npackages are not released when any incompatible API change is made but rather\nwhen major, globally breaking API changes are made.\nNote that 0.MINOR.PATCH packages are expected to have fluid APIs and should\nbe solidified at the 1.MINOR.PATCH release. At that point, APIs are expected\nto be much more stable.\nPyAnsys libraries are expected to be developed outside the product\nrelease cycle in a rapid CI/CD manner. Thus, library versions should\nnot match product versions. For example, PyMAPDL library (ansys-mapdl-core)\nmight have the version 0.59.0 whereas the product (Ansys Parametric\nDesign Language (APDL) might have the version is 22.2 (2022 R2).\nSemantic versioning\nPyAnsys library versions should not match product versions.\nX.Y.Z\nMAJOR.MINOR.PATCH\nMAJOR\nMINOR\nPATCH\nMAJOR\n0.MINOR.PATCH\n1.MINOR.PATCH\nansys-mapdl-core\n0.59.0\n22.2"
    },
    {
        "objectID": "how-to/releasing",
        "href": "how-to/releasing.html#branching-model",
        "title": "Releasing and publishing > Branching model",
        "section": "Branching model",
        "text": "The branching model for a PyAnsys project enables rapid development of\nfeatures without sacrificing stability. The model closely follows the\ntrunk-based development approach:\nThe main branch is the primary development branch. All features,\npatches, and other branches should be merged here. While all PRs\nshould pass all applicable CI checks, this branch might be functionally\nunstable if changes have introduced unintended side effects or bugs\nthat were not caught through unit testing. The version is always suffixed\nwith .dev0 in the main branch.\nWhen a minor release candidate is ready, a new release branch is\ncreated from main with the next incremented minor version\n(for example, release/0.2). This release branch is thoroughly\ntested. When deemed stable, it is tagged with the version (0.2.0\nin this case). Older release branches should not be deleted so that they can be\npatched as needed.\nThere is one or more release/ branches based on minor releases (for\nexample, release/0.2) that contain a stable version of the code base that\nis also reflected on PyPI. Hotfixes from fix/ branches should be\nintegrated both to main and to these branches. When creating a new patch\nrelease is necessary, these release branches have their version updated\nand are tagged with a patched Semantic versioning (for example,\n0.2.1).  This triggers CI to push to PyPI so that hotfixes for past\nversions can be rapidly push without having to worry about untested features.\nBranching model\nmain\n.dev0\nmain\nrelease\nmain\nrelease/0.2\nrelease\n0.2.0\nrelease/\nrelease/0.2\nfix/\nmain\n0.2.1"
    },
    {
        "objectID": "how-to/releasing",
        "href": "how-to/releasing.html#new-releases",
        "title": "Releasing and publishing > New releases",
        "section": "New releases",
        "text": "Releasing is the process of creating a version of the software that developers\nconsider useful for customers or other developers. Releases are usually labeled\nwith tags. These tags are used to quickly identify a release in the version\ncontrol system.\nBefore performing a release, you must verify that your origin main branch is up to date\nwith these commands:\nIf you encounter any issues when running the preceding commands, solve them before\ncontinuing with the release. Ensure that your style, tests, and documentation\nchecks are passing too.\nCreate a new branch for the version that you want to release with this command:\nUpdate X or Y version numbers in your project and replace the dev0\nwith a 0.\nCheck all locations, including\nThe ``setup.py`` file, The ``pyproject.toml`` file, and any\n__init__.py or __version__.py files that your project may contain.\nStage and commit previous changes with these commands:\nTag the previous commit with this command:\nPush the commit and the tag it with these commands:\nPatched versions allow you to fix issues discovered in published releases by\ncherry-picking these fixes from the main branch. For more information, see\nthe get-cherry-pick description\nin the Git documentation.\nBefore performing a patch release, you must first identify which\nrelease/X.Y branch it belongs to with these commands.\nNext, use the following code to cherry-pick the fix commit from the main\nbranch, which solves for the bug. Do not merge changes from the\nmain branch into the release branch. Always cherry-pick them:\nEnsure that your style, tests, and documentation checks are also passing.\nIncrease by one unit the value of Z in your project version. Stage and\namend these new changes with these commands:\nTag the previous commit with this command:\nPush the commit and the tag it using this command:\nNew releases\norigin main\nX\nY\ndev0\n0\n__init__.py\n__version__.py\nmain\nrelease/X.Y\nmain\nmain\nZ"
    },
    {
        "objectID": "how-to/releasing",
        "href": "how-to/releasing.html#artifact-publication",
        "title": "Releasing and publishing > Artifact publication",
        "section": "Artifact publication",
        "text": "When a new version is released, some artifacts are provided with it. In Python,\nthese Artifacts are typically wheel and source files.\nDocumentation in the form of HTML and PDF files are also considered artifacts.\nDo not distribute artifacts without approval.\nA project must be authorized to be publicly released. For an explanation\nof the process, see Project approval and public release.\nThere are three possible places where artifacts can be published:\nThis is a private index used to share artifacts across the company\nwhile making sure that projects remain private.\nThis is the public PyPI used by the Python community to distribute\nlibraries. A project requires Ansys authorization before being\npublished in this index.\nThis is a section created by GitHub within a project repository where\nartifacts can be published. A project requires Ansys authorization\nbefore being public in GitHub.\nArtifact publication"
    },
    {
        "objectID": "how-to/releasing",
        "href": "how-to/releasing.html#private-pypi",
        "title": "Releasing and publishing > Private PyPI",
        "section": "Private PyPI",
        "text": "It is sometimes necessary to host and pull packages that are not ready to be\nhosted on the public PyPI. For example, if a PyAnsys library requires\nauto-generated gRPC interface files from a feature or service that is still\nprivate, this package should be hosted on a private PyPI repository.\nANSYS, Inc. has a private repository at PyAnsys PyPI. You must have the proper\ncredentials for publishing to this private repository:\nCredentials\nValue\nUsername\n__token__\nPassword\nPYANSYS_PYPI_PRIVATE_PAT\nrepository-url\nhttps://pkgs.dev.azure.com/pyansys/_packaging/pyansys/pypi/upload\nThe PYANSYS_PYPI_PRIVATE_PAT is a password in the form of a GitHub secret\nthat is available only to PyAnsys projects. This secret is\navailable during the execution of the CI/CD. Its value is never shown or shared\nin the log files.\nWhen using Twine from the command line, you must\nadd in --repository-url as an extra option. Otherwise, Twine attempts to upload\nthe package to the public PyPI repository.\nForked GitHub repositories do not have access to GitHub secrets. This is\ndesigned to protect against pull requests that could potentially scrape\ntokens from the PyAnsys CI/CD.\nHere’s a cross-platform, one-line command for using Twine to upload a package:\nReplace <TOKEN-REDACTED> with the private PyPI token.\nThe following code lets you publish Python Artifacts in\nthe dist directory to the private PyPI. This code is expected to be included when you\nUse GitHub Actions:\nAlternatively, instead of command-line tool arguments for Twine, you can use environment variables:\nFinally, run this command:\nPrivate PyPI\n__token__\nPYANSYS_PYPI_PRIVATE_PAT\nhttps://pkgs.dev.azure.com/pyansys/_packaging/pyansys/pypi/upload\nPYANSYS_PYPI_PRIVATE_PAT\n--repository-url\n<TOKEN-REDACTED>\ndist"
    },
    {
        "objectID": "how-to/releasing",
        "href": "how-to/releasing.html#public-pypi",
        "title": "Releasing and publishing > Public PyPI",
        "section": "Public PyPI",
        "text": "Publishing Artifacts to PyPI is the way of distributing Python\nlibraries. Before being publicly released, projects must follow the process\nProject approval and public release to obtain public release\nauthorization. Once authorized, contact the\nPyAnsys Core team to get support during the first\nrelease of the project.\nPublishing to PyPI can be performed following the\nTrusted Publisher approach or the\nAPI token approach. When possible, it is recommended\nto use the Trusted Publisher as it provides enhanced security and simplifies\nthe management of authentication credentials. Existing repositories\ncurrently using the API Token approach are encouraged to transition to the\nTrusted Publisher approach to benefit from its security and management\nimprovements.\nPublic PyPI"
    },
    {
        "objectID": "how-to/releasing",
        "href": "how-to/releasing.html#publish-with-trusted-publisher",
        "title": "Releasing and publishing > Publish with trusted publisher",
        "section": "Publish with trusted publisher",
        "text": "Publishing with Trusted Publisher requires an\ninitial setup to configure OIDC trust between PyPI and Github. This action is\nperformed by the PyAnsy core team which adds your\nproject to the list of authorized repositories to release as a Trusted\nPublisher.\nIt is recommended to create en environment in your Github repository to manage\ndeployments. Environments provide a way to configure deployment-specific\nsetting and ensure that sensitive operations are performed in a controller\nmanner. For more information, see the\nEnvironment documentation. Contact the\nPyAnsys Core team  in case of doubts.\nThe following code lets you publish any Python Artifacts contained in\nthe dist directory to the public PyPI. It is expected to be included when you\nUse GitHub Actions.\nPublish with trusted publisher\ndist"
    },
    {
        "objectID": "how-to/releasing",
        "href": "how-to/releasing.html#publish-with-api-token",
        "title": "Releasing and publishing > Publish with API token",
        "section": "Publish with API token",
        "text": "Publishing with API token requires a username and a\npassword:\nCredentials for publishing to public PyPI\nValue\nUsername\n__token__\nPassword\nPYPI_TOKEN\nThe PYPI_TOKEN is a password in the form of a GitHub secret. This secret is\nunique to each project. It can only be obtained after the first release to the\npublic PyPI. The PyAnsys Core team enables the custom\nPYPI_TOKEN once your project has been successfully released for the first\ntime. For future releases, everything is automated.\nHere’s a cross-platform, one-line command for using Twine to download a package:\nReplace <PACKAGE-NAME> and <TOKEN-REDACTED> with the package name and private PyPI token respectively.\nThe following code lets you publish any Python Artifacts contained in\nthe dist directory to the public PyPI. It is expected to be included when you\nUse GitHub Actions.\nPublish with API token\n__token__\nPYPI_TOKEN\nPYPI_TOKEN\nPYPI_TOKEN\n<PACKAGE-NAME>\n<TOKEN-REDACTED>\ndist"
    },
    {
        "objectID": "how-to/releasing",
        "href": "how-to/releasing.html#github",
        "title": "Releasing and publishing > GitHub",
        "section": "GitHub",
        "text": "You can publish Artifacts to GitHub, which makes them available in\nthe https://github.com/ansys/project-name/releases section. The\nvisibility of these artifacts follows the one in the repository. Visibility can\nbe private, internal, or public.\nFor enabling public visibility of a repository, follow the process explained in\nProject approval and public release.\nThe following code lets you publish any Python Artifacts contained in\nthe dist directory to the GitHub release created. It is expected to be included\nwhen you Use GitHub Actions:\nGitHub\nhttps://github.com/ansys/project-name/releases\ndist"
    },
    {
        "objectID": "how-to/releasing",
        "href": "how-to/releasing.html#artifact-download",
        "title": "Releasing and publishing > Artifact download",
        "section": "Artifact download",
        "text": "You can download artifacts from the Ansys private PyPI, public PyPI, and GitHub.\nRequest the value of the PYANSYS_PYPI_PRIVATE_READ_PAT token by sending an\nemail to the pyansys.core@ansys.com email.\nCreate an environment variable named PYANSYS_PYPI_PRIVATE_READ_PAT in your\nlocal machine an assign it the value of the token.\nTake care to always use the --index-url switch rather than the\n--extra-index-url switch. As noted in pip Documentation, the\n--index-url switch changes the Python Package Index, which forces pip\nto use only packages from that package index.\nThe Ansys package index uses PyPI upstream. This prevents other users from being able to\ninject packages from PyPI that would supersede Ansys packages, even if they\nare of a higher version.\nThis is not the case if you use --extra-index-url, which adds to rather\nthan replaces the default package index. For security, do not use\n--extra-index-url.\nDownloading artifacts from the public PyPI can be done by using pip:\nDownloading artifacts from GitHub can be done by checking the\nhttps://github.com/ansys/project-name/releases section.\nNote that if you download the wheel of a Python package, you must manually install\nit with a command like this:\nArtifact download\nPYANSYS_PYPI_PRIVATE_READ_PAT\nPYANSYS_PYPI_PRIVATE_READ_PAT\n--index-url\n--extra-index-url\n--index-url\npip\n--extra-index-url\n--extra-index-url\npip\nhttps://github.com/ansys/project-name/releases"
    },
    {
        "objectID": "coding-style/required-standard",
        "href": "coding-style/required-standard.html#required-standards",
        "title": "Required standards",
        "section": "Required standards",
        "text": "This page collects the required standards for any PyAnsys project. The\nindividual configurations for the tools presented in Code style tools and\nDocumentation style tools are combined together.\nThe following lines should be included in The ``pyproject.toml`` file\nto indicate the configuration of the different code and documentation style tools.\nRequired standards\nPyAnsys"
    },
    {
        "objectID": "coding-style/required-standard",
        "href": "coding-style/required-standard.html#required-pyprojecttoml-file-configuration",
        "title": "Required standards > Required pyproject.toml file configuration",
        "section": "Required pyproject.toml file configuration",
        "text": "Required pyproject.toml file configuration\npyproject.toml"
    },
    {
        "objectID": "coding-style/required-standard",
        "href": "coding-style/required-standard.html#required-flake8-configuration",
        "title": "Required standards > Required flake8 configuration",
        "section": "Required flake8 configuration",
        "text": "The following .flake8 file is also required:\nRequired flake8 configuration\n.flake8"
    },
    {
        "objectID": "coding-style/required-standard",
        "href": "coding-style/required-standard.html#required-pre-commit-configuration",
        "title": "Required standards > Required pre-commit configuration",
        "section": "Required pre-commit configuration",
        "text": "You can take advantage of pre-commit by including a\n.pre-commit-config.yaml file like this one in your project:\nRequired pre-commit configuration\npre-commit\n.pre-commit-config.yaml"
    },
    {
        "objectID": "coding-style/required-standard",
        "href": "coding-style/required-standard.html#github-cicd-integration",
        "title": "Required standards > GitHub CI/CD integration",
        "section": "GitHub CI/CD integration",
        "text": "Finally, you can Test using GitHub actions and\ncreate a style.yml workflow file in the .github/workflows\ndirectory:\nGitHub CI/CD integration\nstyle.yml\n.github/workflows"
    },
    {
        "objectID": "coding-style/index",
        "href": "coding-style/index.html#coding-style",
        "title": "Coding style",
        "section": "Coding style",
        "text": "Coding style refers to the different rules defined in a software project that\nmust be followed when writing source code. These rules ensure that all\nsource code looks the same across different files of the project.\nBecause the PyAnsys ecosystem consists of many projects, coding style rules\nare critical. Their use helps to achieve these goals:\nPrevent against common programming errors\nLimit product complexity\nProvide an easily readable, understandable, and maintainable product\nEstablish a consistent style\nImplement an objective basis for code review\nAll PyAnsys libraries are expected to follow PEP 8 and be consistent in style and\nformatting with the libraries for the “big three” data science packages: NumPy,\nSciPy, and pandas.\nThe purpose of this section is not to repeat coding style documentation\nbut rather to describe coding best practices applicable to the PyAnsys project when there are any\ndelineations, clarifications, or additional procedures above and\nbeyond PEP 8. For example, this section provides a topic on deprecation\nbest practices because there is no official guidance on deprecating features\nwithin Python.\nCoding style"
    },
    {
        "objectID": "packaging/index",
        "href": "packaging/index.html#packaging-style",
        "title": "Packaging style",
        "section": "Packaging style",
        "text": "A PyAnsys library eliminates the need to share code snippets for\nperforming actions. You can instead create workflows consisting of\nPython modules and third-party libraries. This extends Ansys products\nin a way that matches how libraries are created in the Python community\nwhile maintaining the separation between products, APIs, and PyAnsys\nclient libraries.\nTo avoid the anti-pattern of providing single-use scripts, the\ngeneral pattern for a PyAnsys library provides these features:\nClear, GitHub-hosted open source APIs that are consistent with community\nstandards\nReusable packages that can be updated and patched outside of the\nAnsys release schedule, while still being directly dependent on\nAnsys products\nUnit testing, release packaging, and documentation\nThis diagram shows the general pattern that each PyAnsys library should follow:\nThe Ansys product or service exposes an interface that is locally\naccessible (for example, .NET using pythoncom, SWIG, or C\nextensions) or a service that is both locally and remotely\naccessible using REST or gRPC. This interface is referred to as the\nAPI (Application Programming Interface).  While this API can be\ndirectly accessed, this often results in unreadable and unmaintainable\ncode that forces users to rewrite setup boilerplate and other methods\nfrom scratch. Therefore, the best practice is to create a Python layer\nthat maps the raw API into a carefully designed, object-oriented data\nmodel and API.\nPackaging style"
    },
    {
        "objectID": "content-writing/content-how-tos/clone-branch",
        "href": "content-writing/content-how-tos/clone-branch.html#clone-and-branch-a-repository",
        "title": "Clone and branch a repository",
        "section": "Clone and branch a repository",
        "text": "This page describes how to clone (download) a GitHub repository and then provides\nconceptual information on how you use your Git tool to create a local branch where\nyou modify or add to the codebase, eventually submitting your suggested changes in\na PR to the GitHub repository. For Git-related procedural information, see the\ndocumentation for your preferred Git tool.\nClone and branch a repository"
    },
    {
        "objectID": "content-writing/content-how-tos/clone-branch",
        "href": "content-writing/content-how-tos/clone-branch.html#clone-a-repository",
        "title": "Clone and branch a repository > Clone a repository",
        "section": "Clone a repository",
        "text": "Before you can contribute to a PyAnsys project, you must clone the GitHub\nrepository.\nCopy the URL for the repository.\nIf the Ansys Python Manager and the Administrator window are not\nopen, open them. For more information, see Ansys_Python_Manager.\nIn the Administrator window, use the cd command to go to the\ndirectory where you want to clone the repository.\nFor example, run a command like this one:\nRun this git command, where <url> is the URL for the repository:\nFor example, run a command like this one:\nClone a repository\ncd\ngit\n<url>"
    },
    {
        "objectID": "content-writing/content-how-tos/clone-branch",
        "href": "content-writing/content-how-tos/clone-branch.html#pull-changes-from-the-main-branch-on-github",
        "title": "Clone and branch a repository > Pull changes from the main branch on GitHub",
        "section": "Pull changes from the main branch on GitHub",
        "text": "Once you have a clone of the repository, before creating a local branch to work\nin, ensure that you have the latest codebase. Using your Git tool, pull changes\nfrom the remote main branch on GitHub into the main branch of your locally cloned\nrepository.\nPull changes from the main branch on GitHub"
    },
    {
        "objectID": "content-writing/content-how-tos/clone-branch",
        "href": "content-writing/content-how-tos/clone-branch.html#create-a-local-branch",
        "title": "Clone and branch a repository > Create a local branch",
        "section": "Create a local branch",
        "text": "Once your clone of the repository is up to date, use your Git tool to create a local\nbranch to make changes in. For changes related to documentation, your branch name\nshould begin with doc/ followed by a descriptive name. For example, you might\nname your branch doc/overall_review if that describes your suggested changes.\nYou might name your branch doc/hfss_py_edits if your are suggesting changes\nto that particular PY file. For more information on naming branches, see\nbranch_naming.\nCreate a local branch\ndoc/\ndoc/overall_review\ndoc/hfss_py_edits"
    },
    {
        "objectID": "content-writing/content-how-tos/clone-branch",
        "href": "content-writing/content-how-tos/clone-branch.html#commit-local-changes-and-pull-remote-changes",
        "title": "Clone and branch a repository > Commit local changes and pull remote changes",
        "section": "Commit local changes and pull remote changes",
        "text": "As you work in your local branch, periodically commit your changes and\npull changes from the remote main branch to ensure that no changes\nthere conflict with changes that you have made in your local branch. If conflicts do exist,\nresolve them in your local branch before pushing your changes to a PR.\nCommit local changes and pull remote changes"
    },
    {
        "objectID": "content-writing/content-how-tos/clone-branch",
        "href": "content-writing/content-how-tos/clone-branch.html#push-changes-to-a-pr",
        "title": "Clone and branch a repository > Push changes to a PR",
        "section": "Push changes to a PR",
        "text": "When you are ready to push your changes to a PR, first pull changes again\nfrom the remote main branch to your local branch. Providing that there are no\nconflicts to resolve, push your changes to create a PR or add to an existing\nPR. For more information, see create_pr. Ensure that all checks run by\nthe CI/CD process are successful. If any checks fail, see resolve_failing_checks.\nPush changes to a PR"
    },
    {
        "objectID": "content-writing/content-how-tos/create-issues-discussions",
        "href": "content-writing/content-how-tos/create-issues-discussions.html#create-issues-and-discussions",
        "title": "Create issues and discussions",
        "section": "Create issues and discussions",
        "text": "Content contributors can be active participants in enhancing and adding to PyAnsys libraries.\nIn addition to reviewing and creating PRs, you can create issues and discussions\non the Issues page and Discussions page of a GitHub repository.\nIssues report, track, and manage bugs, feature requests, and other project-related\ntasks. Issues help to organize development efforts and prioritize work. Once an\nissue is created, you and others can leave comments to provide additional information,\nsolutions, and progress updates.\nDiscussions foster communication and interaction among community members, contributors,\nand project maintainers. Discussions help to centralize  conversations on particular topics,\nsuch as general usage questions or product announcements. You can also use discussions to\nfacilitate brainstorming, planning, and the gathering of feedback.\nCreate issues and discussions"
    },
    {
        "objectID": "content-writing/content-how-tos/create-issues-discussions",
        "href": "content-writing/content-how-tos/create-issues-discussions.html#create-an-issue",
        "title": "Create issues and discussions > Create an issue",
        "section": "Create an issue",
        "text": "To report an issue, request a new feature, or ask for project-specific troubleshooting assistance,\ncreate an issue in the project’s repository:\nGo to the project repository.\nClick the Issues tab to go to the Issues page.\nIn the upper right corner, click New Issue.\nIf the page that opens lists templates, click Get Started for the\nappropriate template.\nIn most repositories, these issue templates are available:\nBug, problem, error: For filing a bug report\nDocumentation issue: For requesting modifications to the documentation\nAdding an example for Python project: For proposing a new example for a client library\nNew feature: For requesting enhancements to the code\nIf your issue does not fit one of these templates, you can click the Open a blank issue\nlink.\nComplete the form that is shown, providing a clear and detailed description of the issue. Include\nany relevant error messages, code snippets, and screenshots.\nClick Submit new issue.\nIf you have general questions about the PyAnsy ecosystem, contact the\nPyAnsy core team.\nCreate an issue"
    },
    {
        "objectID": "content-writing/content-how-tos/create-issues-discussions",
        "href": "content-writing/content-how-tos/create-issues-discussions.html#create-a-discussion",
        "title": "Create issues and discussions > Create a discussion",
        "section": "Create a discussion",
        "text": "To post questions, share ideas, and get community feedback, create a discussion in\nthe project’s repository:\nGo to the project repository.\nClick the Discussions tab to go to the Discussions page.\nIn some repositories, the Discussions page is not available because the\nproject maintainers want you to always use the Discussions\npage on the Ansys Developer portal instead.\nIn the upper right corner, click New discussion.\nIf the page that opens lists templates, click Get Started for the\nappropriate template.\nIn most repositories, these discussion templates are available:\nAnnouncements: For filing a bug report\nGeneral: For chatting about anything and everything\nIdeas: For sharing ideas for new features\nPolls: For taking a vote from the community\nQ&A: For asking the community for help\nShow and tell: For showing off something that you’ve made\nComplete the form, providing a clear and detailed title and description.\nClick Start discussion.\nCreate a discussion"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#pep-8",
        "title": "PEP 8",
        "section": "PEP 8",
        "text": "This section summarizes important coding style guidelines from PEP 8\nand how they apply to PyAnsys libraries. The Python community devised PEP 8\nto increase the readability of Python code. Some of the most popular\npackages within the Python ecosystem have adopted PEP 8,\nincluding NumPy, SciPy, and pandas.\nPEP 8"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#imports",
        "title": "PEP 8 > Imports",
        "section": "Imports",
        "text": "Code style guidelines follow for import statements.\nImports\nimport"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#import-location",
        "title": "PEP 8 > Import location",
        "section": "Import location",
        "text": "Imports should always be placed at the top of the file, just after any\nmodule comments and docstrings and before module global variables and\nconstants. This reduces the likelihood of an ImportError that\nmight only be discovered during runtime.\nImport location"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#import-order",
        "title": "PEP 8 > Import order",
        "section": "Import order",
        "text": "For better readability, group imports in this order:\nStandard library imports\nRelated third-party imports\nLocal app-specific or library-specific imports\nAll imports within each import grouping should be performed in alphabetical order\nso that they are easily searchable.\nImport order"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#multiple-imports",
        "title": "PEP 8 > Multiple imports",
        "section": "Multiple imports",
        "text": "You should place imports on separate lines unless they are modules from the same\npackage.\nMultiple imports"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#absolute-versus-relative-imports",
        "title": "PEP 8 > Absolute versus relative imports",
        "section": "Absolute versus relative imports",
        "text": "You should use absolute imports over relative imports because they are\nmore readable and reliable.\nAbsolute versus relative imports"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#import-namespaces",
        "title": "PEP 8 > Import namespaces",
        "section": "Import namespaces",
        "text": "You should avoid using wildcards in imports because doing so can make it\ndifficult to detect undefined names. For more information, see using wildcard imports (from … import *).\nin the Python Anti-Patterns documentation.\nImport namespaces"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#naming-conventions",
        "title": "PEP 8 > Naming conventions",
        "section": "Naming conventions",
        "text": "To achieve readable and maintainable code, use concise and descriptive names for functions,\nclasses, methods, and constants. Regardless of the programming language, you must follow these\nglobal rules to determine the correct names:\nChoose descriptive and unambiguous names.\nMake meaningful distinctions.\nUse pronounceable names.\nUse searchable names.\nReplace magic numbers with named constants.\nAvoid encodings. Do not append prefixes or type information.\nNaming conventions"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#variables",
        "title": "PEP 8 > Variables",
        "section": "Variables",
        "text": "Do not use the characters \"l\", \"O\", or \"I\" as single-character\nvariable names. In some fonts, these characters are indistinguishable from the\nnumerals one and zero.\nVariables\n\"l\"\n\"O\"\n\"I\""
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#packages-and-modules",
        "title": "PEP 8 > Packages and modules",
        "section": "Packages and modules",
        "text": "Use a short, lowercase word or words for module names. Separate words\nwith underscores to improve readability. For example, use module.py\nor my_module.py.\nFor a package name, use a short, lowercase word or words. Avoid\nunderscores as these must be represented as dashes when installing\nfrom PyPI.\nPackages and modules\nmodule.py\nmy_module.py"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#classes",
        "title": "PEP 8 > Classes",
        "section": "Classes",
        "text": "Use camel case when naming\nclasses. Do not separate words with underscores.\nClasses"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#functions-and-methods",
        "title": "PEP 8 > Functions and methods",
        "section": "Functions and methods",
        "text": "Use a lowercase word or words when naming Python functions or methods. To\nimprove readability, separate words with underscores.\nWhen naming methods, follow these conventions:\nEnclose only dunder methods with double underscores.\nStart a method that is to be private with double underscores.\nStart a method that is to be protected with a single underscore.\nRemember that these are only conventions for naming functions and methods. In Python,\nthere are no private or protected members, meaning that you can always access even\nthose members that start with underscores.\nFunctions and methods"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#variables",
        "title": "PEP 8 > Variables",
        "section": "Variables",
        "text": "Use a lowercase single letter, word, or words when naming variables. To improve\nreadability, separate words with underscores.\nConstants are variables that are set at the module level and are used by one or\nmore methods within that module. Use an uppercase word or words for constants.\nTo improve readability, separate words with underscores.\nVariables"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#indentation-and-line-breaks",
        "title": "PEP 8 > Indentation and line breaks",
        "section": "Indentation and line breaks",
        "text": "Proper and consistent indentation is important to producing\neasy-to-read and maintainable code. In Python, use four spaces per\nindentation level and avoid tabs.\nIndentation should be used to emphasize:\nBody of a control statement, such as a loop or a select statement\nBody of a conditional statement\nNew scope blocks\nTo improve readability, add blank lines and wrap lines. You\nshould add two blank lines before and after all function and class\ndefinitions.\nInside a class, add a single blank line before any method definition.\nTo make it clear when a “paragraph” of code is complete and a new section\nis starting, use a blank line to separate logical sections.\nIndentation and line breaks"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#maximum-line-length",
        "title": "PEP 8 > Maximum line length",
        "section": "Maximum line length",
        "text": "For source code, best practice is to keep the line length at or below\n100 characters. For docstrings and comments, best practice is to keep\nthe length at or below 72 characters.\nLines longer than these recommended limits might not display properly\non some terminals, and tools or might be difficult to follow. For example,\nthis line is difficult to follow:\nAlternatively, instead of writing a list comprehension, you can use a\nclassic loop.\nNotice that sometimes it is not be possible to keep the line length below the\ndesired value without breaking the syntax rules.\nMaximum line length"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#comments",
        "title": "PEP 8 > Comments",
        "section": "Comments",
        "text": "Because a PyAnsys library generally involves multiple physics domains,\npeople reading its source code do not have the same background as\nthe developers who wrote it. This is why it is important for a library\nto have well commented and documented source code. Comments that\ncontradict the code are worse than no comments. Always make a priority\nof keeping comments up to date with the code.\nComments should be complete sentences. The first word should be\ncapitalized, unless it is an identifier that begins with a lowercase\nletter.\nHere are general guidelines for writing comments:\nAlways try to explain yourself in code by making it\nself-documenting with clear variable names.\nDon’t be redundant.\nDon’t add obvious noise.\nDon’t use closing brace comments.\nDon’t comment out code that is unused. Remove it.\nUse explanations of intent.\nClarify the code.\nWarn of consequences.\nObvious portions of the source code should not be commented.\nFor example, the following comment is not needed:\nHowever, if code behavior is not apparent, it should be documented.\nOtherwise, future developers might remove code that they see as unnecessary.\nComments"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#inline-comments",
        "title": "PEP 8 > Inline comments",
        "section": "Inline comments",
        "text": "Use inline comments sparingly. An inline comment is a comment on the\nsame line as a statement.\nInline comments should be separated by two spaces from the statement:\nInline comments that state the obvious are distracting and should be\navoided:\nFocus on writing self-documenting code and using short but\ndescriptive variable names.\nInline comments"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#docstrings",
        "title": "PEP 8 > Docstrings",
        "section": "Docstrings",
        "text": "A docstring is a string literal that occurs as the first statement in\na module, function, class, or method definition. A docstring becomes\nthe doc special attribute of the object.\nWrite docstrings for all public modules, functions, classes, and\nmethods. Docstrings are not necessary for private methods, but such\nmethods should have comments that describe what they do.\nTo create a docstring, surround the comments with three double quotation marks\non either side.\nFor a one-line docstring, keep both the starting and ending \"\"\" on the\nsame line:\nFor a multi-line docstring, put the ending \"\"\" on a line by itself.\nFor more information on docstrings for PyAnsys libraries, see\nDocumentation style.\nDocstrings\n\"\"\"\n\"\"\""
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#programming-recommendations",
        "title": "PEP 8 > Programming recommendations",
        "section": "Programming recommendations",
        "text": "The following sections provide some PEP 8 recommendations for removing\nambiguity and preserving consistency. Additionally, they address some common\npitfalls that occur when writing Python code.\nProgramming recommendations"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#booleans-and-comparisons",
        "title": "PEP 8 > Booleans and comparisons",
        "section": "Booleans and comparisons",
        "text": "Don’t compare Boolean values to True or False using the\nequivalence operator.\nBecause empty sequences are evaluated to False, don’t compare the\nlength of these objects but rather consider how they would evaluate\nby using bool(<object>).\nIn if statements, use is not rather than not ....\nAlso, avoid if x: when you mean if x is not None:.  This is\nespecially important when parsing arguments.\nBooleans and comparisons\nTrue\nFalse\nFalse\nbool(<object>)\nif\nis not\nnot ...\nif x:\nif x is not None:"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#handling-strings",
        "title": "PEP 8 > Handling strings",
        "section": "Handling strings",
        "text": "Use the .startswith() and .endswith() functions instead of slicing.\nHandling strings\n.startswith()\n.endswith()"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#reading-the-windows-registry",
        "title": "PEP 8 > Reading the Windows registry",
        "section": "Reading the Windows registry",
        "text": "Never read the Windows registry or write to it because this is dangerous and\nmakes it difficult to deploy libraries on different environments or operating\nsystems.\nReading the Windows registry"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#duplicated-code",
        "title": "PEP 8 > Duplicated code",
        "section": "Duplicated code",
        "text": "Follow the DRY principle, which states that “Every piece of knowledge\nmust have a single, unambiguous, authoritative representation within a\nsystem.”  Follow this principle unless it overly complicates\nthe code. The following “Avoid” example converts Fahrenheit to Kelvin\ntwice, which now requires the developer to maintain two separate lines\nthat do the same thing.\nThis is a trivial example, but you can apply this approach for a\nvariety of both simple and complex algorithms and workflows. Another\nadvantage of this approach is that you can implement unit testing\nfor this method.\nNow, you have only one line of code to verify. You can also use\na testing framework such as pytest to test that the method is\ncorrect.\nDuplicated code"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#nested-blocks",
        "title": "PEP 8 > Nested blocks",
        "section": "Nested blocks",
        "text": "Avoid deeply nested block structures (such as conditional blocks and loops)\nwithin one single code block.\nAside from the lack of comments, this complex method\nis difficult to debug and validate with unit testing. It would\nbe far better to implement more validation methods and join conditional\nblocks.\nFor a conditional block, the maximum depth recommended is four. If you\nthink you need more for the algorithm, create small functions that are\nreusable and unit-testable.\nNested blocks"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#loops",
        "title": "PEP 8 > Loops",
        "section": "Loops",
        "text": "While there is nothing inherently wrong with nested loops, to avoid\ncertain pitfalls, steer clear of having loops with more than two levels. In\nsome cases, you can rely on coding mechanisms like list comprehensions\nto circumvent nested loops.\nIf the loop is too complicated for creating a list comprehension,\nconsider creating small functions and calling these instead. Assume\nthat you want to extract all consonants in a sentence.\nThe “Use” approach is more readable and better documented. Additionally,\nyou could implement a unit test for is_consonant.\nLoops\nis_consonant"
    },
    {
        "objectID": "coding-style/pep8",
        "href": "coding-style/pep8.html#security-considerations",
        "title": "PEP 8 > Security considerations",
        "section": "Security considerations",
        "text": "Security, an ongoing process involving people and practices, ensures app confidentiality, integrity, and availability 1.\nAny library should be secure and implement good practices that avoid or mitigate possible security risks.\nThis is especially relevant in libraries that request user input (such as web services).\nBecause security is a broad topic, you should review this useful Python-specific resource:\n10 Unknown Security Pitfalls for Python - By Dennis Brinkrolf - Sonar source blog\nWikipedia - Software development security.\nSecurity considerations"
    },
    {
        "objectID": "content-writing/py-files-writers/docstring-format-rules",
        "href": "content-writing/py-files-writers/docstring-format-rules.html#docstring-formatting-rules",
        "title": "Docstring formatting rules",
        "section": "Docstring formatting rules",
        "text": "In the PY files for PyAnsys libraries, docstrings always start and\nend with triple quotation marks (\"\"\"). Within a docstring, content is\nformatted similarly to RST files.\nThe first docstring in a PY file provides a short summary of the module\nitself. For short summaries to fit on one line in the library’s API reference\nsummary tables, you must keep the line lengths of your docstrings\nto no more than 100 characters. This is different than the maximum line length\nfor source code, which might be set to as many as 120 characters.\nMost of the time, a module contains only a single class. This page summarizes\nPyAnsys-specific formatting rules for the sections more commonly found in a\ndocstring for a class or one of its methods:\nShort summary\nDeprecation warning\nExtended summary\nParameters\nReturns\nRaises\nExamples\nIn most PyAnsys libraries, the docstrings in PY files do not contain these sections:\nYields\nReceives\nOther Parameters\nWarns\nWarnings\nSee Also\nNotes\nReferences\nIf a docstring in a PY file contains one of these sections,\nsee Sections in the numpydoc Style guide\nfor information on how to format its content. This page also indicates the order\nof all sections possible in a docstring.\nDocstring sections are always contained within the triple quotation marks that denote\na docstring. The sections more commonly found in PyAnsys docstrings must be in\nthe order indicated in this page’s right pane.\nSome of the docstrings shown on this page display ellipses to indicate\nthat they provide additional sections.\nYou might want to open one of the larger PY files for your PyAnsys library\nand look at its docstrings while reading about how they are formatted. These\nPY files are typically in the src directory.\nDocstring formatting rules\n\"\"\"\nsrc"
    },
    {
        "objectID": "content-writing/py-files-writers/docstring-format-rules",
        "href": "content-writing/py-files-writers/docstring-format-rules.html#short-summary",
        "title": "Docstring formatting rules > Short summary",
        "section": "Short summary",
        "text": "This unnamed docstring section is a one-line summary written in plain English. This\nsingle line immediately follows the declaration of a function, class, or method to\nbriefly describe what it does. A short summary for these Python objects is mandatory.\nIf it is not present, documentation style tools in the CI/CD process raise errors\nwhen you push changes to a PR.\nYou can declare the short summary on the same line as the opening quotation marks\nof the docstring or on the next line. While PEP 257 accepts both ways,\ndocstrings must be consistent across your project. If the developers of a PyAnsys\nlibrary are declaring the short summary on the same line as the opening quotation marks,\nthey have turned off \"GL01\" checking in the numpydoc_validation_checks dictionary\nof the numpydoc extension. For more information on documentation style tools and the\nvalidation of NumPy-style docstrings, see doc_style_tools_>.\nThe short summary should not use code entities to refer to the names of Python\nobjects unless absolutely necessary. This is because the use of code entities\nreduces readability of the short summary.\nBecause the short summary cannot exceed the maximum line length of 100 characters,\nuse a sentence fragment that starts with a verb and ends with a period. If additional\ninformation is needed to clarify what the function, class, or method does, provide\nthis information in the docstring_extended_summary.\nIn Python, functions are not defined within a class but rather perform actions or\noperations on collections (lists, tuples, dictionaries, and sets). Methods, which\nare functions defined within a class, are associated with instances of that class.\nThey perform actions or operations related to that class. While the subsequent content\nfocuses on docstrings for classes and methods, docstrings for functions are\nbasically formatted the same as those for methods.\nShort summary\n\"GL01\"\nnumpydoc_validation_checks\nnumpydoc"
    },
    {
        "objectID": "content-writing/py-files-writers/docstring-format-rules",
        "href": "content-writing/py-files-writers/docstring-format-rules.html#short-summary-for-a-class",
        "title": "Docstring formatting rules > Short summary for a class",
        "section": "Short summary for a class",
        "text": "A class is a noun representing a collection of methods. For consistency within PyAnsys libraries,\nstart the short summary for a class with a verb ending in “s” or “es” so that the summary table\nof the classes in a module have consistently formatted descriptions:\nInitializing an instance of a class often requires specifying parameters, which are indicated\nin a def __init__ definition. These parameters are described in the docstring_parameters\nsection of the docstring for the class.\nShort summary for a class\ndef __init__"
    },
    {
        "objectID": "content-writing/py-files-writers/docstring-format-rules",
        "href": "content-writing/py-files-writers/docstring-format-rules.html#short-summary-for-a-method",
        "title": "Docstring formatting rules > Short summary for a method",
        "section": "Short summary for a method",
        "text": "A method is a verb representing an action that can be performed. For consistency\nwithin PyAnsys libraries, start the short summary for a method with a verb not ending\nin “s” or “es” so that the summary table of the methods for a class have consistently\nformatted descriptions:\nUsing a method almost always requires specifying parameters, which are indicated in parentheses\nin the method’s definition. These parameters, except for the self parameter, are always described\nin the docstring_parameters section for the method. The self parameter does not have to\nbe documented because it is a reference to the instance of the parent class (and its properties)\nthat the method is being called on.\nMethods with a leading underscore (_) are protected methods, meaning that they are not\nrendered in the documentation unless an explicit request is made to add them using Sphinx\ndirectives. The plus side to this is that docstrings for protected methods can be more\ndeveloper-focused. However, writing clear docstrings for protected methods is still\nimportant.\nIf a method has an @exceperty decorator, it means that it has no parameters. Thus,\nyou can remove the “Parameters” section from the docstring for this method.\nIf a method has an @property decorator, it is turned into a property, which must be described\nas a noun rather than a verb. Because the resulting property cannot have parameters, it does\nnot have a “Parameters” section. If a setter follows the @property decorator, do not\nadd a docstring for the setter. A setter simply exposes both the GET and SET methods rather\nthan only the GET method. Developers should include examples to show how to use the GET and SET\nmethods if necessary. A “Returns” section is only included if the property calculates\nand returns a result. Otherwise, the description should clearly explain the value that\nis returned.\nShort summary for a method\nself\nself\n_\n@exceperty\n@property\nsetter\n@property\nsetter"
    },
    {
        "objectID": "content-writing/py-files-writers/docstring-format-rules",
        "href": "content-writing/py-files-writers/docstring-format-rules.html#deprecation-warning",
        "title": "Docstring formatting rules > Deprecation warning",
        "section": "Deprecation warning",
        "text": "This unnamed docstring section follows the short summary only if the Python object is being\ndeprecated or has been deprecated. It consists of a deprecated directive that warns\nusers about when this object is to be removed (or was removed) from the API. The\ndeprecated directive gives a reason for the deprecation, such as the\nobject is superseded or duplicates functionality found elsewhere. Lastly, it recommends\nhow to obtain the same functionality.\nHere is an example of a PyAEDT method with a deprecated directive. It indicates\nthe version for the deprecation and explains that this method is superseded by functionality\nin another method. It uses the func role to link to the method that should be used:\nDeprecation warning\ndeprecated\ndeprecated\ndeprecated\nfunc"
    },
    {
        "objectID": "content-writing/py-files-writers/docstring-format-rules",
        "href": "content-writing/py-files-writers/docstring-format-rules.html#extended-summary",
        "title": "Docstring formatting rules > Extended summary",
        "section": "Extended summary",
        "text": "If the short summary does not clearly and fully explain the functionality of the object,\nthis unnamed docstring section provides the additional information that is needed in\ncomplete sentences. A blank line must always be inserted before and after the extended\nsummary.\nWhile you can use inline code entities in the extended summary, you should not describe\nany named objects that are parameters here in this section because they are described in\nthe subsequent “Parameters” section. You should place any needed implementation information\nor background theory in a “Notes” section. For more information, see\nSections in the numpydoc Style guide.\nExtended summary"
    },
    {
        "objectID": "content-writing/py-files-writers/docstring-format-rules",
        "href": "content-writing/py-files-writers/docstring-format-rules.html#parameters",
        "title": "Docstring formatting rules > Parameters",
        "section": "Parameters",
        "text": "This named docstring section describes the parameters listed in the definition\nof an instance method. The first parameter in the definition is self by convention.\nAs explained earlier, it represents the instance of the class that a method is being\ncalled on. The other parameters listed in the definition pass input data. In the “Parameters”\nsection, all parameters except for self must be documented in the order in which they appear\nin the definition.\nParameters\nself\nself"
    },
    {
        "objectID": "content-writing/py-files-writers/docstring-format-rules",
        "href": "content-writing/py-files-writers/docstring-format-rules.html#parameters-for-initializing-an-instance-of-a-class",
        "title": "Docstring formatting rules > Parameters for initializing an instance of a class",
        "section": "Parameters for initializing an instance of a class",
        "text": "You can find the parameters for initializing a class in an __init__ definition.\nHere is the __init__ definition for the PyAEDT Emit class:\nThe parameters for this class are defined in the “Parameters” section like this:\nParameters for initializing an instance of a class\n__init__\n__init__\nEmit"
    },
    {
        "objectID": "content-writing/py-files-writers/docstring-format-rules",
        "href": "content-writing/py-files-writers/docstring-format-rules.html#parameters-for-a-function-or-method",
        "title": "Docstring formatting rules > Parameters for a function or method",
        "section": "Parameters for a function or method",
        "text": "You can find the parameters for a function or method in parentheses in its definition\n(function signature). Here is the definition for the add_sweep method in the PyAEDT\nSolveSetup.py file:\nThe parameters for this method are defined in the “Parameters” section like this:\nFor the first parameter, the behavior that occurs when the default of None is used\nis unclear. For the second parameter, no options other than the default are given.\nBecause the goal is to have well written and consistently formatted docstrings, when\nsubmitting suggested changes in a PR, you would want to add comments like these to the\nparameter descriptions:\nFor the sweepname parameter, what behavior occurs when the default of\nNone is used?\nFor the sweeptype parameter, what are all the options so that they\ncan be listed in the description alphabetically in either a sentence or itemized list?\nFor information on making a comment when reviewing a PR, see add_comment_on_line.\nParameters for a function or method\nadd_sweep\nSolveSetup.py\nNone\nsweepname\nNone\nsweeptype"
    },
    {
        "objectID": "content-writing/py-files-writers/docstring-format-rules",
        "href": "content-writing/py-files-writers/docstring-format-rules.html#parameter-formatting",
        "title": "Docstring formatting rules > Parameter formatting",
        "section": "Parameter formatting",
        "text": "The first line for each parameter provides the name and data type and indicates\nif specifying a value is optional. Always follow the parameter name with a space,\na colon, and a space. Next, specify the data type of the parameter, being as precise\nas possible.\nParameter formatting"
    },
    {
        "objectID": "content-writing/py-files-writers/docstring-format-rules",
        "href": "content-writing/py-files-writers/docstring-format-rules.html#parameter-data-types",
        "title": "Docstring formatting rules > Parameter data types",
        "section": "Parameter data types",
        "text": "The preceding examples show the str, bool, int, and list data types.\nAdditional common data types include float, dict, and tuple. For more\ninformation, see py_file_primitive_data_types and py_file_collections.\nBecause your PyAnsys project might support other data types, consult with your developers\nbefore making any changes to them.\nHere are some guidelines to follow when specifying the one or more data types that a parameter\nsupports as inputs:\nFor a parameter with a numerical default, let the developer set the data type. While\nit seems intuitive that a numerical default with a decimal point is a float, a float value\nmight accept an integer (and vice versa).\nWhen the code shows that a parameter is being converted to a string with str(rjc), the\ndata type can be a string, float, or integer. You can format these multiple data types as\nindicated in the next bullet.\nWhen a parameter supports multiple data types, place the word “or” between each type:\nParameter data types\nstr\nbool\nint\nlist\nfloat\ndict\ntuple\nstr(rjc)"
    },
    {
        "objectID": "content-writing/py-files-writers/docstring-format-rules",
        "href": "content-writing/py-files-writers/docstring-format-rules.html#optional-parameters",
        "title": "Docstring formatting rules > Optional parameters",
        "section": "Optional parameters",
        "text": "A parameter is optional if a default is shown in the definition. If no value is programmatically\nspecified for the parameter, the default is used. PyAnsys libraries use two different methods\nfor providing the default for an optional parameter.\nIn the PY files for most projects, the data type is followed by a comma and optional, which is\nthe method used in the two “Parameters” sections shown earlier. Following the short summary of the\nparameter, a complete sentence then provides the default.\nHowever, recent extension enhancements support placing the default after the data type, which\neliminates the need for a sentence indicating what the default is (unless the behavior that occurs\nwhen this default is used is unclear). Here is a “Parameters” section that uses this second method:\nProjects using the older optional method might eventually want to migrate to this newer\nmethod to reduce the length of many of their parameter descriptions.\nOptional parameters\noptional\noptional"
    },
    {
        "objectID": "content-writing/py-files-writers/docstring-format-rules",
        "href": "content-writing/py-files-writers/docstring-format-rules.html#parameter-descriptions",
        "title": "Docstring formatting rules > Parameter descriptions",
        "section": "Parameter descriptions",
        "text": "When writing the description for a parameter, always follow these rules, referring\nback to them as needed:\nIndent the parameter’s short summary and all subsequent sentences four spaces.\nFor the short summary, use a sentence fragment that omits a leading article\n(such as “A,” “An,” or “The”) and conclude this fragment with a period. Although omitting the\narticle contradicts the Articles guideline in the Google style\nguide, removing them at the beginning of short summaries here and in other docstring sections\nensures that the first word is an important descriptor.\nEnd the short summary (and complete sentences) with prepositions if it improves readability.\nFor example, “Frequency to set the adaptive convergence at” is more readable than\n“Frequency at which to set the adaptive convergence.”\nAfter the short summary, use complete sentences, including articles, to provide additional\ninformation.\nWhen a sentence is used to specify the default, this sentence should immediately follow the\nshort summary. If other possible options are not evident, begin the next sentence with an\n“Options are” phrase and then specify all options, including the default, in alphabetical\norder. If there are many options, consider formatting the options in a bulleted list. Or,\nin situations where listing specific options is not practical or necessary, format the\nparameter description similarly to this one:\nWhen specifying the default for a string parameter, surround the default in both\ndouble backticks (``) and double quotation marks (\"):\nWhen the default for a string parameter is None, surround the default only in\ndouble backticks because None has programmatic meaning and is not a string value.\nNone represents the absence of a value or a null value. Thus, the sentence\nindicating this default usually requires a non-restrictive “in which case” clause that\nexplains the behavior that occurs when None is used. Many examples of using an\n“in which case`` clause appear in the “Parameters” section shown earlier for\nthe PyAEDT Emit class.\nStart the description for a Boolean parameter with a “Whether to” phrase and surround\nthe default in only double backticks because True and False have programmatic\nmeaning and are not string values:\nDo not include “or not” in the description because the true or false nature of a Boolean\nparameter makes this obvious. If the default for the Boolean parameter does not clearly\ndescribe the behavior that occurs, follow the default with a non-restrictive “in which case”\nclause that explains the behavior:\nEnclose all code entities in double backticks. If you surround a code entity in only a single\nbacktick (`), it is incorrectly rendered in italics in the documentation.\nUse the present tense for verbs. Occurrences of will cause Vale to\nraise warnings about not using phrases expressing future actions.\nWhen documenting variable length positional or keyword arguments, leave the leading single\nasterisk (*) or double asterisks  (**) in front of their names:\nParameter descriptions\n``\n\"\nNone\nNone\nNone\nNone\nEmit\nTrue\nFalse\n`\nwill\n*\n**"
    },
    {
        "objectID": "content-writing/py-files-writers/docstring-format-rules",
        "href": "content-writing/py-files-writers/docstring-format-rules.html#returns",
        "title": "Docstring formatting rules > Returns",
        "section": "Returns",
        "text": "The docstring for a class should not have a named “Returns” section because it is assumed that\na class always returns an instance of itself. If a class has a “Returns`` section, you can\nremove it from the docstring.\nIn Python, a method decorator is a function that can be used to modify or extend the behavior of\na method in a class without changing the method’s source code. Method decorators are typically\napplied to methods using the @ symbol followed by the decorator function’s name. They are\nusually defined separately from the class and are often used to wrap or modify the method that\nthey decorate.\nWhen a function or method has no decorator, the vanilla implementation of a Python method is\nbeing used, which means that the function or method has no return value. (While there is actually\na return value of None, this is something that you do not document.) For such methods, you\ncan remove the “Returns” section from their docstrings.\nWhen a function returns one or more values, the “Returns” section must provide the\ndata type and a description for each value returned.\nWhen only a non-Boolean value is returned, format the “Returns” like this:\nWhen only a Boolean value is returned, format the “Returns” section like this:\nWhen a method has an @exceperty decorator, it always returns a Boolean value. Thus,\nformat the Returns section for such a method as shown in the preceding example.\nWhen multiple values are returned, format the “Returns” section like the “Parameters”\nsection:\nReturns\n@\nNone\n@exceperty\nReturns"
    },
    {
        "objectID": "content-writing/py-files-writers/docstring-format-rules",
        "href": "content-writing/py-files-writers/docstring-format-rules.html#raises",
        "title": "Docstring formatting rules > Raises",
        "section": "Raises",
        "text": "This named docstring section is optional. It lists the errors that can be raised and explains when\nthey are raised. While many PyAnsys libraries do not include a “Raises” section in their docstrings,\nincluding this section can be valuable for users.\nRaises"
    },
    {
        "objectID": "content-writing/py-files-writers/docstring-format-rules",
        "href": "content-writing/py-files-writers/docstring-format-rules.html#examples",
        "title": "Docstring formatting rules > Examples",
        "section": "Examples",
        "text": "This named section is optional but strongly recommended. The one or more interactive examples placed\nin this section demonstrate usage. They do not provide a testing framework. Those types of tests\nare typically placed in the tests directory. For more information, see testing.\nAccording to documentation published by the Python organization, the doctest module executes\nthe examples in the “Examples” section of the docstring to verify that they work.\nPlace any description of what the example code demonstrates immediately after the Examples section\nheading. Follow this description with a blank line. Then, precede each line of code with three right\ncarats (>>>) to render them in a code block.\nUse blank lines to separate comments from lines of code. Also use blank lines\nto separate multiple code examples.\nHere is an “Examples” section for the element_dot method in PyDPF-Core:\nThe returned value for this example is 10. If you are writing an example and want to test\nit locally, you can copy and paste the lines beginning with the three right carats into\nJupyterLab and execute them. You can then paste the returned value into the example but\nwithout the three right carats.\nExamples\ntests\nExamples\n>>>\nelement_dot\n10"
    },
    {
        "objectID": "content-writing/rst-files-writers/tab-sets",
        "href": "content-writing/rst-files-writers/tab-sets.html#tab-sets",
        "title": "Tab sets",
        "section": "Tab sets",
        "text": "Tab sets are like cards in that they provide for quickly accessing\ninformation that you are specifically interested in. In PyAnsys\ndocumentation, tab sets are often used to provide procedures or commands\nfor different operating systems.\nTo use tab sets in your PyAnsys documentation, you must install\nthe sphinx-design extension and then add\nit to the conf.py file in the doc/source directory and to your\nlist of documentation requirements. For more information, see\nadd_sphinx_extensions.\nTo see and use the tab sets that are shown only as images on this page,\nsee setting_up_dev_environment. To see how the tab sets on this\npage are formatted, click the Show Source link in the page’s right pane.\nAs described in rst_file_formatting, you can copy content from the\nTXT version of this file and then paste it directly into one of your RST files\nfor reuse, modifying it as needed.\nHere is an image of a tab set with tabs for installing Python on machines running different\noperating systems. When you click a tab, the procedure for the selected operating system\nis shown.\nHere is an image of a tab set with two rows. In the top row, you click the tab for your operating\nsystem. In the bottom row, you click the tab for the shell that you want to run the\ncommand in (if more than one shell is available). You can then copy this command and\npaste it in the shell’s command line.\nTab sets\nconf.py\ndoc/source"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#documenting",
        "title": "Documenting",
        "section": "Documenting",
        "text": "PyAnsys documentation must not only be written but also maintained. If you are\ncontributing to PyAnsys documentation, see the Google developer documentation style guide, which provides the general guidelines that you are to follow.\nThis page supplies guidance specific to PyAnsys documentation.\nFor comprehensive information on contributing new content or revising existing\ncontent, see content_writing.\nWhen writing developer documentation, the relationship between code and\ndocumentation is key. To keep documentation up to date with evolving\ncode, always perform these tasks:\nMinimize the content footprint.\nWrite timeless documentation.\nSupport contributions from both inside and outside of the development team.\nPerform periodic reviews.\nDocumenting"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#understand-documentation-sources",
        "title": "Documenting > Understand documentation sources",
        "section": "Understand documentation sources",
        "text": "The generation of PyAnsys documentation uses Sphinx and the Ansys-branded Sphinx theme.\nSphinx assembles content from these sources:\nDocstrings in Python (PY) files\nreStructuredText (RST) files\nExamples in PY or IPYNB files\nUnderstand documentation sources"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#docstrings",
        "title": "Documenting > Docstrings",
        "section": "Docstrings",
        "text": "You must format docstrings in PY files so that Sphinx can parse them. Sphinx provides\nthese extensions for docstring formatting:\nnumpydoc extension\nnapoleon extension\nUsing the numpydoc extension is preferred because it supports an API\ndocumentation structure with one page per method, providing Python community\nmembers with documentation like that generated for the\nNumPy and pandas packages.\nIf your API is very linear, you can use the napoleon extension because it supports\na documentation structure where everything needed to solve a certain problem can be\nshown on one page.\nThe numpydoc manual explains how to use the numpydoc extension with\nSphinx and includes a style guide.\nThe napoleon extension, which parses both numpydoc and Google style docstrings, refers\nyou to the Google Python Style Guide.\nRegardless of the extension that you choose for generating documentation from docstrings,\nusing numpydoc-style docstrings ensures that there is consistency within PyAnsys libraries.\nFor more information, see Documentation style.\nDocstrings\nnumpydoc\nnapoleon\nnumpydoc\nnapoleon"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#restructuredtext-files",
        "title": "Documenting > reStructuredText files",
        "section": "reStructuredText files",
        "text": "To provide general usage information in your documentation, use your favorite\neditor to create reStructuredText (RST) files that you then place in\nThe ``doc`` directory. The index.rst file in the doc/source directory\ndefines the first level of your documentation hierarchy. The toctree\ndirective (which stands for “table of contents tree”) indicates the maximum\nnumber of heading levels that the documentation is to display in the right navigation\npane.\nA directive is a generic block of explicit markup that sets off a specific block\nof text. For more information, see Directives in the\nSphinx documentation.\nThe toctree directive also specifies the locations of the RST files for building\neach section of your documentation.\nAs shown in the preceding figure, each documentation section has its own index.rst file.\nHowever, to optimize web searches of the generated HTML documentation, the names of the\nindex files for sections should be short and descriptive, containing keywords and using\nhyphens (-) as word separators. For more information, see SEO.\nHere is an example of the hierarchical structure for RST files. The main index files is\nnamed index.rst, and the index files for the documentation sections are named intro.rst.\nWhile you do not include the .rst extension when defining the section\nstructure in the toctree directive, the index file referenced for each\nsection should have a short descriptive name.\nAfter you build documentation locally as described in Build\ndocumentation, the first-level heading in the index file for each\nsection is shown as a clickable link in the documentation’s title bar.\nFor more information on defining the documentation structure, see Getting Started in the Sphinx\ndocumentation.\nreStructuredText files\nindex.rst\ndoc/source\ntoctree\ntoctree\nindex.rst\n-\nindex.rst\nintro.rst\n.rst\ntoctree"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#titles-and-headings-in-rst-files",
        "title": "Documenting > Titles and headings in RST files",
        "section": "Titles and headings in RST files",
        "text": "As indicated in Capitalization\nin the Google developer documentation style guide, titles and headings are to be in\nsentence case. In RST files, the line that follows a title or heading is a\nstring of characters of the same length as the heading or title. If the length\nof the characters is less than the length of the title or heading, Sphinx generates\na warning.\nFor consistency within PyAnsys libraries, the use of these special characters\nis recommended but not enforced:\nFor section-level headings, use ###.\nFor subsection-level headings, use ===.\nFor subsubsection-level headings, use ---.\nFor subsubsubsection-level headings, use ~~~.\nFor paragraph-level headings, use +++.\nFor comprehensive syntax information, see the reStrucutredText Markup Specification.\nBecause you must be familiar with the content in this guide before contributing to\na PyAnsys library, explore its pages and then look at the RST files in the repository’s\ndoc/source directory. This should help you to understand the syntax and see how RST\nfiles are nested to create the structure of this guide.\nTitles and headings in RST files\n###\n===\n---\n~~~\n+++\ndoc/source"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#recommended-sections",
        "title": "Documenting > Recommended sections",
        "section": "Recommended sections",
        "text": "Although each PyAnsys library is different, its documentation has the same goal, which\nis to provide instructions and guidelines for users. Thus, you can find some common sections\nacross the documentation for many PyAnsys libraries. Try to include these top-level\nsections in your library’s documentation:\nGetting started: Explains how to install and set up the library.\nUser guide: Describes how to use basic features of the library.\nAPI reference Documents API resources provided by the library.\nExamples: Provides fully fledged examples for using the library.\nContribute: Refers to the PyAnsys developer’s guide\nfor overall guidance and then provides library-specific contribution information.\nFor comprehensive information on writing content, see content_writing.\nRecommended sections\nGetting started\nUser guide\nAPI reference\nExamples\nContribute"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#examples",
        "title": "Documenting > Examples",
        "section": "Examples",
        "text": "Examples come in two formats:\nBasic code snippets demonstrating features\nFull-fledged standalone examples that are meant to be run as downloadable scripts\nPlace basic code snippets in RST files in the doc/source directory.\nPlace full-fledged standalone examples in the examples directory,\nwhich is at the root of the repository. All of these examples must be compliant\nwith PEP 8. They are compiled dynamically during the build process.\nAlways ensure that your examples run properly locally because they are\nverified through the CI performed by GitHub Actions.\nAdding a new standalone example consists of placing it in an applicable\nsubdirectory in the examples directory. If none of the existing directories\nmatch the category of your example, create a subdirectory with a\nREADME.txt file describing the new category\nHere is an example of what the structure for a PyAnsys library typically looks like:\nIn the Sphinx configuration file (doc/conf.py), enable the Sphinx-Gallery extension:\nThe following configuration declares the location of the examples directory\nto be ../examples and the output directory to be examples:\nBecause these examples are built using Sphinx-Gallery, you must follow its coding guidelines.\nGeneral example uses Python and Sphinx-Gallery.\nExamples\ndoc/source\nexamples\nexamples\nREADME.txt\ndoc/conf.py\nexamples\n../examples\noutput\nexamples"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#generate-documentation-without-examples",
        "title": "Documenting > Generate documentation without examples",
        "section": "Generate documentation without examples",
        "text": "The documentation for some PyAnsys repositories is built using Sphinx-Gallery and\nincludes examples generated with the assistance of Ansys products.\nBuilding documentation locally is time consuming in these cases,\nparticularly if you need to test changes only in the documentation excluding the examples.\nTo generate documentation without examples, use command make html-noplot with\nappropriate changes in make.bat and Makefile as mentioned below.\nRefer sphinx-gallery documentation.\nAlternatively, you can modify sphinx_gallery_conf in conf.py as shown below\nGenerate documentation without examples\nmake html-noplot\nmake.bat\nMakefile\nsphinx_gallery_conf\nconf.py"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#generate-documentation-from-docstrings",
        "title": "Documenting > Generate documentation from docstrings",
        "section": "Generate documentation from docstrings",
        "text": "You can use the native sphinx.ext.autodoc extension to generate documentation from the\ndocstrings in your Python files. When using this extension, you can include these directives\nin your RST files:\nautomodule: For documenting modules\nautoclass: For documenting classes\nautofunction: For documenting methods and functions\nFor a full list of auto directives, see Include documentation from docstrings in the Sphinx\ndocumentation.\nGenerate documentation from docstrings\nsphinx.ext.autodoc\nautomodule\nautoclass\nautofunction\nauto"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#document-classes",
        "title": "Documenting > Document classes",
        "section": "Document classes",
        "text": "There are two main ways of using Sphinx to document a class:\nManually describe why and how you use a class in RST files.\nAutomatically generate documentation for classes using the autoclass or\nautosummary directive in RST files.\nDocument classes\nautoclass\nautosummary"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#manually-generate-documentation",
        "title": "Documenting > Manually generate documentation",
        "section": "Manually generate documentation",
        "text": "To describe why and how to use a class in RST files, use the\ncode-block directive:\nInitialize my_module.MyClass with initial parameters. These\nparameters are automatically assigned to the class.\nManually generate documentation\ncode-block\nmy_module.MyClass"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#automatically-generate-documentation",
        "title": "Documenting > Automatically generate documentation",
        "section": "Automatically generate documentation",
        "text": "To automatically generate class descriptions from the numpydoc strings in\nyour Python files, use either the autoclass or autosummary directive\nin your RST files. For information on docstrings and required docstring\nsections, see Numpydoc docstrings.\nFor simple classes, use the autoclass directive:\nThe summary line for a class docstring should fit on one line.\nAttributes should be documented inline with the attribute’s\ndeclaration.\nProperties created with the @property decorator should be\ndocumented in the property’s getter method.\nparam1 (str) – Description of param1.\nparam2 (list of str) – Description of param2. Multiple\nlines are supported.\nparam3 (int, optional) – Description of param3.\nAn example of how to initialize this class should be given.\nClass methods are similar to regular functions.\nparam1 (str) – The first parameter.\nparam2 (str) – The second parameter.\nTrue if successful, False otherwise.\nbool\nDo not include the self parameter in the Parameters section.\nProperties should be documented in their getter method.\nSet or return the readwrite property.\nProperties with both a getter and setter should only be documented in\ntheir getter method.\nIf the setter method contains notable behavior, it should be mentioned\nhere.\nFor complex classes with many methods, use the autosummary directive:\nCustom implementation of a complex number.\nreal (float) – Real component of the complex number.\nimag (float, optional) – Imaginary component of the complex number.\nansys_sphinx_theme.examples.samples.Complex.real\nReal component of this complex number.\nansys_sphinx_theme.examples.samples.Complex.imag\nReal component of this complex number.\nansys_sphinx_theme.examples.samples.Complex.abs\nReturn the absolute value of this number.\nWhen you use the autosummary directive, each class has its own dedicated page.\nEach method and attribute in that class also has its own page.\nAutomatically generate documentation\nautoclass\nautosummary\nautoclass\n@property\nlist\nstr\nint\nTrue\nFalse\nself\nParameters\nautosummary\nansys_sphinx_theme.examples.samples.Complex.real\nansys_sphinx_theme.examples.samples.Complex.imag\nansys_sphinx_theme.examples.samples.Complex.abs\nautosummary"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#ansys_sphinx_theme.examples.samples.ExampleClass",
        "title": "Documenting > Automatically generate documentation > ExampleClass",
        "section": "Automatically generate documentation > ExampleClass",
        "text": "class ansys_sphinx_theme.examples.samples.ExampleClass(param1, param2, param3=0)\n\nThe summary line for a class docstring should fit on one line.\n\nAttributes should be documented inline with the attribute’s\ndeclaration.\n\nProperties created with the @property decorator should be\ndocumented in the property’s getter method.\n\nParameters\n\nparam1 (str) – Description of param1.\n\nparam2 (list of str) – Description of param2. Multiple\nlines are supported.\n\nparam3 (int, optional) – Description of param3.\n\nExamples\n\nAn example of how to initialize this class should be given.\n\n>>> from ansys_sphinx_theme import samples\n>>> example = samples.ExampleClass(\"mystr\", [\"apple\", \"orange\"], 3)\n\n\n\nexample_method(param1, param2)\n\nClass methods are similar to regular functions.\n\nParameters\n\nparam1 (str) – The first parameter.\n\nparam2 (str) – The second parameter.\n\nReturns\n\nTrue if successful, False otherwise.\n\nReturn type\n\nbool\n\nNotes\n\nDo not include the self parameter in the Parameters section.\n\nExamples\n\n>>> example.example_method(\"foo\", \"bar\")\nTrue\n\n\n\nproperty readonly_property: str\n\nProperties should be documented in their getter method.\n\nExamples\n\n>>> example.readonly_property\n\"readonly_property\"\n\n\n\nproperty readwrite_property\n\nSet or return the readwrite property.\n\nProperties with both a getter and setter should only be documented in\ntheir getter method.\n\nIf the setter method contains notable behavior, it should be mentioned\nhere.\n\nExamples\n\n>>> example.readwrite_property\n\"readwrite_property\"\n\n>>> example.readwrite_property = \"hello world\"\n>>> example.readwrite_property\n'hello world'"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#ansys_sphinx_theme.examples.samples.ExampleClass.example_method",
        "title": "Documenting > Automatically generate documentation > example_method",
        "section": "Automatically generate documentation > example_method",
        "text": "example_method(param1, param2)\n\nClass methods are similar to regular functions.\n\nParameters\n\nparam1 (str) – The first parameter.\n\nparam2 (str) – The second parameter.\n\nReturns\n\nTrue if successful, False otherwise.\n\nReturn type\n\nbool\n\nNotes\n\nDo not include the self parameter in the Parameters section.\n\nExamples\n\n>>> example.example_method(\"foo\", \"bar\")\nTrue"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#ansys_sphinx_theme.examples.samples.ExampleClass.readonly_property",
        "title": "Documenting > Automatically generate documentation > readonly_property",
        "section": "Automatically generate documentation > readonly_property",
        "text": "property readonly_property: str\n\nProperties should be documented in their getter method.\n\nExamples\n\n>>> example.readonly_property\n\"readonly_property\""
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#ansys_sphinx_theme.examples.samples.ExampleClass.readwrite_property",
        "title": "Documenting > Automatically generate documentation > readwrite_property",
        "section": "Automatically generate documentation > readwrite_property",
        "text": "property readwrite_property\n\nSet or return the readwrite property.\n\nProperties with both a getter and setter should only be documented in\ntheir getter method.\n\nIf the setter method contains notable behavior, it should be mentioned\nhere.\n\nExamples\n\n>>> example.readwrite_property\n\"readwrite_property\"\n\n>>> example.readwrite_property = \"hello world\"\n>>> example.readwrite_property\n'hello world'"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#ansys_sphinx_theme.examples.samples.Complex",
        "title": "Documenting > Automatically generate documentation > Complex",
        "section": "Automatically generate documentation > Complex",
        "text": "class ansys_sphinx_theme.examples.samples.Complex(real, imag=0.0)\n\nCustom implementation of a complex number.\n\nParameters\n\nreal (float) – Real component of the complex number.\n\nimag (float, optional) – Imaginary component of the complex number.\n\nExamples\n\n>>> my_num = Complex(real=1, imag=-1.0)\n>>> my_num\n(1.0 + 1.0j)"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#document-multiple-classes",
        "title": "Documenting > Document multiple classes",
        "section": "Document multiple classes",
        "text": "To document a set of small but highly cohesive classes, you can combine\nthe two preceding approaches. To accomplish this, you include multiple\nautoclass directives in the same RST file with headings and text blocks as\nnecessary to describe the relationships between the classes.\nFor example, the Granta MI BoM Analytics library uses this combined approach:\nPart compliance\nfirst describes the\nPartComplianceQuery\nclass. It then describes the\nPartComplianceQueryResult\nand\nPartWithComplianceResult\nclasses returned by the query. Because the classes are only ever\nencountered together in this context, they are documented on a\nsingle page.\nIn contrast, the\nRoHSIndicator\nand\nWatchListIndicator\nclasses are shared across multiple queries. Consequently, these classes are\ndocumented separately.\nDocument multiple classes\nautoclass\nPartComplianceQuery\nPartComplianceQueryResult\nPartWithComplianceResult\nRoHSIndicator\nWatchListIndicator"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#build-documentation",
        "title": "Documenting > Build documentation",
        "section": "Build documentation",
        "text": "Sphinx is used to build the documentation. You configure the entire build process in\nThe ``conf.py`` file.\nThe doc directory contains a Makefile file and a make.bat file for\nautomating the building process. Different builders render different\ndocumentation output, such as HTML and PDF.\nBuild documentation\ndoc\nMakefile\nmake.bat\nHTML\nPDF"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#build-html-documentation",
        "title": "Documenting > Build HTML documentation",
        "section": "Build HTML documentation",
        "text": "You can build HTML documentation locally with the command for your OS. On macOS\nor Linux, you use Makefile. On Windows, you use the make.bat file.\nThe resulting HTML files are created in the doc/_build/html directory.\nTo view the HTML documentation in your browser, navigate to this directory\nand double-click the index.html file.\nBuild HTML documentation\nMakefile\nmake.bat\ndoc/_build/html\nindex.html"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#build-pdf-documentation",
        "title": "Documenting > Build PDF documentation",
        "section": "Build PDF documentation",
        "text": "To build PDF documentation locally, you must add the following rules to\nthe Makefile and make.bat files:\nYou can then build the PDF documentation locally with the command for your OS:\nThe resulting PDF and intermediate LaTeX files are created in the\ndoc/_build/latex directory.\nBecause warnings and errors that occur during the LaTeX building and rendering\nprocesses are ignored, it is possible that the PDF file has text formatting errors.\nBuild PDF documentation\nAlways verify the content of your PDF file.\nMakefile\nmake.bat\ndoc/_build/latex"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#enable-multi-version-documentation",
        "title": "Documenting > Enable multi-version documentation",
        "section": "Enable multi-version documentation",
        "text": "When your library has multi-version documentation enabled, the right side of the\ndocumentation’s title bar displays a drop-down button for switching between documentation\nversions. With this button, you can switch from viewing the documentation for the latest\nstable version to the documentation for the development version or a previously\nreleased version.\nTo take advantage of multi-version documentation, your library must use\nansys/actions@v4 or\nlater and be configured based on its level of maturity.\nEnable multi-version documentation"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#upgrade-to-ansysactionsv4-or-later",
        "title": "Documenting > Upgrade to ansys/actions@v4 or later",
        "section": "Upgrade to ansys/actions@v4 or later",
        "text": "If your library uses an Ansys action earlier than ansys/actions@v4, upgrade\nto a later version:\nUpdate all your continuous integration YML files to use ansys/actions@v4 or later.\nMake sure that the \"json_url\" key points to f\"https://{cname}/versions.json\".\nNote that the release/ substring is dropped.\nApply the previous steps as fix patches in all versions that you want to include\nin your multi-version documentation.\nUpgrade to ansys/actions@v4 or later\nansys/actions@v4\nansys/actions@v4\nansys/actions@v4\n\"json_url\"\nf\"https://{cname}/versions.json\"\nrelease/"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#configure-multi-version-documentation",
        "title": "Documenting > Configure multi-version documentation",
        "section": "Configure multi-version documentation",
        "text": "Once your library uses ansys/actions@v4 or later, configure multi-version\ndocumentation:\nEnsure that you are using Ansys Sphinx Theme version 0.8\nor later for building your library’s documentation.\nInclude the following lines in The ``conf.py`` file:\nThe DOCUMENTATION_CNAME environment variable is expected to be\ndeclared in the YML file controlling the deployment of the documentation.\nThe idea is that the canonical name is only defined in a single\nplace, so it can be easily changed if required.\nEnable documentation deployment for development and stable versions. For more\ninformation, see Deploy documentation.\nOnce multi-version documentation is configured, its use is automated. Every time you\nrelease a new version, a link to the documentation for this version is added to the\ndrop-down button.\nThe documentation drop-down button displays the development version and\nlast three stable versions by default. To have the button display more\nversions, use the render-last variable in the\nansys/actions/doc-deploy-stable action.\nIf you require support for enabling multi-version documentation, email the\nPyAnsy Core team.\nConfigure multi-version documentation\nAbout the DOCUMENTATION_CNAME environment variable\nControlling the number of versions shown in the drop-down button\nansys/actions@v4\nDOCUMENTATION_CNAME\nDOCUMENTATION_CNAME\nrender-last"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#deploy-documentation",
        "title": "Documenting > Deploy documentation",
        "section": "Deploy documentation",
        "text": "PyAnsys libraries use GitHub Actions to deploy their documentation online to\nGitHub Pages. This documentation is hosted on the gh-pages branch of the\nlibrary’s repository. Documentation deployment is done by uploading the\nHTML documentation artifact to the gh-pages branch of the library’s repository.\nFor more information, see Creating a GitHub Pages site\nin the GitHub documentation.\nTo automatically deploy both development and stable documentation, add the\ndoc-deploy-dev and doc-deploy-stable jobs to the ci_cd.yml file in\nthe .github/workflows directory:\nDeploy documentation\ngh-pages\ngh-pages\ndoc-deploy-dev\ndoc-deploy-stable\nci_cd.yml\n.github/workflows"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#deploy-to-another-repository",
        "title": "Documenting > Deploy to another repository",
        "section": "Deploy to another repository",
        "text": "If you are planning to deploy documentation to a repository other than the one\nfor your library, make sure that you create this repository before deploying your\ndocumentation for the first time.\nUsing the {{ secrets.GITHUB_TOKEN }} token when deploying to another repository is\nnot possible due to the level of credentials of this token. Instead, use the\nsecrets generated by the PyAnsy Bot app.\nFor deploying the documentation to another repository, use this workflow:\nDeploy to another repository\n{{ secrets.GITHUB_TOKEN }}"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#access-online-documentation",
        "title": "Documenting > Access online documentation",
        "section": "Access online documentation",
        "text": "Documentation for the latest stable release of a PyAnsys library is accessible\nfrom its repository. The canonical name for the documentation is constructed\nusing this structure:\nhttps://<product>.docs.pyansys.com\nIf a library does not have multi-version documentation enabled, you can generally\naccess the latest development version of the documentation by adding dev to\nthe URL:\nhttps://<product>.docs.pyansys.com/dev\nHowever, most libraries do have multi-version documentation enabled, which\nmeans that stable and development versions are collected under the same website.\nFor example, consider the PyAEDT documentation: On the right side of the title bar\nfor its documentation, you use the drop-down button to select the version of the\ndocumentation.\nThe URL for documentation of the latest stable version is https://aedt.docs.pyansys.com/version/stable/index.html.\nThe URL for documentation of the latest development version is https://aedt.docs.pyansys.com/version/dev/index.html.\nAnsys actions are GitHub Actions for automatically keeping the latest development versions\nof both the library and its documentation up to date.\nTo make documentation changes, you create a branch with a name that begins with\na prefix of doc/ that is then followed by a short description of what you\nare changing. For more information, see Branching model.\nAs you are making changes in this branch, you want to periodically generate the\ndocumentation locally so that you can test your changes before you create a\nGitHub pull request. For more information, see Build documentation.\nAccess online documentation\nhttps://<product>.docs.pyansys.com\ndev\nhttps://<product>.docs.pyansys.com/dev\ndoc/"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#use-pymeilisearch-as-a-documentation-search-engine",
        "title": "Documenting > Use PyMeilisearch as a documentation search engine",
        "section": "Use PyMeilisearch as a documentation search engine",
        "text": "PyMeilisearch is a Python client library that lets you use MeiliSearch, an open\nsource search engine, to provide fast and relevant documentation search capabilities.\nTo use PyMeilisearch as a search engine for multi-version documentation, perform\nthese steps.\nEnsure that you are using Ansys Sphinx Theme version 0.8\nor later for building your library’s documentation.\nIn the conf.py file in the doc/source directory, include these lines:\nIn these lines, replace <your-index-name> with the name for your MeiliSearch index.\nThe convert_version_to_pymeilisearch function converts your library’s version into\na format suitable for MeiliSearch indexing.\nEnable documentation index deployment for development and stable versions using Ansys actions:\nReplace <your-package>, <your-index-name>, and <library> with appropriate values\nfor your project.\nThe version of your package is automatically calculated and used for indexing, ensuring that\nyour documentation remains up to date. For more information, see the PyMeilisearch and\nAnsys Sphinx Theme documentation.\nUse PyMeilisearch as a documentation search engine\nconf.py\ndoc/source\nconvert_version_to_pymeilisearch"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#optimize-web-searches",
        "title": "Documenting > Optimize web searches",
        "section": "Optimize web searches",
        "text": "It’s important that web searches rank PyAnsys documentation pages high in organic\nsearch results. The PyAnsys Core team has identified some challenges in regard to\nsearch engine optimization (SE0):\nThe robots.txt file in the root directory of your documentation website\nblocks Google and other search engines from crawling some pages. This file tells\nweb robots about the structure of the documentation website.\nMissing canonical tags result in duplicate pages being found.\nFor multi-version documentation, redirection from the base index.html file\nto the version/stable/index.html file causes problems.\nTo resolve these SEO challenges, upgrade your documentation deployment actions to\ndoc-deploy-stable@v6 and doc-deploy-dev@v6 or higher.\nHere’s how using the latest doc-deploy actions automatically address these SEO challenges:\nThey generate a corrected robots.txt that allows Google and other search engines\nto crawl all your documentation pages.\nThey add rel=\"canonical\" tags, which are HTML elements, in the head sections of HTML pages.\nA canonical tag prevents duplicate content issues by specifying the preferred (canonical) URL\nfor a group of pages that have the same or very similar content. Search engines use the canonical\ntag to determine the best representation of a page. Here is an example of a conical tag:\n<link rel=\"canonical\" href=\"http://www.example.com/\">\nThey copy gh-pages/version/stable/index.html to gh-pages/index.html and prepend\nlocal references with version/stable/, which eliminates the need for redirection.\nOptimize web searches\nrobots.txt\nindex.html\nversion/stable/index.html\ndoc-deploy-stable@v6\ndoc-deploy-dev@v6\ndoc-deploy\nrobots.txt\nrel=\"canonical\"\n<link rel=\"canonical\" href=\"http://www.example.com/\">\ngh-pages/version/stable/index.html\ngh-pages/index.html\nversion/stable/"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#seo-tips",
        "title": "Documenting > SEO tips",
        "section": "SEO tips",
        "text": "Here are additional SEO tips for ensuring that Google and other search engines rank\nyour documentation pages higher in search results:\nAvoid having index.html pages (except for the main one for your documentation).\nSphinx, which renders files named index.rst into pages named index.html, always\ngenerates absolute links. For example, the absolute link for page/index.rst is\npage/index.html. Google prefers links to pages that do not end with index.html.\nUse lowercase for names of directories and files and separate words with hyphens.\nWhile these naming conventions might be difficult to achieve for auto-generated files\nmatching the names of classes, following these naming guidelines when possible makes\nfile searches easier and search results more useful. For comprehensive information, see\nFilenames and file types in the\nGoogle developer documentation style guide.\nUse short filenames containing keywords.\nAvoid Ansys-specific words in filenames. Use a keyword generator like ahrefs\nto find relevant keywords that potential users who are searching for information are\nlikely to use.\nSEO tips\nindex.html\nindex.rst\nindex.html\npage/index.rst\npage/index.html\nindex.html"
    },
    {
        "objectID": "how-to/documenting",
        "href": "how-to/documenting.html#seo-naming-examples",
        "title": "Documenting > SEO naming examples",
        "section": "SEO naming examples",
        "text": "This table shows how to use the preceding tips to rename example files. While the source files for examples\nare generally Python (PY) or Jupyter Notebook (IPYNB) files, Sphinx converts them to reStructuredText (RST)\nfiles for inclusion in the documentation.\nNot recommended\nRecommended\nexamples/hohmann-transfer-using-targeter.rst\nexamples-hohmann-transfer-{orbit,calculator}.rst\nexamples/00-mapdl-examples/transient_thermal.py\nexamples-transient-thermal-{analysis,simulation}.py\nexamples/02-HFSS/HFSS_Spiral.py\nexamples-design-and-simulation-of-spiral-inductors.py\nIf you have questions about SEO, email the PyAnsy Core team.\nSEO naming examples"
    },
    {
        "objectID": "how-to/supporting-python-versions",
        "href": "how-to/supporting-python-versions.html#python-versions",
        "title": "Python versions",
        "section": "Python versions",
        "text": "Like other programming languages, Python evolves with time. New\nfeatures get added to the language, and other features get deprecated. For\nmore information, see Status of Python versions in the Python\nDeveloper’s Guide.\nVersion\nPEP\nReleased\nSecurity support ends\nStatus\n3.13\nPEP 719\n07 Oct 2024\nOct 2029\nStable\n3.12\nPEP 693\n02 Oct 2023\nOct 2028\nStable\n3.11\nPEP 664\n03 Oct 2022\nOct 2027\nStable\n3.10\nPEP 619\n04 Oct 2021\nOct 2026\nStable\nPython versions labeled as stable receive only security\nfixes. Versions labeled as dev are still receiving bug fixes.\nExpect stable versions to be the most commonly used Python versions. Some\npackages like NumPy drop support for older versions of\nPython earlier than their end of life (EOL) as outlined in NEP 29.\nYou can still install an older version from PyPI using pip as\nyour package manager. When pip is used, it downloads and installs\nthe most recent version of the library that supports your version of Python. You\ncan enforce a minimum-required Python version within the setup.py file with\nthis code:\nThis helps pip to know which versions of your library\nsupport which versions of Python. You can also impose an upper limit if you’re\nsure you don’t support certain versions of Python. For example, if you only\nsupport Python 3.10 through 3.13, your command would look like this: python_requires='>=3.10, <3.13'.\nPython versions\nConsider supporting stable Python versions.\nstable\ndev\npip\nsetup.py\npip\npython_requires='>=3.10, <3.13'"
    },
    {
        "objectID": "how-to/supporting-python-versions",
        "href": "how-to/supporting-python-versions.html#verify-python-support",
        "title": "Python versions > Verify Python support",
        "section": "Verify Python support",
        "text": "The best way to validate whether a Python library supports a version of Python\nis by continuous_integration. An example GitHub workflow testing Python\n3.10 through Python 3.13 on Windows and Linux would start like this:\nThe workflow would then list the tests to run.\nVerify Python support"
    },
    {
        "objectID": "doc-style/docstrings",
        "href": "doc-style/docstrings.html#numpydoc-docstrings",
        "title": "Numpydoc docstrings",
        "section": "Numpydoc docstrings",
        "text": "When writing docstrings for PyAnsys libraries, follow the syntax and best practices\ndescribed in Style guide in the numpydoc Manual.\nFor consistency within PyAnsys libraries, always use \"\"\" to introduce and conclude a\ndocstring, keeping the line length shorter than 70 characters. Ensure that there are\nno blank spaces at the end of a line because they cause errors in build checks that you\nmust then resolve.\nA blank line signifies the start of a new paragraph. To create a bulleted or numbered list,\nensure that there is a blank line before the first item and after the last item. Because you\nuse the same markup in docstrings as you do in RST files, see Quick reStructuredText for a markup summary.\nSurround any text that you want to set apart as literal text (code entities) in double backticks\nto render it in a monospaced font within a gray box. Use double backticks to surround the names\nof files, folders, classes, methods, and variables.\nFor example:\nWhile the numpydoc style guide says to surround the names of classes, methods, and\nvariables in a single backtick, you must use double backticks. Surrounding text in\na single backtick in a PyAnsys library formats it in italic type rather than as a\ncode entity.\nNumpydoc docstrings\n\"\"\""
    },
    {
        "objectID": "doc-style/docstrings",
        "href": "doc-style/docstrings.html#required-docstring-sections",
        "title": "Numpydoc docstrings > Required docstring sections",
        "section": "Required docstring sections",
        "text": "PyAnsys library docstrings contain these numpydoc sections as a minimum:\nShort Summary\nExtended Summary if applicable\nParameters if applicable\nReturns if applicable\nExamples\nThese sections should follow numpydoc style. To avoid inconsistencies between\nPyAnsys libraries, adhere to the additional style guidelines that follow.\nRequired docstring sections"
    },
    {
        "objectID": "doc-style/docstrings",
        "href": "doc-style/docstrings.html#short-summary",
        "title": "Numpydoc docstrings > Short summary",
        "section": "Short summary",
        "text": "This is a single line that goes immediately after the declaration of the class\nor function to briefly describe what the class or function does. The\nshort summary is mandatory. If it is not present, Documentation style tools\nraises an error.\nThe short summary can be declared on the same line as the opening quotes or on\nthe next line. While PEP 257 accepts both ways, you must be consistent across your\nproject. If you decide to declare the short summary on the same line,\nsee Numpydoc validation because \"GL01\" checking must be\nturned off.\nThe guidelines for documenting short summaries differ for classes versus\nfunctions.\nShort summary\n\"GL01\""
    },
    {
        "objectID": "doc-style/docstrings",
        "href": "doc-style/docstrings.html#short-summaries-for-classes",
        "title": "Numpydoc docstrings > Short summaries for classes",
        "section": "Short summaries for classes",
        "text": "A class is a noun representing a collection of methods. For consistency within PyAnsys libraries,\nalways start the brief description for a class with a verb ending in “s” followed by an extended\nsummary in a new line if additional information is needed:\nEnsure that there is a line break between the end of a class docstring and the subsequent methods.\nShort summaries for classes"
    },
    {
        "objectID": "doc-style/docstrings",
        "href": "doc-style/docstrings.html#short-summaries-for-methods",
        "title": "Numpydoc docstrings > Short summaries for methods",
        "section": "Short summaries for methods",
        "text": "A method is a verb representing an action that can be performed. For consistency within PyAnsys\nlibraries, always start the brief description for a method with a verb not ending in “s” followed\nby an extended summary in a new line if additional information is needed:\nMethods with a leading underscore (_) are protected methods, meaning that they are not rendered in the\ndocumentation unless an explicit request is made to add them using Sphinx directives. However, clearly\nwritten descriptions for private methods are still important.\nIf a method has the decorator @property, it is turned into a property, which is described as a\nnoun rather than a verb. Because the resulting property cannot have parameters, you remove\nthe “Parameters” section for this method. If a setter follows the decorator @property, do not\nadd a docstring for the setter. A setter simply exposes both the GET and SET methods rather\nonly the GET method. You should include examples to demonstrate usage.\nShort summaries for methods\n@property\n@property"
    },
    {
        "objectID": "doc-style/docstrings",
        "href": "doc-style/docstrings.html#parameters",
        "title": "Numpydoc docstrings > Parameters",
        "section": "Parameters",
        "text": "Functions may have parameters in their signatures. All these parameters should be documented in\nthe “Parameters” section.\nHere is an example of a “Parameters” section for a class in PyAEDT:\nThe name of each parameter is followed by a space, a colon, a space, and then\nthe data type. A parameter is optional if its keyword argument displays a default\nin the function, class, or method signature. For an optional parameter, the\ndata type is followed by a comma and optional  or default: followed by a\nspace and then the value (if supported).\nFor example, if the library in the preceding example supported specifying the default\nafter the data type, the description for the close_on_exit parameter would look\nlike this:\nThe brief description for a parameter is a sentence fragment. However, all\nadditional information is provided in clear, complete sentences. For an optional\nparameter, if the behavior that occurs when the default is used is unclear,\nthe behavior should be described. The preceding “Parameters” section provides\nmany examples. However, here is how you would format the description for the\nsetup_name parameter if the default is specified after the data type:\nParameters\noptional\ndefault:\nclose_on_exit\nsetup_name"
    },
    {
        "objectID": "doc-style/docstrings",
        "href": "doc-style/docstrings.html#returns",
        "title": "Numpydoc docstrings > Returns",
        "section": "Returns",
        "text": "A class does not have a “Returns” section. However functions and methods\ngenerally do a “Returns” section. This section contains the return data type\nand a brief description of what is returned, which is followed by a period:\nIf a Boolean is returned, format the “Returns” section like this:\nIt is possible for the “Returns” section to look like the “Parameters” section\nif variable names are provided:\nIt is also possible for more than one item to be returned:\nIf a method does not have a decorator, the basic implementation of Python\nmethods is used. In this case, while None is returned, you do not document it.\nConsequently, such a method does not have a “Returns” section.\nReturns\nNone"
    },
    {
        "objectID": "doc-style/docstrings",
        "href": "doc-style/docstrings.html#examples",
        "title": "Numpydoc docstrings > Examples",
        "section": "Examples",
        "text": "The “Examples” section provides one or more small code samples that make usage\nof a method or function clear. They provide an easy place to start when\ntrying out the API.\nHere is a sample “Examples” section from a Python file for PyAEDT.\nCode supplied in an “Examples” section must be compliant with the\ndoctest format. This allows\nthe code to be used through pytest to perform regression testing to verify\nthat the code is executing as expected.\nIf the definition of a method or function is updated, the code in the “Examples” section\nmust be updated. Any change within the API without a corresponding change\nin the example code triggers a doctest failure.\nExamples are not meant to replace a test suite but rather complement it. Because\nexamples must always match the API that they are documenting, they are an important\nfeature of maintainable documentation.\nExamples\ndoctest"
    },
    {
        "objectID": "doc-style/docstrings",
        "href": "doc-style/docstrings.html#type-hints",
        "title": "Numpydoc docstrings > Type hints",
        "section": "Type hints",
        "text": "By default, Sphinx renders type hints as part of the function signature per\nPEP 484 – Type Hints. This can become difficult\nto read because the signature becomes very long.\nInstead, you should render type hints as part of each parameter’s description. To\naccomplish this, you must combine the sphinx.ext.autodoc.typehints, sphinx.ext.napoleon,\nand numpydoc extensions in the conf.py file in this order:\nWhen using type hints in this way, you can omit the type information in the “Parameters”\nand “Returns” sections.\nType hints\nsphinx.ext.autodoc.typehints\nsphinx.ext.napoleon\nnumpydoc\nconf.py"
    },
    {
        "objectID": "doc-style/docstrings",
        "href": "doc-style/docstrings.html#additional-directives",
        "title": "Numpydoc docstrings > Additional directives",
        "section": "Additional directives",
        "text": "Because Python docstrings are written using reStructuredText syntax, you can take\nadvantage of some of the directives available in this plaintext markup language.\nHere are some Sphinx directives that can be used in docstrings, although they\nshould be used sparingly as they do not look very good in text terminals.\nnote: Highlights important information to be aware of.\nwarning: Points out an action that might result in data loss or cause\nsome other issue, such as performance degradation.\ndeprecated: X.Y.Z Indicates the deprecation status of an object or\nfeature.\nAdditional directives\nnote\nwarning\ndeprecated\nX.Y.Z"
    },
    {
        "objectID": "doc-style/docstrings",
        "href": "doc-style/docstrings.html#example",
        "title": "Numpydoc docstrings > Example",
        "section": "Example",
        "text": "Here is a generic docstring example compliant with PyAnsys guidelines:\nTo include the docstring of a function within Sphinx, you use the\nautofunction directive:\nThis directive renders the sample function as:\nSummary line <should be one one line>.\nExtended description of function.  Can span multiple lines and\nprovides a general overview of the function.\nUse the .. warning:: directive within the doc-string for any\nwarnings you would like to explicitly state.  For example, if\nthis method will be deprecated in the next release.\narg1 (int) – Description of arg1.\narg2 (str) – Description of arg2.\nDescription of return value.\nbool\nExample\nautofunction\n.. warning::"
    },
    {
        "objectID": "doc-style/docstrings",
        "href": "doc-style/docstrings.html#ansys_sphinx_theme.examples.sample_func.func",
        "title": "Numpydoc docstrings > Example > func",
        "section": "Example > func",
        "text": "ansys_sphinx_theme.examples.sample_func.func(arg1, arg2)\n\nSummary line <should be one one line>.\n\nExtended description of function.  Can span multiple lines and\nprovides a general overview of the function.\n\nUse the .. warning:: directive within the doc-string for any\nwarnings you would like to explicitly state.  For example, if\nthis method will be deprecated in the next release.\n\nParameters\n\narg1 (int) – Description of arg1.\n\narg2 (str) – Description of arg2.\n\nReturns\n\nDescription of return value.\n\nReturn type\n\nbool\n\nExamples\n\n>>> func(1, \"foo\")\nTrue"
    },
    {
        "objectID": "content-writing/examples-writers/nbsphinx",
        "href": "content-writing/examples-writers/nbsphinx.html#use-nbsphinx",
        "title": "Use nbsphinx",
        "section": "Use nbsphinx",
        "text": "nbsphinx uses a source parser for Jupyter notebooks (IPYNB files) to generate\ninteractive examples. This extension uses custom Sphinx directives to show notebook\ncode cells (and their results) in both HTML and LaTeX output. If a notebook is unevaluated\n(has no stored output cells), it is automatically executed during the Sphinx documentation\nbuild process. Notebooks with stored output cells are not executed by default.\nUse nbsphinx\nnbsphinx\nnbsphinx"
    },
    {
        "objectID": "content-writing/examples-writers/nbsphinx",
        "href": "content-writing/examples-writers/nbsphinx.html#install-jupyter-kernel",
        "title": "Use nbsphinx > Install Jupyter kernel",
        "section": "Install Jupyter kernel",
        "text": "To execute and convert Jupyter notebooks, you must have the appropriate Jupyter kernel\ninstalled in your development environment. For example, you can install the IPython kernel\nfrom the ipykernel package.\nInstall Jupyter kernel"
    },
    {
        "objectID": "content-writing/examples-writers/nbsphinx",
        "href": "content-writing/examples-writers/nbsphinx.html#create-and-document-notebooks",
        "title": "Use nbsphinx > Create and document notebooks",
        "section": "Create and document notebooks",
        "text": "When using nbsphinx, you place the IPYNB files for the “Examples” section in the\nexamples directory. To add documentation to your notebooks, use Markdown cells.\nCreate and document notebooks\nnbsphinx\nexamples"
    },
    {
        "objectID": "content-writing/examples-writers/nbsphinx",
        "href": "content-writing/examples-writers/nbsphinx.html#configure-the-use-of-nbsphinx--in-the-sphinx-configuration-file",
        "title": "Use nbsphinx > Configure the use of nbsphinx  in the Sphinx configuration file",
        "section": "Configure the use of nbsphinx  in the Sphinx configuration file",
        "text": "To build the “Examples” section, developers configure the use of the nbsphinx\nextension in the project’s conf.py file. After installing and adding this\nextension to the extensions variable as described in add_sphinx_extensions,\ndevelopers configure the nbsphinx_execute and nbsphinx_thumbnails variables.\nHere is what these variables look like in the conf.py file for PyAnsys Geometry:\nConfigure the use of nbsphinx  in the Sphinx configuration file\nnbsphinx\nnbsphinx\nconf.py\nextensions\nnbsphinx_execute\nnbsphinx_thumbnails\nconf.py"
    },
    {
        "objectID": "content-writing/examples-writers/nbsphinx",
        "href": "content-writing/examples-writers/nbsphinx.html#add-nbsphinx-to-the-documentation-requirements",
        "title": "Use nbsphinx > Add nbsphinx to the documentation requirements",
        "section": "Add nbsphinx to the documentation requirements",
        "text": "To include nbsphinx in your project’s documentation requirements, you must\nadd it as a dependency. Depending on the project’s configuration, you add the required pip\npackages in either the pyproject.toml file or the requirements_doc_txt file.\nFor more information, see doc_ext_requirements.\nAdd nbsphinx to the documentation requirements\nnbsphinx\nnbsphinx\npip\npyproject.toml\nrequirements_doc_txt"
    },
    {
        "objectID": "content-writing/rst-files-writers/code-blocks",
        "href": "content-writing/rst-files-writers/code-blocks.html#code-blocks",
        "title": "Code blocks",
        "section": "Code blocks",
        "text": "You can introduce a short standalone code block in an RST file by ending a sentence with two\ncolons (::).\nThis example shows how to use this method to create a standalone code block.\nThis is a normal text paragraph in your RST file. The next paragraph is a code sample:\nThis is a normal text paragraph again.\nIn most cases, to create standalone code blocks that contain multiple lines of code,\nyou should use either the code or code-block directive. While you can use\nthese two directives interchangeably, the code-block directive offers more flexibility\nand control over code block formatting.\nBoth the code and code-block directives support a language option\nfor specifying the programming language that the code is written in. When you specify\nthe language, the code block uses the syntax highlighting for this language.\nThis example shows a code-block directive for importing and using a Python function.\nThis code directive shows how to import a function (my_function)\nfrom a Python module (mylibrary) and then uses it.\nThis example shows a code-block directive for turning off a log handler.\nThis code-block directive shows how to turn off a log handler.\nCode blocks can include comments and message strings that you might need to edit.\nBecause comments and message strings are more often seen in PY files than in RST\nfiles, see py_code_comments_message_strings in the section on PY files.\nCode blocks\n::\ncode\ncode-block\ncode-block\ncode\ncode-block\nlanguage\ncode-block\ncode\nmy_function\nmylibrary\ncode-block\ncode-block"
    },
    {
        "objectID": "content-writing/rst-files-writers/code-blocks",
        "href": "content-writing/rst-files-writers/code-blocks.html#number-lines-in-a-code-block",
        "title": "Code blocks > Number lines in a code block",
        "section": "Number lines in a code block",
        "text": "You can use the optional linenos attribute to generate line numbers for a code block.\nThis example shows a code-block directive that uses the linenos attribute without\nany value to generate line numbers for all lines.\nThis code-block directive shows to generate line numbers for all lines.\nTo set the line where numbering is to start, you use the optional lineno-start\nattribute, which automatically activates the linenos attribute.\nThis example shows a code-block directive that uses the lineno-start attribute to\nstart numbering at line 12.\nThis code directive numbers starts numbering at line 12.\nNumber lines in a code block\nlinenos\ncode-block\nlinenos\ncode-block\nlineno-start\nlinenos\ncode-block\nlineno-start\ncode"
    },
    {
        "objectID": "content-writing/rst-files-writers/code-blocks",
        "href": "content-writing/rst-files-writers/code-blocks.html#emphasize-lines-of-code",
        "title": "Code blocks > Emphasize lines of code",
        "section": "Emphasize lines of code",
        "text": "With the code-block directive, you can use the optional emphasize-lines attribute\nto emphasize particular lines of code by highlighting them.\nThis example shows a code-block directive that emphasizes lines 3 and 5.\nThis code directive emphasizes lines 3 and 4.\nEmphasize lines of code\ncode-block\nemphasize-lines\ncode-block\ncode"
    },
    {
        "objectID": "content-writing/rst-files-writers/code-blocks",
        "href": "content-writing/rst-files-writers/code-blocks.html#define-a-caption-and-name-for-referencing-a-code-block",
        "title": "Code blocks > Define a caption and name for referencing a code block",
        "section": "Define a caption and name for referencing a code block",
        "text": "With the code-block directive, you can use the optional caption and name\nattributes to use either the ref or numref role to reference this code block from\nelsewhere in your documentation.\nThis example shows a code directive that uses the caption and name attributes.\nThis code directive use the optional caption and name attributes.\nYou then give the name attribute to the numref role to create the cross-reference:\nIf you only define the name attribute, you can use the ref role to create the\ncross-reference providing that you explicitly provide the display text for the link:\nDefine a caption and name for referencing a code block\ncode-block\ncaption\nname\nref\nnumref\ncode\ncaption\nname\ncode\ncaption\nname\nname\nnumref\nname\nref"
    },
    {
        "objectID": "content-writing/rst-files-writers/code-blocks",
        "href": "content-writing/rst-files-writers/code-blocks.html#include-code-files",
        "title": "Code blocks > Include code files",
        "section": "Include code files",
        "text": "You can use the literalinclude directive to include a file containing plain\ntext as a code block in your documentation. For example, this directive includes a Python\nfile named example_code.py in your documentation:\nLike the code-block directive, the literalinclude directive supports the\nlinenos attribute to switch on line numbers, the lineno-start attribute\nto set the line to start the numbering at, the emphasize-lines attribute to emphasize\nparticular lines, and the name attribute to provide an implicit target name.\nFor more information, see Showing code examples in the\nSphinx documentation on directives.\nInclude code files\nliteralinclude\nexample_code.py\ncode-block\nliteralinclude\nlinenos\nlineno-start\nemphasize-lines\nname"
    },
    {
        "objectID": "packaging/templates",
        "href": "packaging/templates.html#templates",
        "title": "Templates",
        "section": "Templates",
        "text": "Starting a new project from scratch is a tedious task. To simplify the starting process\nand make project generation more dynamic, the ansys-templates tool was created. Using this\ntool ensures that any project rendered is compliant with the latest PyAnsys\ncoding and API style guidelines.\nTemplates"
    },
    {
        "objectID": "packaging/templates",
        "href": "packaging/templates.html#the-ansys-templates-tool",
        "title": "Templates > The ansys-templates tool",
        "section": "The ansys-templates tool",
        "text": "The ansys-templates tool is a command-ine interface that provides a\ncollection of templates. When you use this tool to create a PyAnsys project, your\nresponses to the several questions that are asked result in dynamic project generation.\nTo install the latest stable version of this tool, see Getting started in the\nAnsys templates documentation. Here are important links for this tool:\nRepository: https://github.com/ansys/ansys-templates\nDocumentation: https://templates.ansys.com\nIssues: https://github.com/ansys/ansys-templates/issues\nIf you encounter any problem during the installation or usage of this tool,\nopen a new issue on the repository’s Issues\npage.\nThe ansys-templates tool\nansys-templates\nansys-templates"
    },
    {
        "objectID": "packaging/templates",
        "href": "packaging/templates.html#pyansys-templates",
        "title": "Templates > PyAnsys templates",
        "section": "PyAnsys templates",
        "text": "The ansys-templates tool provides two templates for creating PyAnsys\nprojects: pyansys and pyansys-advanced.\nTo access these templates, you must install the ansys-templates package.\nFor information on how to use this tool, see User guide in the\nAnsys templates documentation.\nPyAnsys templates\nansys-templates\npyansys\npyansys-advanced\nansys-templates"
    },
    {
        "objectID": "packaging/templates",
        "href": "packaging/templates.html#pyansys-template",
        "title": "Templates > PyAnsys template",
        "section": "PyAnsys template",
        "text": "The pyansys template ships only with required directories and files to\nquickly set up a PyAnsys-compliant project:\nA src/ansys/product/library/ directory\nA setup.py file\nGeneration of doc and tests directories\nA generic .gitignore file for Python libraries\nBuild, documentation, and test requirements files\nMetadata files like those for the README and LICENSE\nTo create a project based on the pyansys template, run\nthis command:\nPyAnsys template\npyansys\nsrc/ansys/product/library/\nsetup.py\ndoc\ntests\n.gitignore\nREADME\nLICENSE\npyansys"
    },
    {
        "objectID": "packaging/templates",
        "href": "packaging/templates.html#pyansys-advanced-template",
        "title": "Templates > PyAnsys advanced template",
        "section": "PyAnsys advanced template",
        "text": "The pyansys-advanced template is an enhanced version of the pyansys template.\nIt ships with the same directories and files and supports additional features:\nlets you select the project file (setup.py or pyproject.toml)\nUses tox for testing and task automation\nIncludes GitHub Actions for CI purposes\nUses pre-commit for checking coding style\nTo create a project based on the pyansys-advanced template, run this command:\nPyAnsys advanced template\npyansys-advanced\npyansys\nsetup.py\npyproject.toml\npyansys-advanced"
    },
    {
        "objectID": "coding-style/formatting-tools",
        "href": "coding-style/formatting-tools.html#code-style-tools",
        "title": "Code style tools",
        "section": "Code style tools",
        "text": "There are many tools for checking code style. This section presents some of\nthe most popular ones in the Python ecosystem. A minimum configuration is\nprovided for each one so that you can easily include them in your PyAnsys project.\nMost of the tools presented can be configured using the\n``pyproject.toml`` file. Avoiding dotfiles leads to a much\ncleaner root project directory.\nCode style tools"
    },
    {
        "objectID": "coding-style/formatting-tools",
        "href": "coding-style/formatting-tools.html#black",
        "title": "Code style tools > Black",
        "section": "Black",
        "text": "Black is the most popular code formatter in the Python community because it is\nmaintained by the Python Software Foundation. It allows for a minimum\nconfiguration to ensure that the Python code format looks almost the same across\nprojects.\nWhile PEP 8 imposes a default line length of 79 characters, Black has\na default line length of 88 characters.\nThe minimum Black configuration for a PyAnsys project should look like this:\nBlack"
    },
    {
        "objectID": "coding-style/formatting-tools",
        "href": "coding-style/formatting-tools.html#the-isort-tool",
        "title": "Code style tools > The isort tool",
        "section": "The isort tool",
        "text": "The goal of isort  is to properly format import statements by making sure\nthat they follow the standard order:\nLibrary\nThird-party libraries\nCustom libraries\nWhen using isort with Black, it is important to properly configure both\ntools so that no conflicts arise. To accomplish this, use the\n--profile black flag in isort.\nThe isort tool\nisort\nimport\n--profile black\nisort"
    },
    {
        "objectID": "coding-style/formatting-tools",
        "href": "coding-style/formatting-tools.html#flake8",
        "title": "Code style tools > Flake8",
        "section": "Flake8",
        "text": "The goal of Flake8 is to act as a PEP 8 compliance checker. Again, if\nthis tool is being used with Black, it is important to make sure that no\nconflicts arise.\nThe following configuration is the minimum one to set up Flake8 together with\nBlack.\nThe configuration for Flake8 must be specified in a .flake8 file.\nFlake8 has many options that can be set within the configuration file.\nFor more information, see Full Listing of Options and Their Descriptions in the Flake8\ndocumentation.\nThe example configuration defines these options:\nSubdirectories and files to exclude when checking.\nSequence of error codes that Flake8 is to report errors\nfor. The set in the preceding configuration is a basic set of errors\nfor checking and is not an exhaustive list. For more information, see\nError/Violation Codes\nin the Flake8 documentation.\nTotal number of errors to print when checking ends.\nMaximum allowed McCabe complexity value for a block of code.\nThe value of 10 was chosen because it is a common default.\nNumber of occurrences of each error or warning code\nto print as a report when checking ends.\nFlake8\n.flake8\nexclude\nselect\ncount\nmax-complexity\nstatistics"
    },
    {
        "objectID": "coding-style/formatting-tools",
        "href": "coding-style/formatting-tools.html#the-add-license-headers-pre-commit-hook",
        "title": "Code style tools > The Add-license-headers pre-commit hook",
        "section": "The Add-license-headers pre-commit hook",
        "text": "The goal of the add-license-headers pre-commit hook is to add and update license headers\nfor files with REUSE software. By default, the hook runs on\nPROTO files in any directory and on Python files in the src, examples, and tests directories.\nYou can find in the ansys/pre-commit-hooks repository, the MIT.txt file\nthat is added to files.\nFor information on customizing the hook, in this same repository, see the\nREADME file.\nThe Add-license-headers pre-commit hook\nAdd-license-headers\nadd-license-headers\nsrc\nexamples\ntests\nansys/pre-commit-hooks"
    },
    {
        "objectID": "coding-style/formatting-tools",
        "href": "coding-style/formatting-tools.html#code-coverage",
        "title": "Code style tools > Code coverage",
        "section": "Code coverage",
        "text": "Code coverage indicates the percentage of the codebase tested by the test\nsuite. Code coverage should be as high as possible to guarantee that every piece\nof code has been tested.\nFor PyAnsys libraries, code coverage is done using pytest-cov, a pytest plugin\nthat triggers code coverage analysis once your test suite has executed.\nConsidering the layout presented in Required files, the following\nconfiguration for code coverage is the minimum one required for a PyAnsys\nproject:\nCode coverage"
    },
    {
        "objectID": "coding-style/formatting-tools",
        "href": "coding-style/formatting-tools.html#the-pre-commit-tool",
        "title": "Code style tools > The pre-commit tool",
        "section": "The pre-commit tool",
        "text": "To ensure that every commit you make is compliant with the code style\nguidelines for PyAnsys, you can take advantage of pre-commit in your project.\nEvery time you stage some changes and try to commit them, pre-commit only\nallows them to be committed if all defined hooks succeed.\nYou must define the configuration for pre-commit in a\n.pre-commit-config.yaml file. The following lines present a minimum\nconfiguration that includes both code and documentation formatting tools.\nThe pre-commit tool\npre-commit\npre-commit\npre-commit\n.pre-commit-config.yaml"
    },
    {
        "objectID": "coding-style/formatting-tools",
        "href": "coding-style/formatting-tools.html#install-pre-commit",
        "title": "Code style tools > Install pre-commit",
        "section": "Install pre-commit",
        "text": "You can install pre-commit by running this command:\nThen, ensure that you install it as a Git hook by running this command:\nInstall pre-commit\npre-commit\npre-commit\nGit hook"
    },
    {
        "objectID": "coding-style/formatting-tools",
        "href": "coding-style/formatting-tools.html#use-pre-commit",
        "title": "Code style tools > Use pre-commit",
        "section": "Use pre-commit",
        "text": "One installed as described, pre-commit automatically triggers every time\nthat you try to commit a change. If any hook defined in the .pre-commit-config.yaml\nfile fails, you must fix the failing files, stage the new changes, and try to commit\nthem again.\nIf you want to manually run pre-commit, run this command:\nIf any of the hooks fail, this command shows the current and expected style of the code.\nUse pre-commit\npre-commit\npre-commit\n.pre-commit-config.yaml\npre-commit"
    },
    {
        "objectID": "coding-style/formatting-tools",
        "href": "coding-style/formatting-tools.html#the-tox-tool",
        "title": "Code style tools > The tox tool",
        "section": "The tox tool",
        "text": "You might consider using tox in your project. While this automation\ntool is similar to Make, it supports testing of your package in a temporary\nvirtual environment. Being able to test your package in isolation rather than in\n“local” mode guarantees reproducible builds.\nConfiguration for tox is stored in a tox.ini file. Here is the minimum\nconfiguration for a PyAnsys py<product>-<library> project:\nThis minimum configuration assumes that you have a requirements directory that\ncontains requirements_tests.txt and requirements_doc.txt files. In\naddition, the style environment must execute pre-commit, which guarantees\nthe usage of this tool in your project.\nThe tox tool\ntox\ntox\ntox.ini\npy<product>-<library>\nrequirements\nrequirements_tests.txt\nrequirements_doc.txt\nstyle\npre-commit"
    },
    {
        "objectID": "coding-style/formatting-tools",
        "href": "coding-style/formatting-tools.html#install-tox",
        "title": "Code style tools > Install tox",
        "section": "Install tox",
        "text": "You can install tox like any other Python package:\nInstall tox\ntox\ntox"
    },
    {
        "objectID": "coding-style/formatting-tools",
        "href": "coding-style/formatting-tools.html#use-tox",
        "title": "Code style tools > Use tox",
        "section": "Use tox",
        "text": "The tox tool uses environments, which are similar to Makefile rules,\nto make it highly customizable. Descriptions follow of some of the most\nwidely used environments:\ntox -e style: Checks the code style of your project.\ntox -e py: Runs your test suite.\ntox -e doc: Builds the documentation of your project.\nIt is possible to run multiple environments by separating them with commas:\ntox -e <env-name0>,<env-name1>,...\nTo run all available environments, simply type tox.\nUse tox\ntox\ntox\nenvironments\nMakefile\ntox -e style\ntox -e py\ntox -e doc\ntox -e <env-name0>,<env-name1>,...\ntox"
    },
    {
        "objectID": "coding-style/formatting-tools",
        "href": "coding-style/formatting-tools.html#the-pre-commitci-tool",
        "title": "Code style tools > The pre-commit.ci tool",
        "section": "The pre-commit.ci tool",
        "text": "The goal of the pre-commit.ci tool is to run the same hooks as the\npre-commit tool, but in a CI environment. This tool is useful for\nchecking the code style of your project in a CI environment.\nAlthough the PyAnsys ecosystem also has its own code-style action (see\nCode style action),\nthe pre-commit.ci tool provides some additional features:\nIt is free for public projects.\nIt is compatible with any CI provider.\nIt ensures that hook versions are up to date.\nAny changes performed by the hooks are committed back to the repository.\nIt reduces CI run times by caching the hooks used.\nTo use the pre-commit.ci tool, you must have a .pre-commit-config.yaml file for your repository. Next,\nyou should request the PyAnsys Core team to enable the pre-commit.ci tool for your\nrepository.\nThe pre-commit.ci tool is not available for private repositories.\nThe PyAnsys ecosystem strongly recommends using the pre-commit.ci tool in your project. It is a\ngreat way to ensure that your code is compliant with the code style guidelines set by the PyAnsys ecosystem.\nThe pre-commit.ci tool\npre-commit.ci\npre-commit\ncode-style\n.pre-commit-config.yaml"
    },
    {
        "objectID": "coding-style/formatting-tools",
        "href": "coding-style/formatting-tools.html#using-pre-commitci-with-conventional-commits",
        "title": "Code style tools > Using pre-commit.ci with conventional commits",
        "section": "Using pre-commit.ci with conventional commits",
        "text": "If you are using conventional commits in your project,\nvia the check PR title,\nit is important to ensure that the commit messages are compliant with the conventional commits standard.\nUse the following configuration in your .pre-commit-config.yaml file to be compliant:\nUsing pre-commit.ci with conventional commits\npre-commit.ci\n.pre-commit-config.yaml"
    },
    {
        "objectID": "content-writing/content-how-tos/resolve-issues-causing-check-failures",
        "href": "content-writing/content-how-tos/resolve-issues-causing-check-failures.html#resolve-issues-causing-check-failures",
        "title": "Resolve issues causing check failures",
        "section": "Resolve issues causing check failures",
        "text": "All checks in the CI/CD process must pass before a PR can be approved and\nmerged. Even if you remember to run pre-commit and Vale locally and resolve\nall issues, when you push your changes to the PR, checks in the CI/CD process\ncan still fail.\nFor information on continuous integration and the required workflows that run\nchecks on a PR for a PyAnsys library, see continuous_integration.\nWhen a check in the CI/CD process fails, you can click Details to the right\nof the check to view its log. You are taken to the bottom of the log, where you can see\nthe total number of warnings and errors.\nWhile you can scroll the log to find these warnings and errors, the log can be quite\nlong. Use the Search logs option in the upper right of the window to search\nfirst for warning and then for error. (This sequence is recommended because\nwarnings and errors are often related.)\nThe first occurrence of warning in the log is a hint about how to suppress a\nwarning about the name of the initial branch of the repository. This is not\na warning included in the count to resolve and can be safely ignored. Subsequent\noccurrences of warning, and then error, reveal the issues, with the filenames\nand lines where you must resolve them.\nYour objective should always be to eliminate or mitigate issues before creating\nor pushing changes to a PR. For example, running pre-commit and Vale locally\nlets you proactively resolve typos and trailing whitespaces. However,\noftentimes, you must resolve issues that cause the checks run by the CI/CD\nprocess to fail.\nResolve issues causing check failures\npre-commit\nwarning\nerror\nwarning\nwarning\nerror\npre-commit"
    },
    {
        "objectID": "content-writing/content-how-tos/resolve-issues-causing-check-failures",
        "href": "content-writing/content-how-tos/resolve-issues-causing-check-failures.html#resolve-typos",
        "title": "Resolve issues causing check failures > Resolve typos",
        "section": "Resolve typos",
        "text": "In PyAnsys projects, two tools in the CI/CD process check for typos,\ncodespell and Vale. codespell is one\nof the code style tools that pre-commit is configured to run. If\nyou always run pre-commit and Vale before creating or\npushing changes to a PR, these two checks in the CI/CD process\nshould not fail.\nFor more information, see run_precommit.\nFor more information, see run_Vale_locally.\nIf the Vale check on a PR fails, the Documentation style check also\nfails. Resolving the Vale issues might be all you need to do to get\nthe Documentation style check to pass successfully.\nResolve typos\ncodespell\npre-commit\npre-commit\nVale"
    },
    {
        "objectID": "content-writing/content-how-tos/resolve-issues-causing-check-failures",
        "href": "content-writing/content-how-tos/resolve-issues-causing-check-failures.html#resolve-trailing-whitespaces",
        "title": "Resolve issues causing check failures > Resolve trailing whitespaces",
        "section": "Resolve trailing whitespaces",
        "text": "If you forget to run pre-commit locally, when you create or push\nchanges to a PR, the CI/CD process is likely to raise errors about\ntrailing whitespaces. Rather than locating and resolving these errors manually,\nrun pre-commit locally to have it find and automatically trim these trailing\nwhitespaces for you. Then, commit and push the files that pre-commit has modified\nto the PR.\nResolve trailing whitespaces\npre-commit\npre-commit\npre-commit"
    },
    {
        "objectID": "content-writing/content-how-tos/resolve-issues-causing-check-failures",
        "href": "content-writing/content-how-tos/resolve-issues-causing-check-failures.html#resolve-formatting-issues",
        "title": "Resolve issues causing check failures > Resolve formatting issues",
        "section": "Resolve formatting issues",
        "text": "When you push changes to a PR, you might see one of the documentation checks fail,\neven if you ran pre-commit locally and all configured tools ran successfully.\nDocumentation check failures are usually the result of formatting issues and\nincorrect links, resulting in warnings and errors like these:\nFor each warning and error, a filename and line is indicated. You must go to these\nlocations and make the changes necessary to resolve the warnings and errors.\nActions that you might take include these:\nAdd missing start or end strings to various types of format tags.\nAdd a new RST file to the appropriate index.rst file.\nIndent all lines in a literal block correctly.\nAdd blank lines both before and after a list or literal block.\nCorrect links to internal labels and external targets.\nResolve formatting issues\npre-commit\nindex.rst"
    },
    {
        "objectID": "content-writing/content-how-tos/resolve-issues-causing-check-failures",
        "href": "content-writing/content-how-tos/resolve-issues-causing-check-failures.html#resolve-too-long-line-lengths-and-broken-links",
        "title": "Resolve issues causing check failures > Resolve too long line lengths and broken links",
        "section": "Resolve too long line lengths and broken links",
        "text": "In PyAnsys projects, Flake8 is a code style tool in the CI/CD process\nthat checks the quality of the Python code. When you run pre-commit locally,\nFlake8 is one of the tools that it is configured to run. If Flake8 finds a line in a\nPython file that is too long, it raises an error. Providing that this line is a\ndocstring or message string, you can manually change it in the PY file.\nSometimes, however, the line that is too long is for a URL added to the linkcheck_ignore\nvariable in the Sphinx configuration (doc/source/conf.py) file. Here is an example of how\nthis can happen. The central links (doc/source/links.rst) file for this guide contains\nexplicit target names for joining the two Ansys GitHub accounts:\nWhen building documentation, Sphinx checks all links to ensure that they are valid. In most cases,\nbroken links are the result of formatting errors that you must fix manually. However, the\nURLs for the preceding targets are behind firewall rules. Because Sphinx is unable to validate these links,\nit indicates that they are broken.\nBecause Sphinx is also unable to validate the 38-comments-and-docstrings\nanchor in the following named target to a section in the Google Python Style Guide, it identifies it as broken:\nTo resolve links that are identified as broken because they are behind firewall rules, you must add the\nURLs (and any comments about these URLs) to the linkcheck_ignore variable in the Sphinx\nconfig.py file. To resolve links with anchors that are identified as broken, you must\nadd the anchor to the linkcheck_anchors_ignore variable in the Sphinx config.py file.\nHere is what adding these lines looks like:\nIf you committed the preceding changes, Sphinx would no longer find any broken links. However, Flake8\nwould throw line length errors for the two lines that define the items for the linkcheck_ignore variable\nin the Sphinx config.py file. Because you cannot modify the length of these lines, you must follow\neach of these URLs (and any comment about it) with a space and then # noqa: 501.\nYou can scroll to the end of these lines to see how they now conclude with # noqa: 501:\nWhen you commit these changes, Flake sees the # noqa: 501 comments at the end of these lines\nand knows to ignore their long line lengths.\nResolve too long line lengths and broken links\npre-commit\nlinkcheck_ignore\ndoc/source/conf.py\ndoc/source/links.rst\n38-comments-and-docstrings\nlinkcheck_ignore\nconfig.py\nlinkcheck_anchors_ignore\nconfig.py\nlinkcheck_ignore\nconfig.py\n# noqa: 501\n# noqa: 501\n# noqa: 501"
    },
    {
        "objectID": "content-writing/content-how-tos/resolve-issues-causing-check-failures",
        "href": "content-writing/content-how-tos/resolve-issues-causing-check-failures.html#resolve-mismatched-message-strings",
        "title": "Resolve issues causing check failures > Resolve mismatched message strings",
        "section": "Resolve mismatched message strings",
        "text": "As indicated in py_message_strings, you want to ensure that the message\nstrings in PY files provide clear and understandable information or instructions\nto users. Sometimes, editing a message string can cause a test on the PR to fail.\nThis occurs when a test checks for the occurrence of a particular message string,\nbut this message string is no longer found in the PY file.\nWhen a test on a PR fails, you can click Details to the right of this test to\nsee the log. An error indicates that the message string in the test does not match\na message string in the PY file.\nTo quickly find the error, you can use the Search logs option in the\nupper right of the window to search for match=.\nTo resolve the error, you must open the indicated test file and edit the message\nstring in it to match the message string in the PY file.\nFind a test failure due to a message string mismatch to possible include an\nexample and ensure the the information provided in this topic is correct.\nResolve mismatched message strings\nTodo\nDetails\nmatch="
    },
    {
        "objectID": "getting-started/basic",
        "href": "getting-started/basic.html#pyansys-project-organization",
        "title": "PyAnsys project organization",
        "section": "PyAnsys project organization",
        "text": "The PyAnsys project is a collection of many\nPython packages for using Ansys products through Python. The\nAnsys organization on GitHub contains\nseveral repositories with Python libraries for interfacing with Ansys\nproducts or services. To go to the repository for a main PyAnsys library,\nvisit one of these links:\nPyAEDT\nPyAnsys Geometry\nPyDPF-Core\nPyDPF-Post\nPyMAPDL\nPyMAPDL Legacy Reader\nPyMechanical\nPyPIM\nIf you want to create, develop, or contribute to a PyAnsys library,\nvisit these links:\nPyAnsys developer’s guide\nAnsys Sphinx Theme documentation\ngRPC Hello-world example\nMaterial example data\nDevelopers use the following tools to generate library packages from\nPROTO files, create coverage reports, and report on system coverage:\npyansys-protos-generator\nexample-coverage\npyansys-tools-report\nPyAnsys project organization"
    },
    {
        "objectID": "getting-started/basic",
        "href": "getting-started/basic.html#pyansys-repository-creation",
        "title": "PyAnsys project organization > PyAnsys repository creation",
        "section": "PyAnsys repository creation",
        "text": "This is an overview on how to create your own PyAnsys repository in the\nAnsys GitHub organization. A repository is generally a project for a\nparticular PyAnsys library.\nCreate the repository: Create a repository from the\nansys/template repository. See Creating a repository from a template\nin the GitHub documentation. Be sure that the repository visibility is initially private.\nRename the package: Rename ansys/product/library to match\nyour product or library. For example, the package name for\nPyMAPDL is ansys/mapdl/core. Do the\nsame renaming in the setup.py file. Do this as a pull request. In fact, only add\ncode as pull requests. Do not push to the main branch of the repository.\nAdd source: Add your source files to\nansys/<product>/<library> or create them.  Also add unit tests to the\ntests directory, following the pytest convention. Be sure to maintain\nsufficient coverage when adding to your library. See the pytest-cov documentation.\nIf your tests require an active service, app, or product,\nbe sure to set it up to run in an automated manner.\nUpdate documentation: The documentation source and content\nvary from repository to repository. In the doc directory, there are child\ndirectories for different sections of the documentation, which can include getting\nstarted and user guides, examples, and an API reference. Ensure that all\ndocumentation is updated. See Documentation\nstyle.\nPrepare the package for release: When you are ready to release\nyour package publicly, email pyansys.core@ansys.com\nto obtain the release checklist for obtaining official Ansys approval.\nOnce you have completed this checklist, change the repository visibility\nto public and create a release branch.\ngRPC - Starting Guide\nC Extension - Starting Guide\nOthers like requests, RPC, COM, etc.\nPyAnsys repository creation\nTodo\nTodo\nTodo\nansys/product/library\nansys/mapdl/core\nsetup.py\nmain\nansys/<product>/<library>\ntests\ndoc"
    },
    {
        "objectID": "content-writing/py-files-writers/py-formatting",
        "href": "content-writing/py-files-writers/py-formatting.html#py-file-formatting",
        "title": "PY file formatting",
        "section": "PY file formatting",
        "text": "Python’s flexibility and object-oriented nature allows developers to create\nand work with a wide range of custom objects and data structures.\nPY file formatting"
    },
    {
        "objectID": "content-writing/py-files-writers/py-formatting",
        "href": "content-writing/py-files-writers/py-formatting.html#hierarchical-order-of-python-objects",
        "title": "PY file formatting > Hierarchical order of Python objects",
        "section": "Hierarchical order of Python objects",
        "text": "The next several topics describe the fundamental objects in Python,\nstarting with the most primitive data types and ending with more complex abstractions.\nFor explanations of important terms, see the Python Glossary\nin the official documentation published by the Python organization.\nHierarchical order of Python objects"
    },
    {
        "objectID": "content-writing/py-files-writers/py-formatting",
        "href": "content-writing/py-files-writers/py-formatting.html#primitive-data-types",
        "title": "PY file formatting > Primitive data types",
        "section": "Primitive data types",
        "text": "The primitive, or built-in, data types provided by Python represent simple values\nthat can be used to perform various operations and transformations to solve a wide\nrange of problems:\nInteger: Represents whole numbers, both positive and negative, that are used\nfor simple arithmetic operations and counting. For example, 5, -3, 0,\nand 1000. When defining this data type, int is used.\nFloat: Represents numbers with decimal points that are used for calculations\ninvolving non-integer values. For example, 3.14, -0.5, 2.0, and\n1.0e-5 (scientific notation). When defining this data type, float is used.\nIn PyAnsys libraries, parameters that specify angle, amplitude, capacitance,\nimpedance, resistance, and voltage have float as their data types.\nString: Represents a sequence of characters, such as text, that are used for\ntext processing, manipulations, and representation. For example, \"Hello, World!\",\n\"Python\", and \"123\". When defining this data type, str is used.\nBoolean: Represents binary values that are used to make logical decisions and\ncontrol program flow. For example, True and False. When defining this type\nof data, bool is used.\nPrimitive data types\n5\n-3\n0\n1000\nint\n3.14\n-0.5\n2.0\n1.0e-5\nfloat\nfloat\n\"Hello, World!\"\n\"Python\"\n\"123\"\nstr\nTrue\nFalse\nbool"
    },
    {
        "objectID": "content-writing/py-files-writers/py-formatting",
        "href": "content-writing/py-files-writers/py-formatting.html#collections",
        "title": "PY file formatting > Collections",
        "section": "Collections",
        "text": "Collections are data structures used in Python to group multiple values into a single\ncontainer so that the data can be organized and manipulated:\nList: An ordered collection of items separated by commas and enclosed in square\nbrackets []. Lists can contain elements of different data types, including integers,\nfloats, strings, and other objects. For example, [1, 2, 3, \"apple\", \"banana\"]. Lists\nare mutable, meaning you can change their content (add, remove, or modify elements).\nWhen defining this data type, list is used.\nTuple: An ordered collection of items separated by commas and enclosed in parentheses ().\nFor example, (1, \"apple\", 3.14). Tuples can contain elements of different data types,\nsimilar to lists. Tuples are immutable, meaning their content cannot be changed once created.\nWhen defining this data type, tuple is used.\nDictionary: A collection of key-value pairs enclosed in curly braces {}. For example,\n{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}. Each key is associated with a value,\ncreating a mapping between keys and their corresponding values. Dictionaries are unordered,\nmeaning they don’t guarantee the order of elements. When defining this data type, dict is used.\nSet: An unordered collection of unique elements enclosed in curly braces {}. For example,\n{1, 2, 3, 4}. Sets are useful for eliminating duplicate values from a list and performing set\noperations like union, intersection, and difference. When defining this data type, set is used.\nIn summary, both lists and tuples are used for ordered sequences, dictionaries for key-value mappings,\nand sets for managing unique elements.\nAlso, collections can be nested within one another, creating more complex data structures.\nCollections\n[]\n[1, 2, 3, \"apple\", \"banana\"]\nlist\n()\n(1, \"apple\", 3.14)\ntuple\n{}\n{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}\ndict\n{}\n{1, 2, 3, 4}\nset"
    },
    {
        "objectID": "content-writing/py-files-writers/py-formatting",
        "href": "content-writing/py-files-writers/py-formatting.html#functions",
        "title": "PY file formatting > Functions",
        "section": "Functions",
        "text": "Functions are at the same level in the Python object hierarchy as collections. Functions\nare used to operate on collections.\nFunctions"
    },
    {
        "objectID": "content-writing/py-files-writers/py-formatting",
        "href": "content-writing/py-files-writers/py-formatting.html#custom-objects",
        "title": "PY file formatting > Custom objects",
        "section": "Custom objects",
        "text": "Developers define custom objects to create their own data structures and behaviors.\nThis allows them to model real-world entities, concepts, or abstract data types in\ntheir programs, encapsulate data and behavior into reusable units, and create instances\nof these custom objects to work with.\nClass: A blueprint or template for creating objects (instances). A class defines\nthe structure and behavior of objects of that class. Encapsulating data and behavior\ninto a single unit hides implementation details and makes managing and maintaining\ncode easier. A class typically includes attributes (variables) to store data and methods\n(functions) to perform actions or operations related to the class. Classes are defined\nusing the class keyword, followed by the class name. A class often includes an __init__\nmethod to initialize object attributes.\nInstance: An individual object created from a class. An instance represents\na specific occurrence or realization of the class’s blueprint. Instances have their own\nunique data attributes and can perform actions using the methods defined in their class.\nYou can create multiple instances of the same class, each with its own state and behavior.\nMethod: A function defined within a class that is associated with instances of that class.\nMethods encapsulate behavior related to the class and can access and manipulate the\nobject’s attributes. Methods can be called on instances to perform specific operations or\ncomputations.\nAttribute: A variable defined within a class that is used to store data associated with\ninstances of the class. Attributes can represent characteristics or properties of objects\ncreated from the class. They can be accessed and modified through instances or methods.\nEnum: A custom object (data type) that defines a fixed set of named, constant values that\nare meaningful and need to be represented symbolically. Enums (enumerations) are created by\nsubclassing the enum module in Python and defining class attributes as the enum members,\nthereby providing a structured way to work with a set of symbolic names. Enums help improve\ncode readability and maintainability by providing descriptive names for values, making the\ncode more self-explanatory.\nCustom objects\nclass\n__init__\nenum"
    },
    {
        "objectID": "content-writing/py-files-writers/py-formatting",
        "href": "content-writing/py-files-writers/py-formatting.html#modules",
        "title": "PY file formatting > Modules",
        "section": "Modules",
        "text": "Modules are the PY files containing the Python code. They can include any of the preceding\nobjects. Modules, which use the UTF-8 encoding standard by default, organize the code into\nreusable units that can be imported into other Python scripts. For example, this line of\ncode imports the PyAEDT layout.py file:\nThe layout.py file contains two classes, EdbLayout and Shape. The EdbLayout\nclass manages EDB methods for primitives accessible from the Edb.modeler property. The\nShape class manages EDB methods for shapes. When you import this PY file into a Python\nscript, you can use all methods in these classes plus any functions defined in the file.\nIn a PY file, you should separate methods with one blank line and separate functions with two blank\nlines. Think of this as keeping the members of a class together versus separating functions,\nwhich are isolated.\nModules\nlayout.py\nlayout.py\nEdbLayout\nShape\nEdbLayout\nEdb.modeler\nShape"
    },
    {
        "objectID": "content-writing/py-files-writers/py-formatting",
        "href": "content-writing/py-files-writers/py-formatting.html#exceptions",
        "title": "PY file formatting > Exceptions",
        "section": "Exceptions",
        "text": "Exceptions can be raised during program execution to handle the error scenarios that can occur\nwhen working with any of the preceding objects. Exceptions can be caught and handled to prevent\nprogram crashes.\nExceptions"
    },
    {
        "objectID": "content-writing/py-files-writers/py-formatting",
        "href": "content-writing/py-files-writers/py-formatting.html#docstrings",
        "title": "PY file formatting > Docstrings",
        "section": "Docstrings",
        "text": "When developers create Python objects, they write docstrings, which are enclosed in triple\nquotation marks (\"\"\"), to describe these objects and explain how to use them programmatically.\nFor an overview of how PyAnsys libraries use NumPy-style docstrings, see numpy_docstrings.\nThis page links to descriptions for required docstring sections and provides general\ninformation on how to format the content in each section. For comprehensive\ninformation on NumPy-style docstrings and sections, see the numpydoc Style guide.\nBecause the docstrings for Python objects are read exponentially more by users than by\nthe developers who draft them, docstrings should be reviewed carefully, with suggestions in\nPRs suggesting how to improve and consistently format them across this and other PyAnsys libraries.\nBy using the same docstring conventions and consistent docstring formatting, PyAnsys users\ncan quickly locate and fully understand how to use PyAnsys libraries.\nBecause Python docstrings are written in reStructuredText syntax, they can\ncontain lists, notices, links, italic and bold formatting, inline code entities,\ncode blocks, and more, just like RST files.\nFor a summary of PyAnsys-specific docstring formatting rules, see docstring_formatting_rules.\nDocstrings\n\"\"\""
    },
    {
        "objectID": "content-writing/content-contrib-setup/essentials",
        "href": "content-writing/content-contrib-setup/essentials.html#essentials-for-content-writing",
        "title": "Essentials for content writing",
        "section": "Essentials for content writing",
        "text": "This page provides essential information for writing content for PyAnsys documentation.\nThese earlier topics are also related to documentation or contributing to a PyAnsys\nlibrary:\ndocumenting_developers provides an overview of key documentation concepts\nalong with some how-to information.\ndoc_style_developers provides valuable information about API documentation,\nSphinx configuration, NumPy-style docstrings, documentation  generation, and tools\nfor documentation style and coverage.\ncontributing provides the general coding paradigms for PyAnsys development\nthat you must understand before contributing to a PyAnsys library.\nEssentials for content writing"
    },
    {
        "objectID": "content-writing/content-contrib-setup/essentials",
        "href": "content-writing/content-contrib-setup/essentials.html#google-developer-documentation-style-guide",
        "title": "Essentials for content writing > Google developer documentation style guide",
        "section": "Google developer documentation style guide",
        "text": "All contributors to PyAnsys documentation must follow the guidelines in the\nGoogle developer documentation style guide.\nWhile you should become familiar with this entire style guide, periodically revisit the\nHighlights page to ensure that you are adhering to its most important points.\nWhen the Ansys templates tool is used to create a PyAnsys project from the\npyansys or pyansys-advanced template, Vale, a rule-based tool for maintaining\na consistent style and voice in your documentation, is implemented. This tool, which is one of\nmany run by the CI/CD process, is configured to check content in RST and Markdown (MD) files\nbased on the Google developer documentation style guide.\nTo eliminate or mitigate the number of warnings and errors that Vale raises in a PR, you can install\nVale locally and then run it before you create or submit changes to a PR. For more information,\nsee install_Vale_locally and run_Vale_locally.\nGoogle developer documentation style guide\npyansys\npyansys-advanced"
    },
    {
        "objectID": "content-writing/content-contrib-setup/essentials",
        "href": "content-writing/content-contrib-setup/essentials.html#pyansys-documentation",
        "title": "Essentials for content writing > PyAnsys documentation",
        "section": "PyAnsys documentation",
        "text": "On the right of the home page for a PyAnsys library’s GitHub repository, the About area\nhas a link to the library’s documentation. For example, here is the About area\nfor this guide:\nYou can also view the documentation for public PyAnsys libraries from the\nPyAnsys landing page or from the Ansys Python Manager by\nselecting Help > PyAnsys Documentation. For more information about this Python QA\napp, see Ansys_Python_Manager.\nAll links to PyAnsys documentation take you to documentation for the stable (latest)\nrelease because this is what users of the library generally want to see. In some cases,\nusers might want to see documentation for a legacy version of the library. Project contributors,\non the other hand, likely want to see the documentation for the development (main) branch of the\nlibrary.\nRather than hosting many separate documentation sites, the PyAnsys team supports enabling multi-versions,\nwhich makes it possible for you to select the documentation for different versions from a dropdown button\non the right side of the documentation title bar.\nThis dropdown button provides for selecting the documentation for the stable version, development (dev)\nversion, and three previous legacy versions by default. However, the selections it displays\ncan be customized. For more information on enabling and customizing this\ndropdown button, see multi_version_enabling.\ndev from this dropdown to view the documentation for the main branch.\nWhen you are viewing PyAnsys documentation, the right navigation pane typically\ndisplays Show Source and Edit on GitHub links. For information on using\nthe Show Source link to see how a page’s source file is formatted and how\nyou can reuse this content, see rst_file_formatting. For information on\nusing the Edit on GitHub link to use the GitHub web editor to submit\nsuggested changes to a page in a PR, see edit_on_GitHub.\nPyAnsys documentation"
    },
    {
        "objectID": "index",
        "href": "index.html#pyansys-developers-guide",
        "title": "PyAnsys developer’s guide",
        "section": "PyAnsys developer’s guide",
        "text": "The PyAnsys developer’s guide is the central\ndocument for:\nAnsys developers who want to create and “own” PyAnsys libraries\nAnyone in the Python community who wants to contribute to a\nPyAnsys library\nAnyone who is interested in learning more about the PyAnsys\nproject and PyAnsys libraries\nThis guide describes the PyAnsys project organization and administration.\nIt also provides how-to information and summarizes packaging, coding, and\ndocumentation styles. It concludes with an abstractions section that explains\nhow apps and complex services expose only functionalities that matter to users.\nFrom the About area for the repository, you can click the link to view\nthe latest web-based release of this guide. In the Releases\narea, you can view information about all releases. In the Assets  area for\nany release, you can download a PDF file of the guide.\nPyAnsys developer’s guide"
    },
    {
        "objectID": "index",
        "href": "index.html#getting-started",
        "title": "PyAnsys developer’s guide > Getting started",
        "section": "Getting started",
        "text": "An introduction to the PyAnsys project and its ecosystem.\nSteps to authorize the public release of a PyAnsys library.\nGetting started"
    },
    {
        "objectID": "index",
        "href": "index.html#explanations",
        "title": "PyAnsys developer’s guide > Explanations",
        "section": "Explanations",
        "text": "Step-by-step guidelines.\nExplanations of the PyAnsys architecture.\nExplanations"
    },
    {
        "objectID": "index",
        "href": "index.html#style-guidelines",
        "title": "PyAnsys developer’s guide > Style guidelines",
        "section": "Style guidelines",
        "text": "Best practices for distributing Python code.\nBest practices for writing Python code.\nBest practices for writing documentation.\nStyle guidelines"
    },
    {
        "objectID": "examples/pyvista_example",
        "href": "examples/pyvista_example.html#adding-a-new-gallery-example",
        "title": "Adding a new gallery example",
        "section": "Adding a new gallery example",
        "text": "This example shows how to add a new example to the PyAnsys Sphinx-Gallery. You can use this example as a template\nfor adding your examples.\nEach example should have a reference tag/key in the form:\n.. _<example-name>_example:.\nThe .. _ is necessary. Everything that follows is your reference tag, which\ncan potentially be used within a docstring. All references should be in snake case.\nThe first section, which is text, provides a brief overview of what the example is.\nWhen using this example as a template, you would change the title to an appropriate\none for your example.\nAdd new examples as Python scripts like this:\nexamples/<index>-<directory-name>/<some-example>.py\nAvoid creating directories unless absolutely necessary. If you must\ncreate a directory, make sure to add a README.txt file containing a\nreference, a title, and a one-sentence description of the directory.\nOtherwise, Sphinx ignores the new directory.\nExample file names should use snake case and be hyphen-separated:\nsome-example.py\nAfter this text section is the first code block. This is where you\ntypically set up your imports.\nAdding a new gallery example\n.. _<example-name>_example:\n.. _\nexamples/<index>-<directory-name>/<some-example>.py\nREADME.txt\nsome-example.py"
    },
    {
        "objectID": "examples/pyvista_example",
        "href": "examples/pyvista_example.html#section-title",
        "title": "Adding a new gallery example > Section title",
        "section": "Section title",
        "text": "Code blocks can be broken up with text sections, which are interpreted as\nReStructuredText.\nThe text sections are also translated into a markdown cell in the generated Jupyter\nnotebook or in the HTML documentation.\nText sections can contain any information that you may have regarding the example,\nsuch as step-by-step comments and notes regarding motivations.\nAs in Jupyter notebooks, if a statement is unassigned at the end of a code\nblock, output is generated and printed to the screen according to its\n__repr__ method. Otherwise, you can use the print() function to output text.\nSection title\n__repr__\nprint()"
    },
    {
        "objectID": "examples/pyvista_example",
        "href": "examples/pyvista_example.html#plots-and-images",
        "title": "Adding a new gallery example > Plots and images",
        "section": "Plots and images",
        "text": "If you use anything that outputs an image (for example, the\npyvista.Plotter.show() function), the resulting image is rendered in the\nHTML documentation.\nUnless sphinx_gallery_thumbnail_number = <int> is included at the top\nof the example script, the first figure (this one) is used for the\ngallery thumbnail image.\nAlso note that this image number uses one-based indexing.\nPlots and images\npyvista.Plotter.show()\nsphinx_gallery_thumbnail_number = <int>"
    },
    {
        "objectID": "examples/pyvista_example",
        "href": "examples/pyvista_example.html#caveat---plotter-must-be-within-one-cell",
        "title": "Adding a new gallery example > Caveat - plotter must be within one cell",
        "section": "Caveat - plotter must be within one cell",
        "text": "It’s not possible to have a single pyvista.Plotter object across\nmultiple cells because these are closed out automatically at the end of a\ncell.\nThis code exercise the pyvista.Actor repr to demonstrate\nwhy you might want to instantiate a plotter without showing it in the same\ncell:\nCaveat - plotter must be within one cell\npyvista.Plotter\npyvista.Actor\nrepr"
    },
    {
        "objectID": "examples/pyvista_example",
        "href": "examples/pyvista_example.html#this-cell-cannot-run-the-plotter",
        "title": "Adding a new gallery example > This cell cannot run the plotter",
        "section": "This cell cannot run the plotter",
        "text": "Because the plotter is already closed by Sphinx-Gallery, the following code\nwould raise an error:\nThis cell cannot run the plotter"
    },
    {
        "objectID": "examples/pyvista_example",
        "href": "examples/pyvista_example.html#create-a-pull-request",
        "title": "Adding a new gallery example > Create a pull request",
        "section": "Create a pull request",
        "text": "Once your example is complete and you’ve verified that it builds locally, you can\ncreate a pull request.\nBranches containing examples should be prefixed with docs/ as per Branch-naming conventions.\nYou only need to create the Python source example (PY file). Sphinx-Gallery\nautomatically generates the Jupyter notebook and the RST file for generating\nthe HTML documentation page.\nTotal running time of the script: (0 minutes 1.596 seconds)\nDownload Jupyter notebook: pyvista_example.ipynb\nDownload Python source code: pyvista_example.py\nDownload zipped: pyvista_example.zip\nGallery generated by Sphinx-Gallery\nCreate a pull request\ndocs/\nDownload Jupyter notebook: pyvista_example.ipynb\nDownload Python source code: pyvista_example.py\nDownload zipped: pyvista_example.zip"
    },
    {
        "objectID": "how-to/compatibility",
        "href": "how-to/compatibility.html#product-compatibility",
        "title": "Product compatibility",
        "section": "Product compatibility",
        "text": "As PyAnsys libraries evolve, backward and forward compatibility issues can\noccur. Here are examples of two common issues:\nAn Ansys product has implemented certain features in its new server version\nthat are not available in previous server versions. This causes backward\nincompatibility issues.\nNew server versions have suppressed support for certain operations after a\na given version. This causes forward incompatibility issues.\nAlthough there are different ways to handle these issues, some PyAnsys libraries,\nsuch as PyMAPDL and PyDPF-Core, handle them in\nthe same way. To homogenize implementations in PyAnsys libraries, following their\napproach is recommended.\nProduct compatibility"
    },
    {
        "objectID": "how-to/compatibility",
        "href": "how-to/compatibility.html#check_versionpy-module-approach",
        "title": "Product compatibility > check_version.py module approach",
        "section": "check_version.py module approach",
        "text": "A version checking module determines whether the Ansys product server you are connecting\nto provides support for certain operations. For an implementation example, see the\ncheck_version.py\nfile for the DPF-Core library.\nOne of the easiest ways to keep track of the versions supported is to set up a\nminimum version data structure in which forward compatibility is ensured.\nServer versions earlier than the minimum version do not have access to this\nfunctionality. The previously referenced check_version.py file uses the\nVERSIONS_MAP structure.\nMost Ansys products provide forward compatibility, meaning that features\nintroduced in an earlier version are also supported in later versions. Suppressing\na feature would lead to a backward compatibility issue.\nBecause the same type of issues can happen with the PyAnsys servers wrapping\nAnsys products, creating a similar maximum version data structure is\nis necessary. While there are no such implementations yet, it should work\nin the same way as the minimum version data structure.\ncheck_version.py module approach\ncheck_version.py\ncheck_version.py\nVERSIONS_MAP"
    },
    {
        "objectID": "how-to/compatibility",
        "href": "how-to/compatibility.html#version_requires-decorator",
        "title": "Product compatibility > version_requires decorator",
        "section": "version_requires decorator",
        "text": "The @version_requires decorator applies version logic to\nfunctionalities and methods available in the client. You can see how this\ndecorator is used in the check_version.py\nfile. Here is a generalized example:\nWhenever a client object is created and a call is made to any of its methods,\nsuch as meth_a(), the decorator checks that the version is later than the one\nspecified in the decorator’s call. If it is not, a VersionError is raised.\nThe self._server member of the class is the server that the client is connected to. This\nmember is expected to have version information in its self._server._server_version\nattribute. The decorator uses this version information to determine if the version is\nhigher than the threshold.\nYou can create a @max_version_supported decorator to implement this same\nkind of logic for forward incompatibility. Changing the @version_requires\ndecorator to @min_version_required is recommended.\nversion_requires decorator\nversion_requires\n@version_requires\nclient\nmeth_a()\nVersionError\nself._server\nself._server._server_version\n@max_version_supported\n@version_requires\n@min_version_required"
    },
    {
        "objectID": "how-to/repository-protection",
        "href": "how-to/repository-protection.html#repository-protection",
        "title": "Repository protection",
        "section": "Repository protection",
        "text": "Handling repositories implies handling sensitive information, especially\nwhen configuring workflows to access private servers. Because workflows\nacquire Ansys product licenses and handle secrets, it is important to\nimplement good protection rules.\nIn the following sections, different safety measures are presented.\nRepository protection"
    },
    {
        "objectID": "how-to/repository-protection",
        "href": "how-to/repository-protection.html#general-configuration",
        "title": "Repository protection > General configuration",
        "section": "General configuration",
        "text": "Being an owner or an administrator of a repository gives you access to the\nSettings menu. To access the general configuration settings for the repository,\nselect Settings > General.\nThe PyAnsys core team recommends choosing these settings in the Pull Requests\nsection:\nSelect Allow squash merging to force all commits of a pull request (PR)\nto be condensed into a single commit. This way, if a PR is not successful, it can\nbe reverted easily.\nSelect Default to pull request title for squash merge commits to\nprovide a uniform way of naming PRs and merging them to the main branch.\nSelect Always suggest updating pull request branches to update\nyour branch to the main branch before merging. (This can be\nenforced as explained later.)\nSelect Automatically delete head branches for cleanup purposes.\nOnce a PR is merged into the main branch, the PR-related branch is\ndeleted so that the repository contains only active branches.\nGeneral configuration\nmain\nmain\nmain"
    },
    {
        "objectID": "how-to/repository-protection",
        "href": "how-to/repository-protection.html#rulesets",
        "title": "Repository protection > Rulesets",
        "section": "Rulesets",
        "text": "Rulesets can be used to protect branches and tags. These can\nbe imported and exported from GitHub. The PyAnsys core team provides\ntemplate files for the branch protection and tag protection rulesets.\nRuleset files:\nMain branch protection ruleset\nBranch naming protection ruleset\nTag create\nTag delete\nPlease refer to the GitHub documentation\nto learn how to import and export rulesets.\nYou still need to manually set the rules for the required status checks\nas described below, as they usually are specific to the repository.\nRulesets\nMain branch protection ruleset\nBranch naming protection ruleset\nTag create\nTag delete"
    },
    {
        "objectID": "how-to/repository-protection",
        "href": "how-to/repository-protection.html#branch-protection",
        "title": "Repository protection > Branch protection",
        "section": "Branch protection",
        "text": "Branch protection is critical in terms of avoiding malicious code insertion and access\nto confidential data. To access the branch protection rulesets for the repository,\nselect Settings > Code and automation > Branches.\nNext to Branch protection rules, click Add rule.\nUnder Branch name pattern, type the name of the branch that you want to protect\n(usually main, but you can also protect other branches, such as gh-pages).\nRegular expressions (also known as regex) are\naccepted. For example, you might want to protect all release/ branches.\nThe PyAnsys core team recommends setting these rules for the main branch:\nSelect Require a pull request before merging so that only owners\nor administrators are able to directly merge to the main branch.\nSelect Require approvals to ensure that all PRs are reviewed. (PRs\ncreated by owners or administrators do not require approval.)\nSelect Require review from Code Owners so that code owners are forced to review\nall PRs to prevent malicious code from being merged into the main branch.\nCode owners should be able to identify pieces of code that are not allowed.\nTo ensure a code owner is always available to approve and merge PRs, creating an\nOwners/Admins team with multiple members is recommended.\nSelect Require status checks to pass before merging so that only\ncode that compiles, passes tests, and is formatted correctly can be merged. This\nis the sole purpose of CI/CD.\nSelect Require branches to be up to date before merging to ensure\nthat all code has been tested and is compatible with the main branch\nbefore it can be merged. This is an important concept because someone may have merged\ncode into the main branch that clashes with your code.\nSelect Require status checks to pass before merging so that the PR\ncannot be merged until all workflow checks are successful. The minimal checks to\nimplement are for code style, documentation style, build and unit testing,\ndocumentation building, and smoke tests.\nSelect Require conversation resolution before merging to force reviewers to\ngo through and resolve all comments. This ensures that all comments are read and\npossibly applied.\nFor other branches, conventional commits can be enforced by creating a rule\nwith the following parameters:\nEnforcement status: Active\nTarget branches:\n- Include: All branches\n- Exclude: main, gh-pages\nApplies to: Branch name\nRequirement: Must match a given regex pattern\nMatching pattern: ^(feat|fix|chore|docs|style|refactor|test|perf|ci|build|dependabot|release|maint)\\/.*\nDescription: Branch name must match the conventional commits pattern\nBranch protection\nmain\ngh-pages\nregex\nrelease/\nmain\nmain\nmain\nmain\nmain\nActive\nAll branches\nmain\ngh-pages\nBranch name\nMust match a given regex pattern\n^(feat|fix|chore|docs|style|refactor|test|perf|ci|build|dependabot|release|maint)\\/.*\nBranch name must match the conventional commits pattern"
    },
    {
        "objectID": "how-to/repository-protection",
        "href": "how-to/repository-protection.html#tag-protection",
        "title": "Repository protection > Tag protection",
        "section": "Tag protection",
        "text": "Protect tags so that only code owners and administrators can create them.\nTo access the tag protection settings for the repository, select Settings >\nCode and automation > Tags.\nFollowing the PyAnsys tagging convention, protect the  v* tag.\nTag protection\nv*"
    },
    {
        "objectID": "how-to/repository-protection",
        "href": "how-to/repository-protection.html#workflow-protection",
        "title": "Repository protection > Workflow protection",
        "section": "Workflow protection",
        "text": "Protect workflows in the settings for actions. The focus here is on forks,\nwhich let you make changes to a project without affecting the original repository. To\naccess the actions settings for the repository, select Settings > Actions > General.\nUnder Fork pull request workflows from outside collaborators, the preferred option\nis Require approval for all outside collaborators for repositories that are to be\nreleased publicly. The minimum option is Require approval for first-time contributors.\nBecause workflows contain sensitive information, it is important to preserve security and control.\nThe rules for workflows are more flexible. For example, if you have common outside collaborators who\nhas been contributing for some time, you may want to add them as members of the repository so that\ntheir PR workflows do not have to be accepted every time that they intend to run them.\nInternal and private repositories are only available to organization users and repository members,\nrespectively. Thus, no specific rules for outside collaborators are needed.\nWorkflow protection"
    },
    {
        "objectID": "content-writing/content-how-tos/request-bot-review",
        "href": "content-writing/content-how-tos/request-bot-review.html#request-a-bot-review",
        "title": "Request a bot review",
        "section": "Request a bot review",
        "text": "You can have the Ansys Review Bot perform a review of your changes.\nIf this bot is already turned on in your project, this comment from the bot\ndisplays on the Conversation page of the PR:\nIf your project is in the Ansys internal account, the comment that you write\nis @ansys-internal-reviewer-bot review. This topic assumes that you\nwrite @ansys-reviewer-bot review because your project is public.\nIf you do not see a comment from the bot but want to use it, email\npyansys.core@ansys.com to request that they turn\nthe bot on in your project.\nTo use the bot:\nCopy @ansys-reviewer-bot review from the bot’s comment.\nScroll to the bottom of the page, paste this comment in the Write tab, and click Comment.\nThe bot responds with this comment:\nDepending on how large your PR is, it may take a few minutes before comments\nfrom the bot begin to appear. If comments never appear, the bot service has\nlikely stopped responding. You can email pyansys.core@ansys.com\nto request that they restart the service.\nResolve bot comments just like you resolve other reviewer comments. For more\ninformation, see resolve_reviewer_comments.\nYou can run the bot again when you next push changes to the PR.\nRequest a bot review\n@ansys-internal-reviewer-bot review\n@ansys-reviewer-bot review\n@ansys-reviewer-bot review"
    },
    {
        "objectID": "content-writing/examples-writers/sphinx-gallery",
        "href": "content-writing/examples-writers/sphinx-gallery.html#use-sphinx-gallery",
        "title": "Use sphinx-gallery",
        "section": "Use sphinx-gallery",
        "text": "The sphinx_gallery extension (Sphinx-Gallery) is\nused to generate a gallery of examples from RST (or TXT) files, Python scripts\n(PY files), and even Jupyter notebooks (IPYNB files). The RST (or TXT) files\nintroduce the one or more sections of examples. The PY and IPYNB files provide\nthe corresponding standalone, downloadable code. Sphinx-Gallery generates an HTML\npage for each example, as well as both Python and Jupyter notebook files, which\nusers can download and execute.\nUse sphinx-gallery"
    },
    {
        "objectID": "content-writing/examples-writers/sphinx-gallery",
        "href": "content-writing/examples-writers/sphinx-gallery.html#set-up-examples",
        "title": "Use sphinx-gallery > Set up examples",
        "section": "Set up examples",
        "text": "When using Sphinx-Gallery, you place the files for the Examples section in the\nexamples directory. An RST (or TXT) file in this directory provides the\nintroductory content for the section. In most libraries, a Readme.txt file\nis used.\nIn a larger library, you typically organize examples in subdirectories by categories.\nAn RST or TXT file in each subdirectory then provides the introductory content for the\nexamples in this category.\nYou also place the PY and IPYNB files in the examples directory or subdirectories.\nThese files contain the code and documentation explaining this code.\nFor Python files, you must use a specific structure and reStructuredText syntax. For more\ninformation, see Structuring Python scripts for Sphinx-Gallery\nin the Sphinx-Gallery documentation. For information on adding a new example in a PY\nfile, see adding_a_new_gallery_example. You can use this example as a template\nfor creating Python files for Sphinx-Gallery.\nFor IPYNB files, you use Markdown cells to provide section headings and to provide\ncontext and explanations for the code in the code cells.\nSet up examples\nExamples\nexamples\nReadme.txt\nexamples"
    },
    {
        "objectID": "content-writing/examples-writers/sphinx-gallery",
        "href": "content-writing/examples-writers/sphinx-gallery.html#configure-the-use-of-sphinx-gallery-in-the-sphinx-configuration-file",
        "title": "Use sphinx-gallery > Configure the use of Sphinx-Gallery in the Sphinx configuration file",
        "section": "Configure the use of Sphinx-Gallery in the Sphinx configuration file",
        "text": "To build the “Examples” section, developers configure the use of Sphinx-Gallery in\nthe project’s Sphinx configuration (doc/source/conf.py) file. After installing\nand adding this extension to the extensions variable as described in\nadd_sphinx_extensions, developers configure the sphinx_gallery_conf variable.\nThis variable declares many values, including examples_dirs for the relative path to the\nexamples directory and gallery_dirs for the relative path where the gallery-generated\noutputs are saved.\nWhile the values declared by the sphinx_gallery_conf variable vary from project to\nproject, here is what this variable looks like in the conf.py file for PyAEDT:\nConfigure the use of Sphinx-Gallery in the Sphinx configuration file\ndoc/source/conf.py\nextensions\nsphinx_gallery_conf\nexamples_dirs\nexamples\ngallery_dirs\nsphinx_gallery_conf\nconf.py"
    },
    {
        "objectID": "content-writing/examples-writers/sphinx-gallery",
        "href": "content-writing/examples-writers/sphinx-gallery.html#add-sphinx-gallery-to-the-documentation-requirements",
        "title": "Use sphinx-gallery > Add Sphinx-Gallery to the documentation requirements",
        "section": "Add Sphinx-Gallery to the documentation requirements",
        "text": "To include Sphinx-Gallery in your project’s documentation requirements, you must\nadd it as a dependency. Depending on the project’s configuration, you add the required pip\npackages in either the pyproject.toml file or the requirements_doc_txt file.\nFor more information, see doc_ext_requirements.\nAdd Sphinx-Gallery to the documentation requirements\npip\npyproject.toml\nrequirements_doc_txt"
    },
    {
        "objectID": "all-styles",
        "href": "all-styles.html#style",
        "title": "Style",
        "section": "Style",
        "text": "In the PyAnsys ecosystem, three key styles contribute to an\nenhanced developer experience:\nPackaging style: Focuses on creating clear, GitHub-hosted open source APIs, allowing\nfor reusable packages that can be updated independently of the Ansys release schedule.\nCoding style: Ensures that code adheres to PEP 8 and aligns with the conventions of\nmajor data science packages like NumPy, SciPy, and pandas for consistency and readability.\nDocumentation style: Emphasizes the significance of cohesive and user-friendly\ncontent and well-documented APIs to improve the on-boarding experience and increase library adoption.\nAll PyAnsys libraries follow the Google developer documentation style guide, which includes using sentence-case titles, active voice,\npresent tense, and clear, concise sentences.\nBest practices for distributing Python code.\nBest practices for writing Python code.\nBest practices for writing PyAnsys library documentation.\nStyle"
    },
    {
        "objectID": "examples/index",
        "href": "examples/index.html#general-example",
        "title": "General example",
        "section": "General example",
        "text": "This gallery consists of introductory example using PyVista.\nsphx_glr_examples_pyvista_example.py\nGallery generated by Sphinx-Gallery\nGeneral example"
    },
    {
        "objectID": "packaging/build-systems",
        "href": "packaging/build-systems.html#build-system",
        "title": "Build system",
        "section": "Build system",
        "text": "The build system is a fundamental tool for packaging Python\nlibraries. It generates distribution files that can be shared with\nusers and developers.\nBuild system"
    },
    {
        "objectID": "packaging/build-systems",
        "href": "packaging/build-systems.html#artifacts",
        "title": "Build system > Artifacts",
        "section": "Artifacts",
        "text": "The build system allows maintainers to generate artifacts for their Python\nlibraries. Here, artifacts refers to both wheel and source files:\nWheel files have a WHL extension.\nSource files have a TAR.GZ or ZIP extension.\nThese are the files that you upload to PyPI when releasing a new version of a\nPyAnsys project.\nNot all files are included by default in the source distribution. A MANIFEST.in\nfile is required at the root of the project to specify additional\nfiles. For more information, see Controlling files in the distribution\nin the Setuptools documentation.\nThe interaction between the maintainer and the build system is performed using a\nbuild system tool. This tool provides both a frontend and a backend. The maintainers\ntrigger the frontend, which then calls the backend to read the\nproject directory and generate the artifacts.\nArtifacts\nMANIFEST.in"
    },
    {
        "objectID": "packaging/build-systems",
        "href": "packaging/build-systems.html#pep-517-and-pep-518",
        "title": "Build system > PEP 517 and PEP 518",
        "section": "PEP 517 and PEP 518",
        "text": "For a long time, the setup.py file was the only way of specifying the\nproject structure, metadata, and installation workflow that pip was to follow.\nHowever, having to execute a Python file when installing a Python package\nintroduced the following problems:\nIt was not possible to know which dependencies required the setup.py file\nto be properly executed.\nThe default Python package installer, pip, expected Setuptools\nto be the default build system tool, excluding others like Flit and Poetry.\nThese problems led to the acceptance of PEP 517 and PEP 518.\nPEP 517 and PEP 518\nsetup.py\nsetup.py"
    },
    {
        "objectID": "packaging/build-systems",
        "href": "packaging/build-systems.html#pep-517",
        "title": "Build system > PEP 517",
        "section": "PEP 517",
        "text": "PEP 517 allows Python developers to specify the build-backend tool for\ngenerating artifacts. The earlier image shows the most popular backends:\nSetuptools, while very popular, lacks the ability to declare build-time dependencies\nand is difficult to extend.\nFlit is a lightweight build system tool for Python.\nPoetry focuses on dependency management and environment isolation.\nPEP 517 introduced the build-backend key inside the\n[build-system] table in the pyproject.toml file.\nPEP 517\nbuild-backend\n[build-system]\npyproject.toml"
    },
    {
        "objectID": "packaging/build-systems",
        "href": "packaging/build-systems.html#pep-518",
        "title": "Build system > PEP 518",
        "section": "PEP 518",
        "text": "In addition to the setup.py file, PEP 518 includes a project file named\npyproject.toml. Its main goal is to specify build-time dependencies.\nHowever, some build-system tools like Flit or Poetry are able to specify all\nproject metadata inside the pyproject.toml file and eliminate the need to use\nthe setup.py file.\nTo specify the build-time requirements, the [build-system] table must be\ndeclared in the pyproject.toml file. Within it, the requires key is\nassigned to a list of strings, which are the build-time requirements.\nThe combination of PEP 517 and PEP 518 leads to the following syntax in a\npyproject.toml file:\nPEP 518\nsetup.py\npyproject.toml\npyproject.toml\nsetup.py\n[build-system]\npyproject.toml\nrequires\npyproject.toml"
    },
    {
        "objectID": "packaging/build-systems",
        "href": "packaging/build-systems.html#build-backend-tools",
        "title": "Build system > Build-backend tools",
        "section": "Build-backend tools",
        "text": "This section lists some of the most popular build systems in the\nPython ecosystem. Although all of them achieve the same goal, there are a few\ndifferences regarding their capabilities and the way of specifying project\nmetadata.\nBuild-backend tools"
    },
    {
        "objectID": "packaging/build-systems",
        "href": "packaging/build-systems.html#setuptools",
        "title": "Build system > Setuptools",
        "section": "Setuptools",
        "text": "Setuptools has been a part of the Python ecosystem for a long time. Unless\nyou require high control over your project’s installation steps, you should use\nFlit or Poetry.\nIf you do not need a dynamic installation process, you can consider using a\nsetup.cfg file. However, the setup.py file is still required. The setup.cfg file\nshould have a call to the setup function to act as the entry point of the\nbuild backend system.\nAll of these setuptools metadata fields are valid and must be\nspecified either in the setup.py or setup.cfg file.\nSetuptools\nsetup.cfg\nsetup.py\nsetup.cfg\nsetup\nsetup.py\nsetup.cfg"
    },
    {
        "objectID": "packaging/build-systems",
        "href": "packaging/build-systems.html#flit",
        "title": "Build system > Flit",
        "section": "Flit",
        "text": "Flit is a modern and lightweight build system that requires developers\nto manage virtual environments on their own. Developers must:\nCreate a virtual environment and activate it.\nInstall the package in editable mode.\nFlit is the default tool for creating a new PyAnsys project when using the\nAnsys templates.\nThe [project] section specifies the project’s metadata and required dependencies.\nFor more information, see The pyproject.toml config file\nin the Flit documentation.\nFlit\n[project]"
    },
    {
        "objectID": "packaging/build-systems",
        "href": "packaging/build-systems.html#poetry",
        "title": "Build system > Poetry",
        "section": "Poetry",
        "text": "Poetry has a poetry.lock file, which provides strong dependency pinning. When\ninstalling a package, Poetry creates a virtual environment, thus ensuring an isolated\npackage development environment.\nNevertheless, it is possible to make Poetry ignore the poetry.lock file with this\ncommand:\nUsing Poetry is popular because it offers these features:\nSupports pinning dependency versions using a poetry.lock file that can be\nused for testing and CI\nAllows downstream packages to still consume a loose dependency specification\nIntegrates with dependabot to update the pinned version\nThe [tool.poetry] section contains metadata and defines project\ndependencies. For more information, see The pyproject.toml file\nin the Poetry documentation.\nPoetry\npoetry.lock\npoetry.lock\npoetry.lock\n[tool.poetry]"
    },
    {
        "objectID": "content-writing/content-how-tos/show_cheat_sheet_thumbnail",
        "href": "content-writing/content-how-tos/show_cheat_sheet_thumbnail.html#show-cheat-sheet-thumbnail",
        "title": "Show cheat sheet thumbnail",
        "section": "Show cheat sheet thumbnail",
        "text": "If a cheat sheet exists for a PyAnsys library, you can show a thumbnail of it in the\nleft navigation pane of one or more documentation pages. For example, this image\nshows a cheat sheet thumbnail on the landing page of the PyMechanical documentation:\nTo show a thumbnail, you add a child cheatsheet dictionary to the Sphinx\nconfiguration (conf.py) file in the doc directory as described in\nCheat Sheets\nin the Ansys Sphinx Theme documentation.\nShow cheat sheet thumbnail\ncheatsheet\nconf.py\ndoc"
    },
    {
        "objectID": "getting-started/index",
        "href": "getting-started/index.html#getting-started",
        "title": "Getting started",
        "section": "Getting started",
        "text": "The PyAnsys project exposes Ansys technologies\nin client libraries within the Python ecosystem. Each library provides clear,\nconcise, and maintainable APIs. Useful Pythonic functions, classes, and plugins\nprovide for interacting with targeted products and services in a high-level,\nobject-orientated approach.\nThe PyAnsys ecosystem refines the component-level interaction\nwith Ansys solvers and tools. It also eliminates the inconsistent and\nrestrictive scripting environments found within product\ninstallations. For more information, see componentizing.\nAdditionally, libraries play vital roles in key simulation tasks,\nincluding these:\nApplication automation\nMachine learning\nPostprocessing\nData visualization\nWorkflow orchestration\nData manipulation and export\nLibraries also include plugins and interfaces to packages in the vast Python\necosystem. Here are some examples:\nArrays using NumPy\nData structures and tables using pandas\n2D visualization using Matplotlib\n3D visualization using PyVista\nAdvanced scientific computing using SciPy\nMachine learning using TensorFlow\nIf you are new to GitHub and open source projects, see The ReadMe Project. This monthly newsletter highlights\nthe best from the open source software community, providing links\nto feature articles, developer stories, guides, and podcasts.\nGetting started"
    },
    {
        "objectID": "getting-started/index",
        "href": "getting-started/index.html#contributing-to-this-guide",
        "title": "Getting started > Contributing to this guide",
        "section": "Contributing to this guide",
        "text": "If you would like to contribute to this guide, maintainers gladly\nreview all pull requests. For more information, see Documentation style.\nThis repository uses pre-commit to\nautomate style checking. To use it, enter your Python environment and install\npre-commit with this command:\nYou can then run pre-commit manually with this command:\nThis performs various style and spelling checks to ensure your contributions\nmeet minimum coding style and documentation standards.\nYou can make sure that these checks are always run prior to git commit\nrunning them by installing pre-commit as a Git hook with this command:\nNow, each time you run git commit, your commit is only created if it\npasses the minimum style checks that also run on the GitHub CI/CD.\nContributing to this guide\npre-commit\npre-commit\ngit commit\npre-commit\ngit commit"
    },
    {
        "objectID": "content-writing/content-how-tos/edit-on-GitHub",
        "href": "content-writing/content-how-tos/edit-on-GitHub.html#edit-on-github",
        "title": "Edit on GitHub",
        "section": "Edit on GitHub",
        "text": "The easiest way to contribute to a PyAnsys project is to make changes in GitHub,\nletting the CI/CD process handle the build. When you are viewing PyAnsys\ndocumentation, the right navigation pane might display an Edit on GitHub link.\nYou can use this feature to submit changes to this page by creating a PR on GitHub:\nClick the Edit on GitHub link.\nThe GitHub web editor opens with the Edit tab active by default.\nYou can search within the file using ctrl+F.\nMake suggested changes to the file.\nWhen finished, in the top right corner of the window, click Commit\nchanges.\nThe Propose changes window opens.\nSupply a commit message and an optional extended description.\nBecause you must create a new branch for committing this suggestion\nand creating the PR, supply a branch name.\nFor PRs related to documentation, your branch name should begin with\ndoc/ followed by a descriptive name. For example, you might supply\ndoc/fix_broken_link if that is what your suggested change does. For\nmore information, see branch_naming.\nClick Propose changes.\nThe PR is created, and the checks configured by the CI/CD process run. Your next\nsteps are basically the same as if you had created the PR from a cloned copy of\nthe repository:\nResolve failed checks.\nDownload and view documentation artifacts.\nTag reviewers.\nResolve reviewer comments.\nMerge your PR.\nFor more information, see create_pr.\nIf you need to make changes to other files as part of this PR, you can use\nyour preferred GitHub tool to check out the branch and work in it.\nEdit on GitHub\ndoc/\ndoc/fix_broken_link"
    },
    {
        "objectID": "content-writing/content-how-tos/view-revision-history",
        "href": "content-writing/content-how-tos/view-revision-history.html#view-revision-history-on-github",
        "title": "View revision history on GitHub",
        "section": "View revision history on GitHub",
        "text": "GitHub blame is a feature that lets you view the revision history of a\nfile in a repository to see who made changes and when these changes were\nmade. You can use the GitHub blame feature for a variety of use cases, including\nthese:\nKnowledge expansion: When implementing something new or making changes in unfamiliar areas\nof the codebase, you can use GitHub blame to see how someone else implemented or changed\nsomething similar.\nCode review: When reviewing code changes, you can use GitHub blame to understand why\nspecific lines were modified and by whom, making it easier to provide feedback or\nask questions.\nDebugging: When you encounter a bug or an issue in the code, you can use GitHub blame\nto help identify when and where a particular piece of problematic code was introduced.\nAccountability: When you need to resolve disputes or track collaborations, you can use\nGitHub blame to see who made specific changes.\nHere is how you use GitHub blame:\nNavigate to a specific file in a GitHub repository.\nTo see the revision history for the entire file, click the Blame tab in the\nfile header.\nTo go to the revision history for a particular line of code, on the Code tab,\nclick this line number, and when the box with an ellipsis (...) appears, click\nthis box and select View git blame.\nThis takes you to the selected line on the Blame tab. However, you can always\nscroll to see the revision history for all lines in the file.\nOn the right side, you see how long ago a change was made to the file or a specific line,\nwho made the change, and the PR in which the change was made. Clicking the PR takes\nyou to it so that you can view all changes made to all files in the PR.\nView revision history on GitHub\n..."
    },
    {
        "objectID": "content-writing/index",
        "href": "content-writing/index.html#content-writing",
        "title": "Content writing",
        "section": "Content writing",
        "text": "Earlier sections of this guide are written primarily for PyAnsys developers by PyAnsys\ndevelopers. This section is written for anyone who wants to contribute new or revise\nexisting content in the documentation for a PyAnsys library. The goal is to\nprovide content contributors with the information that they need to write clear, consistent,\neffective, and user-friendly content in the order in which they need to know it.\nComprehensive information on content writing is organized as follows:\ncontent_contrib_setup: Describes PyAnsys libraries, explains how to set up a\ncontent development environment, and provides links to many resources relevant to\ncreating and maintaining PyAnsys documentation.\nrst_files_writers: Explains how reStructuredText (RST) files define the hierarchy\nof the documentation and provide manually authored content. This section also describes\nhow to view and reuse the formatting of any documentation page and summarizes the\nformatting rules to follow so that your content contributions are rendered correctly.\npy_files_writers: Explains how docstrings in Python (PY) files provide\ndescriptions for the Python objects that are used to interact with a PyAnsys library. This\nsection also explains how PY files are set up and summarizes the formatting rules\nfor docstrings, code comments, and message strings.\nexamples_writers: Explains the Sphinx extensions that PyAnsys developers are\nusing to generate the examples in the “Examples” section of their PyAnsys documentation\nand how to format the source files so that the content in the renders correctly.\ncontent_how_tos: Explains how to perform tasks associated with contributing\nto PyAnsys documentation, including how to review and create GitHub PRs (pull requests).\nAs you become a more experienced contributor, this subsection and the resources_writers\npage are likely to be the content that you refer to most often.\nContent writing"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#project-structure",
        "title": "Project structure",
        "section": "Project structure",
        "text": "Most of the projects in the PyAnsys ecosystem ship in the form of a Python\nlibrary with other additional files. All these files form what it is called a\nproject. A project can be uploaded to a repository to better track the changes\napplied to it.\nProject structure"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#naming-convention",
        "title": "Project structure > Naming convention",
        "section": "Naming convention",
        "text": "Large organizations providing Python packages follow a consistent naming\nconvention. Ansys follows two naming conventions, depending on the nature of the project.\nNaming convention"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#pyansys-library",
        "title": "Project structure > PyAnsys library",
        "section": "PyAnsys library",
        "text": "The project name is in the format Py<project>. For example, PyAEDT is the\nproject name for AEDT (Ansys Electronics Desktop) and PyMAPDL is the\nproject name for MAPDL (an abbreviation for Mechanical APDL).\nThe repository name as hosted on GitHub should be all lowercase to follow\nGitHub community standards. For example, pyaedt and pymapdl.\nThe Python library name is in the format ansys-<product/service>-<feature>.\nFor example, ansys-mapdl-core\nis the name for the core MAPDL library.\nThe previous structure leads to the following namespace when executing the import\nstatement:\nUsing long Python library names provides two primary advantages:\nNamespace packages can be used to designate official Ansys packages.\nConsistent branding and style can be applied to PyAnsys libraries.\nPyAnsys library\nPy<project>\nPyAEDT\nPyMAPDL\nansys-<product/service>-<feature>"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#grpc-interface-package",
        "title": "Project structure > gRPC interface package",
        "section": "gRPC interface package",
        "text": "Lower-level gRPC interface packages like ansys-api-mapdl should always be\nnamed ansys-api-<product/service> and may contain an additional level:\nansys-api-<product/service>-<secondlevel>.\nThis structure leads to the following namespace within Protobuf (PROTO) files:\ngRPC interface package\nansys-api-<product/service>\nansys-api-<product/service>-<secondlevel>"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#python-libraries",
        "title": "Project structure > Python libraries",
        "section": "Python libraries",
        "text": "A Python library is the formal way of distributing Python source code. It allows\nfor reuse and for specifying Python code dependencies. Guidelines in this section\nare compliant with Python Packaging Authority (PyPA) and PyAnsys recommendations.\nThe best way to keep up to date with Python packaging is to check the Python\nPackaging User Guide, maintained by the PyPA. PyAnsys guidelines are built\non top of the PyPA guidelines.\nPython libraries"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#scripts-modules-subpackages-and-packages",
        "title": "Project structure > Scripts, modules, subpackages, and packages",
        "section": "Scripts, modules, subpackages, and packages",
        "text": "To understand the structure of a Python Library, it is important to know\nthe difference between Python scripts, modules, subpackages, and packages.\nScript: Any Python file with logic source code\nModule: Any Python script hosted next to an __init__.py file\nSubpackage: Any directory containing various Python modules\nPackage: Any directory containing Python modules and subpackages\nScripts, modules, subpackages, and packages\nScript\nModule\n__init__.py\nSubpackage\nPackage"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#differences-between-a-python-package-and-library",
        "title": "Project structure > Differences between a Python package and library",
        "section": "Differences between a Python package and library",
        "text": "Although the terms package and library are often used interchangeably, there is\na key difference between them. As shown in the following image, a Python package is\na collection of Python modules and subpackages, while a Python library is a collection\nof Python packages.\nDifferences between a Python package and library"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#required-files",
        "title": "Project structure > Required files",
        "section": "Required files",
        "text": "The structure of any PyAnsys library contains these directories and files:\nDescriptions follow for some of the directories in this structure:\ndoc: Contains files related to documentation, guidelines, and examples\nsrc: Contains all Python modules and scripts that form the project\ntests: Contains all unit tests for checking the integrity of the project\nsetup.py or pyproject.toml: Configures the project.\nRequired files\ndoc\nsrc\ntests\nsetup.py\npyproject.toml"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#the-doc-directory",
        "title": "Project structure > The doc directory",
        "section": "The doc directory",
        "text": "Prior to distributing software, it is important to document it. Documenting software\nconsists of explaining how to install and use all functions, classes, and methods that\nit ships with. The documentation should also include use case scenarios.\nA PyAnsys project typically has these documentation sections:\nGetting started: Defines requirements and provides installation information\nUser guide: Explains how to use the software\nAPI reference: Describes the source code\nExamples: Provides use case scenarios that demonstrate the capabilities of the software\nContribute: Links to the PyAnsys developer’s guide for overall guidance and supplies\nproject-specific contribution information\nProjects in the PyAnsys ecosystem take advantage of Sphinx, a tool for\nbuilding documentation for Python-based projects. Sphinx requires a doc\ndirectory with a specific structure:\n_build: Contains the rendered documentation in various formats, such as HTML\nand PDF.\nsource: Contains the RST files with the manually authored content. Folder\nand file names in this directory should use hyphens as space delimiters for search\noptimization of the generated HTML documentation.\nmake.bat and Makefile: Automates documentation cleaning and building\ncommands. You use make.bat when running on Windows and Makefile\nwhen running on macOS or Linux. For information on the required configuration for\nthese files, see Automation files.\nThe source directory must contain at least these files:\nconf.py: Python script that declares the Sphinx configuration.\nThe minimum required configuration is explained in The\n``conf.py`` file.\nindex.rst: Main index (landing) page for the overall documentation. Some\nprojects reuse README.rst files in the main index.rst file.\nFor more information, see readme_files. In newer projects, however, the index.rst\nfile uses a grid of cards to present the organization of the documentation in a visual manner.\nYou generally add any images or documents that you would like to include in a _static\ndirectory.\nThe doc directory\ndoc\nGetting started\nUser guide\nAPI reference\nExamples\nContribute\ndoc\ndoc\n_build\nsource\nmake.bat\nMakefile\nmake.bat\nMakefile\nsource\nconf.py\nindex.rst\nREADME.rst\nindex.rst\nindex.rst\n_static"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#the-src-directory",
        "title": "Project structure > The src directory",
        "section": "The src directory",
        "text": "All the Python source code must be located in the src directory. This is where the\nbuild system looks when generating the wheel and source distributions.\nThe names of directories and files in the src directory cannot contain spaces or hyphens.\nReplace these characters with an underscore (_).\nThe structure of the src directory determines the namespace of the PyAnsys\nlibrary. A namespace allow you to easily split subpackages from a package into\nsingle, independent distributions.\nThere are different approaches available for creating a namespace.\nAnsys namespaces use the native namespace packages from\nPEP 420.\nTherefore, the source directory of any PyAnsys library must look like this:\nThe src directory\nsrc\nsrc\nsrc\n_\nsrc\nsrc"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#the-tests-directory",
        "title": "Project structure > The tests directory",
        "section": "The tests directory",
        "text": "To guarantee the integrity of a PyAnsys project, a good test suite is required.\nPyAnsys projects use the pytest framework.\nA good practice is to emulate the structure of the src/ansys/product/library\ndirectory, although this is not always necessary.\nNotice the use of tests_* when creating child directories within the\ntests directory. For unit testing files, names use the test_*.py prefix.\nThis is the preferred way of naming directories and files in the\ntests directory.\nThe tests directory\ntests\nsrc/ansys/product/library\ntests_*\ntests\ntest_*.py\ntests"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#the-authors-file",
        "title": "Project structure > The AUTHORS file",
        "section": "The AUTHORS file",
        "text": "You use the AUTHORS file to specify the authorship of the repository. The\nAnsys Legal department has defined its format. You can add external contributors\nto this file on demand. Make sure that you adapt the project name on your\nspecific repository’s AUTHORS file.\nThe AUTHORS file\nAUTHORS\nAUTHORS\nAUTHORS"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#the-changelogmd-file",
        "title": "Project structure > The CHANGELOG.md file",
        "section": "The CHANGELOG.md file",
        "text": "You use the CHANGELOG.md file to collect new features, fixed bugs, documentation\nimprovements, and new contributors. It provides a summary of the latest\nenhancements to the project.\nThe CHANGELOG.md file\nCHANGELOG.md\nCHANGELOG.md"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#the-code_of_conductmd-file",
        "title": "Project structure > The CODE_OF_CONDUCT.md file",
        "section": "The CODE_OF_CONDUCT.md file",
        "text": "You use the CODE_OF_CONDUCT.md to specify how users, developers, and maintainers\nare to behave while working in the project. PyAnsys projects usually adopt the Contributor\nCovenant Code of Conduct, which is very popular across open source projects.\nThe CODE_OF_CONDUCT.md file\nCODE_OF_CONDUCT.md\nCODE_OF_CONDUCT.md"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#the-contributingmd-file",
        "title": "Project structure > The CONTRIBUTING.md file",
        "section": "The CONTRIBUTING.md file",
        "text": "You use the CONTRIBUTING.md file to provide a quick entry-point for developers\nwho are willing to contribute to the project. It usually provides references to\nthis information:\nWhere the source code of the project is hosted.\nWhich steps must be followed to install the software in “development” mode.\nWays of contributing to the source code.\nIdeally, the CONTRIBUTING.md file for a PyAnsys project should link\nto the PyAnsys developer’s guide for overall\nguidance.\nThe CONTRIBUTING.md file\nCONTRIBUTING.md\nCONTRIBUTING.md\nCONTRIBUTING.md"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#the-contributorsmd-file",
        "title": "Project structure > The CONTRIBUTORS.md file",
        "section": "The CONTRIBUTORS.md file",
        "text": "You use the CONTRIBUTORS.md file to list the contributors to the repository. Its\npurpose is to credit the authors for their individual contributions and provide a\nrecord of authorship for the codebase. Provide first and last names and\nlinks to GitHub usernames.\nThe CONTRIBUTORS.md file\nCONTRIBUTORS.md\nCONTRIBUTORS.md"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#the-license-file",
        "title": "Project structure > The LICENSE file",
        "section": "The LICENSE file",
        "text": "The LICENSE file provides the legal framework for the software. PyAnsys projects\nmust use the MIT License. Here is the template:\nJust because a software does not ship with a LICENSE file, it does not mean\nit is free or open source. If you need to use unlicensed software, contact\nits development team so that they can provide you with the correct license.\nThe LICENSE file\nLICENSE\nLICENSE\nLICENSE"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#the-readme-file",
        "title": "Project structure > The README file",
        "section": "The README file",
        "text": "Each PyAnsys library must have a README file in the root directory.\nThe preferred format of this file is reStructuredText Markup Syntax,\nalthough you can also use Markdown Syntax. While Markdown syntax has better\nGitHub support, you can reuse ReStructuredText (RST) files within Sphinx documentation.\nFor more information, see readme_files.\nThe README file should at the minimum contain these elements:\nPyAnsys library title\nGeneral description\nInstallation instructions (using pip install and git clone) but only if the library\nreuses README file content in its documentation\nWhile older projects tend to reuse content in their README.rst files in the\nmain index.rst files in their doc/source directories, newer projects do not.\nInstead, they provide a bulleted list with documentation links and descriptions in\ntheir “”README`` files. In the main index.rst files for their documentation,\nthey then use a grid of cards to visually explain and link to documentation sections.\nThis keeps the README file focused on why you might want to explore the\nlibrary and lets you quickly view documentation sections of interest.\nThe README.rst file is also reused within the project file metadata. It is\nusually included in the long-description field.\nThe README file\nREADME\nREADME\nREADME\npip install\ngit clone\nREADME\nREADME.rst\nindex.rst\ndoc/source\nindex.rst\nREADME\nREADME.rst\nlong-description"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#the-pyprojecttoml-file",
        "title": "Project structure > The pyproject.toml file",
        "section": "The pyproject.toml file",
        "text": "PEP 518 introduced the use of a project file named pyproject.toml.\nThis file is mandatory because it allows pip to resolve the\nrequirements for building the library. The following tabs expose the [build-system] section\nfor build-system backend tools commonly used in the Python ecosystem:\nThe pyproject.toml file\npyproject.toml\npyproject.toml\n[build-system]"
    },
    {
        "objectID": "packaging/structure",
        "href": "packaging/structure.html#the-setuppy-file",
        "title": "Project structure > The setup.py file",
        "section": "The setup.py file",
        "text": "For a long time, Python developers used the setup.py file to build and\ndistribute their libraries. Unlike a static pyproject.toml file, the\nsetup.py file is a Python script. This means that Python code is interpreted\nwhen building the library. This approach supports customizing the build\nprocess but can also introduce security issues.\nThe setup.py file is only compatible with Setuptools, which is why\nyou should consider using a pyproject.toml file instead.\nWhile you can use a setup.cfg file to specify the metadata and packages, the setup.py\nfile must also be present. For more information, see these pages in the Setuptools\ndocumentation:\nBuilding and Distributing Packages with Setuptools\nConfiguring setuptools using setup.cfg files\nAs a minimum configuration for a PyAnsys project, you can use this setup.py\ntemplate:\nThe setup.py file\nsetup.py\nsetup.py\npyproject.toml\nsetup.py\nsetup.py\npyproject.toml\nsetup.cfg\nsetup.py\nsetup.py"
    },
    {
        "objectID": "content-writing/py-files-writers/code-comments-message-strings",
        "href": "content-writing/py-files-writers/code-comments-message-strings.html#code-comments-and-message-strings",
        "title": "Code comments and message strings",
        "section": "Code comments and message strings",
        "text": "Python files often contain code comments and message strings.\nCode comments provide information to make the code more readable and maintainable.\nMessage strings display information to users. Developers can also use message\nstrings for debugging purposes to better understand the flow of the code.\nCode comments and message strings"
    },
    {
        "objectID": "content-writing/py-files-writers/code-comments-message-strings",
        "href": "content-writing/py-files-writers/code-comments-message-strings.html#code-comments",
        "title": "Code comments and message strings > Code comments",
        "section": "Code comments",
        "text": "Developers know that adding comments to code is a good practice when logic\nmight be hard to understand at first glance. They also know to avoid over-commenting\nor writing comments that explain the obvious.\nA code comment starts with the hash character (#) and a single space. It extends to\nthe end of the physical line. At least two spaces should separate the code from the comment.\nWhile the code comments in PY files are not visible to users of the library. they are visible\nto contributors of the library. Thus, when reviewing PY files, make suggestions for correcting\nmisspelled words and bad grammar.\nYou can also make suggestions for formatting code comments consistently. For example, you\nmight suggest starting all code comments in a PY file with a simple verb that always begins\nwith an uppercase letter. You might also suggest always concluding all code comments within\na code block with a period (.). Or, you might suggest that only code comments containing\nmultiple sentences should conclude with a period. Your objective is to ensure that code comments\nwithin a code block are well written and consistently formatted.\nCode comments\n#\n."
    },
    {
        "objectID": "content-writing/py-files-writers/code-comments-message-strings",
        "href": "content-writing/py-files-writers/code-comments-message-strings.html#message-strings",
        "title": "Code comments and message strings > Message strings",
        "section": "Message strings",
        "text": "Developers can create a simple message by assigning to a variable a text string that\nis enclosed in either two single ('') or two double (\"\") quotation marks. They can\ncreate a multi-line message using triple (\"\"\") quotation marks like those used\nfor docstrings. Additionally, developers can perform other operations on strings,\nsuch as concatenation, interpolation, slicing, and formatting.\nWhenever possible, use double quotation marks to surround a message string.\nIf you need to use quotation marks inside the message string, then use single quotation marks.\nThe only exception is if you need to use double quotations inside the message string to\nspecify possible options that are string values. In this case, use single quotation\nmarks to surround this line and double quotation marks for other options:\nWhen reviewing PY files, you want to ensure that message strings are clear\nand concise so that users can easily understand the information or instructions\nthat they provide.\nYou should always ensure that a message string is a complete sentence\nwith a concluding period. Message strings should not conclude with an exclamation\npoint (!) or use the word please.\nBecause tests written for a PyAnsys project might check that a particular message\nis shown, making changes to a message string could necessitate you having to also update\nany test that checks for it. For more information, see resolve_mismatched_message_strings.\nMessage strings\n''\n\"\"\n\"\"\"\n!"
    },
    {
        "objectID": "how-to/testing",
        "href": "how-to/testing.html#testing",
        "title": "Testing",
        "section": "Testing",
        "text": "Unit testing and integration testing are critical for the successful continuous\nintegration (CI) and delivery of any library belonging to the PyAnsys\nproject.\nIn 1993, Kent Beck developed Test Driven Development (TDD) as part\nof the Extreme Programming software development process. TDD is the practice\nof writing unit tests before writing production code. The benefit of this practice\nis that you know each new line of code is working as soon as it is written. It’s\neasier to track down problems because only a small amount of code has been implemented\nsince the execution of the last test. Furthermore, all test cases do not have to be\nimplemented at once but rather gradually as the code evolves.\nYou should follow TDD when developing your PyAnsys project. Examples\nand best practices for unit tests follow.\nTesting"
    },
    {
        "objectID": "how-to/testing",
        "href": "how-to/testing.html#test-framework",
        "title": "Testing > Test framework",
        "section": "Test framework",
        "text": "For consistency, PyAnsys tools and libraries should use either the pytest or\nunittest framework\nfor unit testing. The pytest framework is recommended unless a constraint\nin your project prevents you from using it. As described in Required files,\nyou should place unit tests in The ``tests`` directory in the library’s\nroot directory.\nTest framework\npytest"
    },
    {
        "objectID": "how-to/testing",
        "href": "how-to/testing.html#add-testing-dependencies",
        "title": "Testing > Add testing dependencies",
        "section": "Add testing dependencies",
        "text": "Requirements for testing dependencies should be included in The\n``setup.py`` file, The ``pyproject.toml`` file, or a\nrequirements_tests.txt file. Only pytest and pytest-cov\nmust be specified as third-party dependencies because unittest is included\nin The Python Standard Library.\nYou can use pip to install these testing dependencies:\nAdd testing dependencies\nrequirements_tests.txt\npytest\nunittest\npip"
    },
    {
        "objectID": "how-to/testing",
        "href": "how-to/testing.html#organize-test-files",
        "title": "Testing > Organize test files",
        "section": "Organize test files",
        "text": "You must place your test files in The ``tests`` directory. To\nguarantee that tests are run against the library source code, follow a src\nlayout as explained in The ``src`` directory rather than\nhaving your Python library source located directly in the repository root directory.\nThis helps you to achieve these objectives:\nAvoid testing the source of the repository rather than testing the installed package.\nCatch errors caused by files that might be missed by the installer, including any\nC extensions or additional internal packages.\nOrganize test files\nsrc"
    },
    {
        "objectID": "how-to/testing",
        "href": "how-to/testing.html#test-execution",
        "title": "Testing > Test execution",
        "section": "Test execution",
        "text": "Once you have installed pytest, you can execute the test suite with this command:\nTest execution\npytest"
    },
    {
        "objectID": "how-to/testing",
        "href": "how-to/testing.html#filter-tests",
        "title": "Testing > Filter tests",
        "section": "Filter tests",
        "text": "To run a subset of all available tests, you can taking advantage\nof the keywords and markers flags:\nFilter tests by keywords\nFilter tests by markers\nFor more information about filtering tests, see Working with custom markers in the pytest\ndocumentation.\nFilter tests\nkeywords\nmarkers\npytest"
    },
    {
        "objectID": "how-to/testing",
        "href": "how-to/testing.html#testing-methodology",
        "title": "Testing > Testing methodology",
        "section": "Testing methodology",
        "text": "You should consider three levels of testing for your PyAnsys library: unit,\nintegration, and functional.\nUnit testing validates your library at the lowest possible level, isolating\nindividual classes and methods without any communication with other libraries\nor services.\nIntegration testing validates that your library works in the context of an\napp or software stack. For example, if your library extends or wraps\nthe features of an external service, you must test that service\nin conjunction with your library. On GitHub, the ideal approach for this would\nbe to start your service using Docker and then test accordingly. You should still be\ntesting at the individual class or method level, but you can now test how\nmultiple libraries or services interact. This is mandatory for testing APIs and\nis preferred over mocking the service.\nFunctional testing should be used for validating workflows or long-running\nexamples. Assume that you have a library that wraps a CAD service. You\nwould validate that you can create complex geometry while directly interfacing\nwith the service. Functional tests are great at discovering edge cases that are\nnot normally found at the unit or integration level. However, functional testing\nshould be limited to only a handful of examples because these tend to be long\nrunning and difficult to validate.\nEach PyAnsys project should have all three levels of testing implemented in its\ntesting framework. Consider implementing functional tests as examples within\nyour project’s documentation examples. This lets you write helpful\nuser-facing tests while accomplishing functional testing.\nTesting methodology"
    },
    {
        "objectID": "how-to/testing",
        "href": "how-to/testing.html#unit-testing",
        "title": "Testing > Unit testing",
        "section": "Unit testing",
        "text": "Unit testing tests at the lowest possible level, isolated\nfrom other applications or libraries. For Python tool libraries like\nansys-tools-protoc-helper, unit testing is sufficient to get high coverage\n(> 80%) of your library while actually testing the library.\nThese tests should be written to test a single method in isolation. For example,\nthe following parse_chunks.py file has a method that deserializes chunks. The\nassociated test_parse_chunks_py file tests this method in isolation.\nThis example assumes that you do not have a serialize_chunks function in your\nlibrary. If you did, you could exclude it from the test_parse_chunks.py file.\nUnit testing\nparse_chunks.py\ntest_parse_chunks_py\nserialize_chunks\ntest_parse_chunks.py"
    },
    {
        "objectID": "how-to/testing",
        "href": "how-to/testing.html#integration-testing",
        "title": "Testing > Integration testing",
        "section": "Integration testing",
        "text": "This section explains Wrapped service methods and how to\nTest using remote method invocation.\nIntegration testing"
    },
    {
        "objectID": "how-to/testing",
        "href": "how-to/testing.html#wrapped-service-methods",
        "title": "Testing > Wrapped service methods",
        "section": "Wrapped service methods",
        "text": "Any PyAnsys library that provides features by wrapping a gRPC interface\nshould include tests of the gRPC methods exposed by the PROTO files and wrapped\nby the Python library. They would not be expected to test the features of\nthe server but rather the APIs exposed by the server. For example, if testing\nthe GetNode gRPC method, then your integration test would test the wrapped\nPython function. If the Python library wraps this gRPC method with a\nget_node method, your test would be implemented within the\ntests/test_nodes.py file.\nThe goal of the unit test should be to test the API rather than the product or\nservice. The GetNode gRPC method should have already been tested when\ndesigning and developing the service.\nWrapped service methods\nGetNode\nget_node\ntests/test_nodes.py\nGetNode"
    },
    {
        "objectID": "how-to/testing",
        "href": "how-to/testing.html#test-using-remote-method-invocation",
        "title": "Testing > Test using remote method invocation",
        "section": "Test using remote method invocation",
        "text": "For a Remote Method Invocation (RMI)-like method, it is only\nnecessary to test the method with a basic case and potentially with any edge\ncases. A RMI-like API might send and receive strings that are executed on the\nserver using a custom API or language only available within the context of the\nservice.\nFor example, if a method has a RMI service definition named SendCommand() and\na Python wrapping named send_command, your code and the example test would look\nlike this:\nNote that this test only validates that the \"CREATE,1\" command has been\nreceived, executed, and sent back to the client. It does not validate all\ncommands. Running such a test is necessary only if there are edge cases, which\ninclude characters that cannot be streamed or use long-running commands.\nTest using remote method invocation\nSendCommand()\nsend_command\n\"CREATE,1\""
    },
    {
        "objectID": "how-to/testing",
        "href": "how-to/testing.html#functional-testing",
        "title": "Testing > Functional testing",
        "section": "Functional testing",
        "text": "Functional testing should test the Python library using scripts or examples\nthat are expected to be executed by the user. Unlike unit or integration\ntesting, functional tests are testing the library as a whole by calling\nseveral methods to accomplish a task. You should run these tests only after unit\nand integration testing is complete. Ideally, you should run them outside the\npytest framework while building documentation with Sphinx-Gallery.\nFunctional tests should not contribute to global library coverage. Testing\nshould always be done on individual functions or methods.\nFunctional testing\npytest"
    },
    {
        "objectID": "how-to/testing",
        "href": "how-to/testing.html#test-code-coverage",
        "title": "Testing > Test code coverage",
        "section": "Test code coverage",
        "text": "Because Python is an interpreted language, syntax errors can only be\ncaught during the almost trivial compile times. Thus, developers of Python libraries\nshould aim to have high coverage for their libraries. Coverage is defined as parts\nof the executable and usable source that are tested by unit tests. You can use\nthe pytest-cov library to view the coverage for your library.\nTest code coverage"
    },
    {
        "objectID": "how-to/testing",
        "href": "how-to/testing.html#configure-code-coverage",
        "title": "Testing > Configure code coverage",
        "section": "Configure code coverage",
        "text": "If you do not configure code coverage properly, the resulting report does\nnot show the real scope covered by the test suite.\nAssuming that a PyAnsys project follows The ``src`` directory layout,\nyou must pass the following flag when executing tests:\nThis command tells pytest-cov to look for source code in the\nsrc/ansys/<product> directory and generate a terminal report for all tests\nlocated in The ``tests`` directory.\nWhile 100% coverage is ideal, the law of diminishing returns applies to\nthe coverage of a Python library. Consequently, achieving 80-90% coverage is\noften sufficient. For parts of your library that are difficult or impossible\nto test, consider using # pragma: no cover at the end of the method\ndefinition, branch, or line to denote that part of the code cannot be\nreasonably tested. For example, if part of your module performs a simple\nimport test of matplotlib and raises an error when the library is not\ninstalled, it is not reasonable to attempt to test this and assume full\ncoverage:\nYou should only avoid coverage of parts of your library where you cannot\nreasonably test without an extensive testing suite or setup. Most methods and\nclasses, including edge cases, can be reasonably tested. Even parts of your code\nthat raise errors like TypeError or ValueError when users input the\nwrong data type or value can be reasonably tested.\nConfigure code coverage\nPyAnsys\npytest-cov\nsrc/ansys/<product>\n# pragma: no cover\nimport\nmatplotlib\nTypeError\nValueError"
    },
    {
        "objectID": "how-to/testing",
        "href": "how-to/testing.html#enforce-code-coverage",
        "title": "Testing > Enforce code coverage",
        "section": "Enforce code coverage",
        "text": "One way of enforcing unit test coverage with a project on GitHub is to use\ncodecov.io to enforce minimum patch (and optionally project) coverage. Because\nthis app is already available to the Ansys GitHub organization, you can simply\nadd a codecov.yml file to the root directory of your repository. This example\nfile provides a sample configuration:\nUsing a codecov.yml file requires that each PR has a patch coverage of 90%, meaning that 90% of any\nsource added to the repository (unless ignored) must be covered by unit tests.\nEnforce code coverage\ncodecov.io\ncodecov.yml\ncodecov.yml"
    },
    {
        "objectID": "how-to/testing",
        "href": "how-to/testing.html#test-using-github-actions",
        "title": "Testing > Test using GitHub Actions",
        "section": "Test using GitHub Actions",
        "text": "Effective CI/CD assumes that unit testing is developed during feature\ndevelopment or bug fixes. However, given the limited scope of the local\ndevelopment environment, it is often not possible to enforce testing on\nmultiple platforms, or even to enforce unit testing in general. However, with the proper\nautomated CI/CD, such testing can still occur and be enforced automatically.\nGitHub Actions is the preferred automated CI/CD platform for running Python\nlibrary unit tests for PyAnsys. It can be used immediately by cloning the\nproject template.\nTest using GitHub Actions"
    },
    {
        "objectID": "getting-started/componentization",
        "href": "getting-started/componentization.html#componentizing-ansys-packages",
        "title": "Componentizing Ansys packages",
        "section": "Componentizing Ansys packages",
        "text": "Componentization is the process of subdividing the functionality of large apps\ninto multiple self-contained services with independent APIs. API creation surrounding\nexisting Ansys products naturally aligns to publishing packages that mimic the full\ndomain and scope of each product.\nEmphasizing component libraries and services during API exposure sets a new paradigm\nfor Ansys product architecture that inherently breaks apart larger monolithic desktop\napps into subsets of functionality, with the expectation of compatibility and reusability\nacross the entire Ansys portfolio.\nMany Ansys products already have a scripting solution in place, and wrapping that execution\nenvironment with a RunScript API endpoint is a low-barrier option to gain access to\nremote, programmatic execution. This solution lacks API granularity, however, as the abstraction\nis simply an unvalidated script input and some blob output that must be parsed and evaluated\nwithout a prescribed response definition.\nThe documentation for the scripting environment still remains deep within the product and\neach script execution request is hard to organize and maintain. Thus, there remains a\nsignificant cognitive disconnect when consuming this API abstraction. The lack of API\ndefinition within the top-level abstraction also makes data management difficult and direct\ncompatibility with other PyAnsys libraries challenging.\nIn addition to API clarity, the underlying product keeps a very large installation\nfootprint that is a burden in modern, flexible cloud deployments. While avoiding re-architecting\na product in the short-term can give a quick win, in most cases, it should only be used\nas a stopgap solution, providing a window of opportunity to learn more about how the user\nprefers to consume the individual functionalities of a product.\nCreating well-architected component libraries maximizes product usage with these key benefits:\nReusable shared components\nImproved API quality\nPackage size reduction\nProduct compatibility and composability\nOptimized, on-demand user solutions\nComponentizing Ansys packages\nRunScript"
    },
    {
        "objectID": "abstractions/service",
        "href": "abstractions/service.html#service",
        "title": "Service",
        "section": "Service",
        "text": "Some Ansys products are exposed as services that permit remote\nexecution using technologies like REST or gRPC.  These services\nare typically exposed in a manner where the API has already been\nabstracted because not all methods can be exposed through a remote API.\nHere, the abstraction of the service is as crucial as in the case of\nthe desktop API. In this case, remote API calls should be identical\nif the service is local or remote, with the only difference being that local\ncalls are faster to execute.\nConsider the following code examples. The left-hand side shows the\namount of work to start, establish a connection to, and submit an\ninput file to MAPDL using auto-generated gRPC interface files. For\nmore information, see pyansys-protos-generator.  The\nright-hand side shows the same workflow but uses PyMAPDL.\nUsing the gRPC Auto-generated Interface\nUsing the PyMAPDL Library\nThe approach on the right has a number of advantages, including:\nReadability due to the abstraction of the service startup\nShort package names\nSimplified interface for starting MAPDL\nFull documentation strings for all classes, methods, and functions\nTo properly abstract a service, you must have the option to\neither launch the service and connect to it locally if the software exists on\nyour machines or connect to a remote instance of the service. One\nway to do this is to include a function to launch the service.\nThis example includes the launch_mapdl function, which brokers a connection with\nthe Mapdl class:\nThis straightforward approach connects to a local or remote instance\nof MAPDL using gRPC by instantiating an instance of the Mapdl class.\nAt this point, because the assumption is that MAPDL is always remote, it’s\npossible to issue commands to MAPDL, including those requiring\nfile transfer like Mapdl.input.\nService\nlaunch_mapdl\nMapdl\nMapdl\nMapdl.input"
    },
    {
        "objectID": "doc-style/formatting-tools",
        "href": "doc-style/formatting-tools.html#documentation-style-tools",
        "title": "Documentation style tools",
        "section": "Documentation style tools",
        "text": "There are plenty of tools for documentation style and coverage. This section\npresents some of the most popular ones in the Python ecosystem. A minimum\nconfiguration is provided for each one so you can easily include them in your\nPyAnsys project.\nMost of the tools presented can be configured using The\n``pyproject.toml`` file, avoiding dotfiles and thus leading to a much\ncleaner root project directory.\nDocumentation style tools"
    },
    {
        "objectID": "doc-style/formatting-tools",
        "href": "doc-style/formatting-tools.html#the-blacken-docs-tool",
        "title": "Documentation style tools > The blacken-docs tool",
        "section": "The blacken-docs tool",
        "text": "When writing documentation, code blocks are frequently used to provide examples.\nHowever, these code snippets cannot be verified with the usual code\nformatting tools. This is where blacken-docs comes into play. You can execute\nthis tool by running this command:\nThe blacken-docs tool\nblacken-docs"
    },
    {
        "objectID": "doc-style/formatting-tools",
        "href": "doc-style/formatting-tools.html#the-codespell-tool",
        "title": "Documentation style tools > The codespell tool",
        "section": "The codespell tool",
        "text": "The codespell tool checks for common misspellings in text files. This implies that it\nis not limited to Python files but can run checks on any human-readable file.\nIt is possible to ignore words that are flagged as misspelled. You can specify these words in a\nfile that can then be passed to codespell by running this command:\nThe codespell tool\ncodespell\ncodespell"
    },
    {
        "objectID": "doc-style/formatting-tools",
        "href": "doc-style/formatting-tools.html#the-docformatter-tool",
        "title": "Documentation style tools > The docformatter tool",
        "section": "The docformatter tool",
        "text": "The docformatter tool automatically formats Python docstrings according\nto PEP 257. To make sure docformatter wraps your docstrings at a given\nnumber of characters, use this configuration:\nThe docformatter tool\ndocformatter\ndocformatter"
    },
    {
        "objectID": "doc-style/formatting-tools",
        "href": "doc-style/formatting-tools.html#the-doctest-tool",
        "title": "Documentation style tools > The doctest tool",
        "section": "The doctest tool",
        "text": "The doctest tool is a module from the Python standard library, which means it is\nincluded by default with your Python installation. It is used for checking the\nexamples provided inside docstrings to make sure that they reflect the current usage\nof the source code. You can integrate doctest with pytest in The\n``pyproject.toml`` file:\nThe doctest tool\ndoctest\npytest"
    },
    {
        "objectID": "doc-style/formatting-tools",
        "href": "doc-style/formatting-tools.html#the-interrogate-tool",
        "title": "Documentation style tools > The interrogate tool",
        "section": "The interrogate tool",
        "text": "The interrogate tool checks docstring coverage. Similar to source code\ncoverage tools, this tool tests how many modules, functions, classes, and\nmethods in a Python library hold a docstring.\nAlternate tools to interrogate are docstr-coverage and\ndocstring-coverage. However, interrogate is modern and maintained, with\noutput resembling that of pytest-cov, which is the equivalent tool\nfor source code coverage.\nThe interrogate tool\ninterrogate"
    },
    {
        "objectID": "doc-style/formatting-tools",
        "href": "doc-style/formatting-tools.html#numpydoc-validation",
        "title": "Documentation style tools > Numpydoc validation",
        "section": "Numpydoc validation",
        "text": "To validate the style of Numpydoc docstrings, you can\ntake advantage of the Sphinx numpydoc extension. Note that this extension\nchecks only for those objects whose docstrings must be rendered. It is not a\ncommand line tool that checks the style of all docstrings in your source code.\nBecause numpydoc is a Sphinx extension, it must be configured in the\nconf.py file. For more information, see The ``doc`` directory. Start by adding it to the\nlist of extensions:\nOnce the numpydoc extension is added, you can select which built-in validation checks\nmust be addressed by using the numpydoc_validation_checks dictionary:\nThis issues the following warning for any object without a docstring:\nNumpydoc validation\nnumpydoc\nconf.py\nnumpydoc\nnumpydoc_validation_checks"
    },
    {
        "objectID": "doc-style/formatting-tools",
        "href": "doc-style/formatting-tools.html#the-pydocstyle-tool",
        "title": "Documentation style tools > The pydocstyle tool",
        "section": "The pydocstyle tool",
        "text": "The pydocstyle tool checks the compliance of Python docstrings with PEP 257.\nIts configuration can be defined in the The ``pyproject.toml`` file.\nBy default, pydocstyle matches all *.py files except those starting with\ntest_*.py. The default configuration should be enough for a PyAnsys project.\nHowever, if additional configuration is needed, it must be included under the\n[tool.pydocstyle] entry:\nThe pydocstyle tool\npydocstyle\n*.py\ntest_*.py\n[tool.pydocstyle]"
    },
    {
        "objectID": "doc-style/formatting-tools",
        "href": "doc-style/formatting-tools.html#vale",
        "title": "Documentation style tools > Vale",
        "section": "Vale",
        "text": "Vale is a tool for maintaining a consistent style and voice in your documentation.\nIts configuration is defined in a .vale.ini file in the library’s doc folder.\nFor PyAnsys libraries, Vale is configured to apply the guidelines in the\nGoogle developer documentation style guide,\nalong with any custom Ansys rules and terminology lists, to reStructuredText (RST)\nand Markdown (MD) files.\nWhen Vale is implemented in your PyAnsys library, you can check\nany content changes that you make in supported files locally.\nIn the library’s doc folder, download the package with this command:\nCheck all files in the doc folder by running this command:\nTo check all files in the repository, go to the root directory and run\nthis command:\nTo check all files in only a particular folder, type vale followed by the\nname of the folder.\nAddress any warnings and issues that display by either editing the\nfile to fix or adding a term to the accept.txt file in\ndoc\\styles\\config\\vocabularies\\ANSYS.\nVale\n.vale.ini\ndoc\nVale\nVale\ndoc\ndoc\nroot\nvale\naccept.txt\ndoc\\styles\\config\\vocabularies\\ANSYS"
    },
    {
        "objectID": "content-writing/content-contrib-setup/index",
        "href": "content-writing/content-contrib-setup/index.html#content-contribution-setup",
        "title": "Content contribution setup",
        "section": "Content contribution setup",
        "text": "Python’s vast ecosystem of mathematical and scientific computing tools\nis widely used for data analysis. PyAnsys is a collection of Python-based,\nopen source projects tailored specifically for engineers who want to automate\ntheir simulation workflows and extend the capabilities of Ansys products.\nThis section describes Python client libraries and explains how to\nset up your development environment for contributing to the documentation\nfor PyAnsys libraries.\nA Python client library is a collection of source files.\nA package is a superset of the Python client library. It includes all source\nfiles and the supporting files needed to install and use the library.\nAdditionally, this section provides essential information for\ndocumenting PyAnsys libraries and includes links to all the many\nresources relevant to creating and maintaining this documentation. The\nresources_writers page and the content_how_tos section are\nlikely to be the content that you refer to most as you become proficient\nin contributing to PyAnsys documentation.\nContent contribution setup"
    },
    {
        "objectID": "content-writing/content-how-tos/work-around-Vale",
        "href": "content-writing/content-how-tos/work-around-Vale.html#work-around-vale-issues",
        "title": "Work around Vale issues",
        "section": "Work around Vale issues",
        "text": "Some issues that Vale raises may not be considered problematic in PyAnsys\nprojects. This page describes some common workarounds.\nWork around Vale issues"
    },
    {
        "objectID": "content-writing/content-how-tos/work-around-Vale",
        "href": "content-writing/content-how-tos/work-around-Vale.html#turn-vale-off-and-on",
        "title": "Work around Vale issues > Turn Vale off and on",
        "section": "Turn Vale off and on",
        "text": "If Vale flags content as an issue but you know that this content is\nacceptable, you can use the following directives to turn Vale off\nbefore the flagged content and then back on again after it:\nAs shown in the preceding example, you must insert blank lines to separate the\ndirectives for tuning Vale off and on from your documentation content.\nIn an Markdown (MD) file, the syntax for these two Vale directives looks like this:\nTurn Vale off and on"
    },
    {
        "objectID": "content-writing/content-how-tos/work-around-Vale",
        "href": "content-writing/content-how-tos/work-around-Vale.html#insert-non-breaking-spaces",
        "title": "Work around Vale issues > Insert non-breaking spaces",
        "section": "Insert non-breaking spaces",
        "text": "Vale displays a warning when it thinks a non-breaking space should be used\nin an RST or MD file. Here is how you insert a non-breaking space:\nIn an RST file, use the Unicode character U+00A0 for a non-breaking space.\nYou can insert it by typing &nbsp; or &#160.\nIn an MD file, use the HTML entity &nbsp; or the Unicode character U+00A0\nfor a non-breaking space. You can insert it by typing &#160.\nFor example, you can insert a non-breaking space between two words in either an RST or\nMD file by typing word1&nbsp;word2.\nInsert non-breaking spaces\nU+00A0\n&nbsp;\n&#160\n&nbsp;\nU+00A0\n&#160\nword1&nbsp;word2"
    },
    {
        "objectID": "content-writing/content-how-tos/work-around-Vale",
        "href": "content-writing/content-how-tos/work-around-Vale.html#allow-the-use-of-a-word-not-allowed-by-the-google-word-list",
        "title": "Work around Vale issues > Allow the use of a word not allowed by the Google word list",
        "section": "Allow the use of a word not allowed by the Google word list",
        "text": "Based on an implemented Google word list, Vale raises warnings when certain\nwords, such as “check” and “functionality,” are used. While you can ignore\nVale warnings, if the list of warnings becomes long, it can become annoying\nto have to scroll through them to find the errors that you must address.\nTo have Vale accept your use of a word that is raising a warning, you can\nadd the word to the accept.txt file in the doc/styles/config/vocabularies/ANSYS\ndirectory.\nAllow the use of a word not allowed by the Google word list\naccept.txt\ndoc/styles/config/vocabularies/ANSYS"
    },
    {
        "objectID": "content-writing/content-how-tos/work-around-Vale",
        "href": "content-writing/content-how-tos/work-around-Vale.html#turn-off-a-particular-style-rule",
        "title": "Work around Vale issues > Turn off a particular style rule",
        "section": "Turn off a particular style rule",
        "text": "Vale implements many Google style rules, which you can see in the YML files in\nthe doc/styles/Google directory. If necessary, you can turn off a particular\nrule.\nFor example, the rule in the Colons.yml file specifies that a lowercase letter\nmust follow a colon. However, in some cases, you might want to allow uppercase letters to\nfollow colons.\nWhile you could turn Vale off and on each time an error is raised for this colon\nstyle violation, this approach is cumbersome if there are many such errors.\nThus, you can turn off the Google style rule that is causing Vale to raise the\nerror:\nGo the doc directory and open the vale.ini file.\nLocate this content:\nAdd a line to turn off the Google rule about requiring a lowercase\nletter after a colon:\nTurn off a particular style rule\ndoc/styles/Google\nColons.yml\ndoc\nvale.ini"
    },
    {
        "objectID": "content-writing/content-how-tos/work-around-Vale",
        "href": "content-writing/content-how-tos/work-around-Vale.html#tell-vale-to-ignore-inline-roles",
        "title": "Work around Vale issues > Tell Vale to ignore inline roles",
        "section": "Tell Vale to ignore inline roles",
        "text": "If Vale raises errors about content within inline roles, such as ref, file, and class,\nimplement a workaround by telling Vale to ignore this content. Simply follow the previous\nsteps, adding the TokenIgnores argument to the end of the vale.ini file:\nTell Vale to ignore inline roles\nref\nfile\nclass\nTokenIgnores\nvale.ini"
    },
    {
        "objectID": "content-writing/content-how-tos/work-around-Vale",
        "href": "content-writing/content-how-tos/work-around-Vale.html#clear-files-generated-by-local-building-of-the-documentation",
        "title": "Work around Vale issues > Clear files generated by local building of the documentation",
        "section": "Clear files generated by local building of the documentation",
        "text": "When you build documentation locally, the build process generates _autosummary files in one\nor more _autosummary directories. When you next run Vale locally, it\nfinds these files and checks them, which likely results in it identifying lots of issues.\nTo clear _autosummary files, clean the doc folder:\nIf the Ansys Python Manager and the Administrator window are not\nopen, open them. For more information, see Ansys_Python_Manager.\nIn the Administrator window, use the cd command to go to the\nroot folder.\nRun this Git command:\nUse the cd command to go to the doc directory.\nRun this Vale command to install the latest rules for the Google style guidelines locally:\nRun this Vale command to check all RST and MD files in the doc directory:\nIf _autosummary directories or files are still present, you have accidentally pushed them\nto the repository. You want to delete these directories or files from your local branch and\npush this change to the repository so that they are also removed from it.\nClear files generated by local building of the documentation\n_autosummary\n_autosummary\n_autosummary\ndoc\ncd\ncd\ndoc\ndoc\n_autosummary"
    },
    {
        "objectID": "content-writing/content-how-tos/work-around-Vale",
        "href": "content-writing/content-how-tos/work-around-Vale.html#specify-a-specific-vale-version",
        "title": "Work around Vale issues > Specify a specific Vale version",
        "section": "Specify a specific Vale version",
        "text": "If you push your changes to a PR and Vale raises lots of inappropriate errors about titles not\nbeing in sentence case when they are already in title case, in the .github/workflows directory,\nopen the ci_cd.yml file and then update the “Documentation Style Check” to use Vale\nversion 3.4.1:\nSpecify a specific Vale version\n.github/workflows\nci_cd.yml"
    },
    {
        "objectID": "content-writing/rst-files-writers/rst-format-rules",
        "href": "content-writing/rst-files-writers/rst-format-rules.html#rst-formatting-rules",
        "title": "RST formatting rules",
        "section": "RST formatting rules",
        "text": "This page provides a summary of RST formatting rules to help you ensure that your\nPyAnsys documentation renders correctly. For a summary of the most important\nwriting guidelines, frequently review the Highlights\npage in the Goggle developer documentation style guide.\nUse sentence case for headings and titles in PyAnsys documentation, as specified\nin Headings and titles in the Google developer\ndocumentation style guide.\nUnderline (and optionally overline) titles to indicate the heading hierarchy.\nThe string of characters indicating the title level should be the same length\nas the title and must be at least as long as the title. You should have only\none top-level heading per RST file. For consistency within PyAnsys libraries,\nthe use of these characters is recommended but not enforced:\nFor section-level headings, use ###.\nFor subsection-level headings, use ===.\nFor subsubsection-level headings, use ---.\nFor subsubsubsection-level headings, use ~~~.\nFor paragraph-level headings, use +++.\nUse blank lines to separate text into paragraphs.\nEnsure that there are no blank spaces at the end of a line. (Running pre-commit\nfinds and removes any trailing white spaces, saving you from seeing errors\nwhen you create or push changes to a PR. For more information, see run_precommit.)\nAs in Python, indentation is significant. All lines of the same paragraph must be left-aligned to the\nsame level of indentation. Use spaces (and not tabs) for indentation.\nDo not exceed the maximum number of characters per line specified for your PyAnsys\nlibrary. While the Style guide for Sphinx-based documentation\nindicates that lines should be limited to a maximum of 79 characters, many PyAnsys\nlibraries extend this limit, sometimes to as many as 120 characters, because\ntoday’s larger screens can support longer lines.\nUse standard American spelling and punctuation.\nUse you and your rather then we and our.\nOmit the word “please” and replace the phrase “In order to” with “To.”\nItalicize a word or phrase by surrounding it in a single asterisk (*) or a\nsingle backtick (`). In PyAnsys documentation, the following are italicized: words\nor phrases to be emphasized, first occurrence of a new term, and a publication title\nthat is not a link.\nBold a word or phrase by surrounding it in double asterisks (**). In PyAnsys\ndocumentation, the following are in bold: words or phrases to be strongly\nemphasized and GUI (graphical user interface) components.\nSeparate lists from paragraphs with blank links. Begin each item in a bulleted list\nwith either a - or * followed by a space. Begin each item in a numbered\nlist with #. followed by a space. While nested lists are supported, you must\nseparate them from parent list items with blank lines and indent them appropriately.\nFor a horizontal list, use the hlist directive. This directive\nspecifies that list items are to display horizontally in three columns:\nThis is how the preceding list items, which are in a sublist, are rendered in the\ndocumentation:\nList item 1\nList item 2\nList item 3\nList item 4\nList item 5\nIndicate a code entity within text by surrounding it in double backticks\n(``). While the numpydoc Style guide\nsays to surround a code entity in a single backtick, this renders it incorrectly\nas italics in PyAnsys documentation. Surrounding it in double backticks\ncorrectly renders it in a monospaced font within a gray block.\nAlways follow a code entity with a noun that indicates the object type. For general\nguidelines, see Code in text in the Google developer\ndocumentation style guide.\nCode entities include the names of Python objects, such as packages, modules, functions,\nclasses, methods, and attributes. Code entities also include the following objects:\nFile paths\nNames of directories, files, and environment variables\nText to type in a command line or in a GUI component\nSome PyAnsys projects use the file and envvar interpretive text\nroles for names of files and environment variables:\nmyfile.txt\nMY_ENVAR\nRoles insert semantic markup in your source files for cross-references to named\ntargets of the type indicated by the role. Because the CSS for the\nAnsys Sphinx Theme assigns the same semantic markup to\nthe file role as it does to a filename surrounded in double backticks, it\ndoes not matter which markup you use.\nUse the code role to format text as a code entity if surrounding the text in double\nbackticks is problematic because it contains characters that cause regular\nexpression errors. For example, in this sentence describing the use of double backticks,\nthe code role had to be used to format the double backticks as a code entity:\nIf you want, you can use the code role within any sentence to identify small\npieces of inline code, individual identifiers (like function names or variable names),\nor inline code phrases. Most of the time though, using double backticks is easier.\nTo create a standalone code block within your documentation, use either the\ncode or code-block directive. For more information on code blocks,\nsee code_blocks.\nTo comment out lines in an RST file so that they do render in the documentation,\nplace two periods (..) and a space before each line that you want to hide:\nWhile this approach is useful if the native sphinx.ext.todo extension has not been\nadded to the extensions variable in your documentation’s Sphinx configuration\n(doc/source/conf.py) file, adding this extension is recommended. The specially\nformatted block of text for the .. todo:: directive does not render in the\ndocumentation by default. Plus, you can easily search for occurrences of this directive\nlater. For more information, see add_native_sphinx_ext.\nSubsequent pages describe how to use other common Sphinx roles and directives. For\ncomprehensive lists of roles and directives, see Roles and\nDirectives in the Sphinx documentation.\nRST formatting rules\n###\n===\n---\n~~~\n+++\npre-commit\nyou\nyour\nwe\nour\n*\n`\n**\n-\n*\n#.\nhlist\n``\nfile\nenvvar\nmyfile.txt\nMY_ENVAR\nfile\ncode\ncode\ncode\ncode\ncode-block\n..\nsphinx.ext.todo\nextensions\ndoc/source/conf.py\n.. todo::"
    },
    {
        "objectID": "abstractions/index",
        "href": "abstractions/index.html#abstractions",
        "title": "Abstractions",
        "section": "Abstractions",
        "text": "Abstraction in Python is the process of hiding the real implementation\nof an app from the user and emphasizing only usage.\nOne of the main objectives of PyAnsys libraries is to wrap (encapsulate)\ndata and methods within units of execution while hiding data or parameters\nin protected variables.\nThe topics in this section demonstrate how apps and complex services\nexpose functionalities that matter to users and hide all else, such as conditional\nstatements and algorithms. For example, background details, implementation,\nand states can all be hidden.\nAbstractions"
    },
    {
        "objectID": "how-to/logging",
        "href": "how-to/logging.html#logging",
        "title": "Logging",
        "section": "Logging",
        "text": "The following logging guidelines are best practices discovered through implementing\nlogging services and modules within PyAnsys libraries. Suggestions and improvements\nare welcomed.\nFor logging techniques, see Logging HOWTO in the Python\ndocumentation. These tutorials are particularly helpful:\nBasic Logging Tutorial\nAdvanced Logging Tutorial\nLogging"
    },
    {
        "objectID": "how-to/logging",
        "href": "how-to/logging.html#logging-overview",
        "title": "Logging > Logging overview",
        "section": "Logging overview",
        "text": "Logging helps to track events occurring in the app. A log record is\ncreated for each event. This record contains detailed information about the\ncurrent app operation. Whenever information must be exposed, displayed,\nand shared, logging is the way to do it.\nLogging is beneficial to both users and app developers. It serves several\npurposes, including these:\nExtracts some valuable data for users to know the status of their work\nTracks the progress and course of app usage\nProvides the developer with as much information as possible if an issue happens\nThe message logged can contain generic information or embed data specific to the\ncurrent session. Message content is associated with a severity level, such as INFO,\nWARNING, ERROR, and DEBUG. Generally, the severity level indicates the\nrecipient of the message. For example, an INFO message is directed to the user,\nwhile a DEBUG message is directed to the developer.\nLogging overview\nINFO\nWARNING\nERROR\nDEBUG\nINFO\nDEBUG"
    },
    {
        "objectID": "how-to/logging",
        "href": "how-to/logging.html#logging-best-practices",
        "title": "Logging > Logging best practices",
        "section": "Logging best practices",
        "text": "The logging capabilities in PyAnsys libraries should be built upon Python’s standard\nlogging library. A PyAnsys library should not replace the standard logging library\nbut rather provide a way for both it and the PyAnsys library to interact. Subsequent\nsections provide best logging practices.\nLogging best practices\nlogging"
    },
    {
        "objectID": "how-to/logging",
        "href": "how-to/logging.html#avoid-printing-to-the-console",
        "title": "Logging > Avoid printing to the console",
        "section": "Avoid printing to the console",
        "text": "A common habit while prototyping a new feature is to print a message into the\ncommand line executable. Instead of using the common print method, you should use a\nstream handler\nand redirect its content. This allows messages to be filtered based on\ntheir severity level and apply formatting properly. To accomplish this, add a\nBoolean argument in the initializer of the logging.Logger class that\nspecifies how to handle the stream.\nAvoid printing to the console\nlogging.Logger"
    },
    {
        "objectID": "how-to/logging",
        "href": "how-to/logging.html#turn-on-and-off-handlers",
        "title": "Logging > Turn on and off handlers",
        "section": "Turn on and off handlers",
        "text": "You might sometimes want to turn off a specific handler, such as a file\nhandler where log messages are written. If so, you must properly close\nand remove the existing handler. Otherwise, you might be denied file access\nlater when you try to write new log content.\nThis code snippet shows how to turn off a log handler:\nTurn on and off handlers"
    },
    {
        "objectID": "how-to/logging",
        "href": "how-to/logging.html#use-app-filters",
        "title": "Logging > Use app filters",
        "section": "Use app filters",
        "text": "An app filter shows all its value when the content of a message depends on some\nconditions. It injects contextual information in the core of the message.\nThis can be used to harmonize message rendering when the app output varies\nbased on the data processed.\nUsing an app filter requires the creation of a class based on the logging.Filter\nclass from the logging module and the implementation of its filter()\nfunction. This function contains all modified content to send to the stream:\nUse app filters\nlogging.Filter\nlogging\nfilter()"
    },
    {
        "objectID": "how-to/logging",
        "href": "how-to/logging.html#use--formatting-for-strings",
        "title": "Logging > Use %-formatting for strings",
        "section": "Use %-formatting for strings",
        "text": "Although using the f-string for formatting most strings is often recommended,\nwhen it comes to logging, using the former %-formatting is preferable.\nWhen %-formatting is used, the string is not evaluated at runtime. Instead, it\nis evaluated only when the message is emitted. If any formatting or evaluation\nerrors occur, they are reported as logging errors and do not halt code.\nUse %-formatting for strings"
    },
    {
        "objectID": "how-to/logging",
        "href": "how-to/logging.html#app-and-service-logging-modules",
        "title": "Logging > App and service logging modules",
        "section": "App and service logging modules",
        "text": "PyAnsys libraries use app and service logging modules to extend\nor expose features from an Ansys app, product, or service, which can\nbe local or remote.\nFor a PyAnsys library, there are two main loggers that expose or\nextend a service-based app:\nglobal_logger\ninstance_logger\nThese loggers are customized classes that wrap the logging.Logger\nclass from the logging module and add specific features to it. This\nimage shows the logging approach used by PyMAPDL and the scopes\nof the global and instance loggers.\nYou can see the source for a custom PyAnsys logger in the first of the following\ncollapsible sections and in the pyansys_logging.py\nfile in the pyansys-dev-guide repository. The second collapsible section shows some unit tests\nthat show how to use this custom PyAnsys logger:\nApp and service logging modules\nlogging.Logger\nlogging\npyansys-dev-guide"
    },
    {
        "objectID": "how-to/logging",
        "href": "how-to/logging.html#global-logger",
        "title": "Logging > Global logger",
        "section": "Global logger",
        "text": "A global logger named py*_global is created when importing\nansys.product.service (ansys.product.service.__init__). This logger\ndoes not track instances but rather is used globally. Consequently, its use\nis recommended for most scenarios, especially those where simple modules\nor classes are involved.\nFor example, if you intend to log the initialization of a library or module,\nimport the global logger at the top of your script or module:\nIf the default name of the global logger is in conflict with the name of\nanother logger, rename it:\nThe default logging level of the global logger is ERROR (logging.ERROR).\nYou can change the output to a different error level like this:\nAlternatively, to ensure that all handlers are set to the desired log level,\nuse this approach:\nBy default, the global logger does not log to a file. However, you can\nenable logging to both a file and the standard output by adding\na file handler:\nIf you want to change the characteristics of the global logger from the beginning of\nthe execution, you must edit the __init__ file in the directory of your\nlibrary.\nTo log using the global logger, simply call the desired method as a normal logger:\nGlobal logger\npy*_global\nansys.product.service\nansys.product.service.__init__\nERROR\nlogging.ERROR\n__init__"
    },
    {
        "objectID": "how-to/logging",
        "href": "how-to/logging.html#instance-logger",
        "title": "Logging > Instance logger",
        "section": "Instance logger",
        "text": "An instance logger is created every time that the _MapdlCore class is\ninstantiated. Using this instance logger is recommended when using the pool\nlibrary or when using multiple instances of MAPDL. The main feature of the instance\nlogger is that it tracks each instance and includes the instance name when logging.\nThe names of instances are unique. For example, when using the MAPDL gRPC\nversion, the instance name includes the IP and port of the corresponding instance,\nmaking the logger unique.\nYou can access instance loggers in two places:\n_MapdlCore._log for backward compatibility\nLOG._instances, which is a field of the dict data type with a key that\nis the name of the created logger\nThese instance loggers inherit from the pymapdl_global output handlers and\nlogging level unless otherwise specified. An instance logger works similarly to\nthe global logger. If you want to add a file handler, use the log_to_file\nmethod. If you want to change the log level, use the logging.Logger.setLevel()\nmethod.\nThis code snippet shows how to use an instance logger:\nInstance logger\n_MapdlCore\npool\n_MapdlCore._log\nLOG._instances\ndict\npymapdl_global\nlog_to_file\nlogging.Logger.setLevel()"
    },
    {
        "objectID": "how-to/logging",
        "href": "how-to/logging.html#ansys-product-loggers",
        "title": "Logging > Ansys product loggers",
        "section": "Ansys product loggers",
        "text": "An Ansys product, due to its architecture, can have several loggers. The\nlogging library supports working with a finite number of loggers. The\nlogging.getLogger() factory function helps to access each logger by its name. In\naddition to name mappings, a hierarchy can be established to structure the\nloggers’ parenting and their connections.\nFor example, if an Ansys product is using a pre-existing custom logger\nencapsulated inside the product itself, the PyAnsys library benefits from\nexposing it through the standard Python tools. You should use the\nstandard library as much as possible. It facilitates every contribution\nto the PyAnsys library, both external and internal, by exposing common tools that\nare widely adopted. Each developer is able to operate quickly and\nautonomously. The project takes advantage of the entire set of features exposed\nin the standard logger and all the upcoming improvements.\nAnsys product loggers\nlogging\nlogging.getLogger()"
    },
    {
        "objectID": "how-to/logging",
        "href": "how-to/logging.html#custom-log-handlers",
        "title": "Logging > Custom log handlers",
        "section": "Custom log handlers",
        "text": "You might need to catch Ansys product messages and redirect them to another\nlogger. For example, Ansys Electronics Desktop (AEDT) has its own internal\nlogger called the message manager, which has three main destinations:\nGlobal which is for the entire project manager\nProject, which is related to the project\nDesign, which is related to the design, making it the most specific destination of the three loggers\nThe message manager does not use the standard Python logging module, which\ncan be a problem when exporting messages and data from it to a common tool.\nIn most cases, it is easier to work with the standard Python module to extract\ndata. To overcome this AEDT limitation, you must wrap the existing message\nmanager into a logger based on the standard logging library:\nThe wrapper implementation is essentially a custom handler based on a\nclass inherited from the logging.Handler class. The initializer\nof this class requires the message manager to be passed as an argument to link the standard\nlogging service with the AEDT message manager:\nThe purpose of the logging.Handler class is to send log messages in the\nAEDT logging stream. One of the mandatory actions is to overwrite the emit()\nfunction. This function operates as a proxy, dispatching all log messages to the\nmessage manager. Based on the record level, the message is sent to the appropriate\nlog level, such as INFO, ERROR, or DEBUG, into the message manager to\nfit the level provided by the Ansys product. As a reminder, the record is an object\ncontaining all kind of information related to the event logged.\nThis custom handler is use in the new logger instance (the one based on the\nlogging library). To avoid any conflict or message duplication, before adding\na handler on any logger, verify if an appropriate handler is already available.\nCustom log handlers\nlogging\nlogging.Handler\nlogging.Handler\nemit()\nINFO\nERROR\nDEBUG\nlogging"
    },
    {
        "objectID": "content-writing/rst-files-writers/notices",
        "href": "content-writing/rst-files-writers/notices.html#notices",
        "title": "Notices",
        "section": "Notices",
        "text": "In the Google developer documentation style guide, the\nNotes, cautions, warnings, and other notices\npage explains how notices offset important information. In addition to defining\ntypes, it explains when notices should be used. Avoid overuse because notices\nlose their visual distinctiveness if a page has multiple notices or two (or more)\nconsecutive notices.\nPyAnsys documentation uses these admonition directives for common notices:\nnote\ncaution\nwarning\nimportant\ntip\nNotices\nnote\ncaution\nwarning\nimportant\ntip"
    },
    {
        "objectID": "content-writing/rst-files-writers/notices",
        "href": "content-writing/rst-files-writers/notices.html#note-example",
        "title": "Notices > Note example",
        "section": "Note example",
        "text": "Here is a formatting example for a note directive:\nHere is how this note is rendered in the documentation:\nSome examples require additional Python packages. Ensure\nthat you have these packages installed.\nNote example\nnote"
    },
    {
        "objectID": "content-writing/rst-files-writers/notices",
        "href": "content-writing/rst-files-writers/notices.html#caution-example",
        "title": "Notices > Caution example",
        "section": "Caution example",
        "text": "Here is a formatting example for a caution directive:\nHere is how this caution is rendered in the documentation:\nTo ensure proper operation, modify this XML file carefully.\nAll paths specified in this file must adhere to the path\nconventions of the respective operating system.\nCaution example\ncaution"
    },
    {
        "objectID": "content-writing/rst-files-writers/notices",
        "href": "content-writing/rst-files-writers/notices.html#warning-example",
        "title": "Notices > Warning example",
        "section": "Warning example",
        "text": "Here is a formatting example for a warning directive:\nHere is how this warning is rendered in the documentation:\nThis method requires NumPy <numpy_>`_ to be installed on your machine.\nWarning example\nwarning"
    },
    {
        "objectID": "content-writing/rst-files-writers/notices",
        "href": "content-writing/rst-files-writers/notices.html#important-example",
        "title": "Notices > Important example",
        "section": "Important example",
        "text": "Here is a formatting example for an important directive:\nHere is how this important notice is rendered in the documentation:\nNet tracing is a critical requirement for using the auto_compoments\ndefinition type when defining components on the die.\nImportant example\nimportant\nauto_compoments"
    },
    {
        "objectID": "content-writing/rst-files-writers/notices",
        "href": "content-writing/rst-files-writers/notices.html#tip-example",
        "title": "Notices > Tip example",
        "section": "Tip example",
        "text": "Here is a formatting example for a tip directive:\nHere is how this tip is rendered in the documentation:\nWhen you are viewing PyAnsys documentation, the right navigation pane typically\ndisplays Edit on GitHub and Show Source links. For information on\nusing the Edit on GitHub link to use the GitHub web editor to submit\nsuggested changes to a page in a PR, see edit_on_GitHub. For information\non using the Show Source link to see how a page’s source file is formatted and\nhow you can reuse this content, see rst_file_formatting.\nTip example\ntip"
    },
    {
        "objectID": "how-to/packaging",
        "href": "how-to/packaging.html#packaging",
        "title": "Packaging",
        "section": "Packaging",
        "text": "Packaging is the process for distributing software to guarantee that final users\ncan use it. By packaging Python libraries, it is possible to declare which\nsource code or binary files must be distributed, project metadata, and\nthird-party dependencies.\nPackaging style collects the fundamentals of Python packaging and packaging style\nguidelines that apply to PyAnsys projects.\nPackaging"
    },
    {
        "objectID": "how-to/packaging",
        "href": "how-to/packaging.html#dependencies",
        "title": "Packaging > Dependencies",
        "section": "Dependencies",
        "text": "It is common to take advantage of third-party libraries to simplify\nsource code. The formal way of doing so is by specifying these third-party\nlibraries as dependencies. There are two types of dependencies: Required\ndependencies and Optional dependencies.\nDependencies"
    },
    {
        "objectID": "how-to/packaging",
        "href": "how-to/packaging.html#required-dependencies",
        "title": "Packaging > Required dependencies",
        "section": "Required dependencies",
        "text": "Required dependencies are third-party libraries that a software requires to\nproperly function. If these dependencies are not installed or present, the\nsoftware does not work as expected.\nRequired dependencies must be declared in The ``setup.py`` file or\nin The ``pyproject.toml`` file, according to the\nselected Build system:\nRequired dependencies"
    },
    {
        "objectID": "how-to/packaging",
        "href": "how-to/packaging.html#optional-dependencies",
        "title": "Packaging > Optional dependencies",
        "section": "Optional dependencies",
        "text": "Optional dependencies are third-party libraries without which a software is not\nable to execute particular features. This makes it convenient to declare\ndependencies for ancillary functions such as plotting, tests, or documentation. You\ncan programmatically integrate dependencies that are to be installed as optional\nrequirements rather than individual packages.\nYou may want to have optional packages for your PyAnsys library for a variety of\nreasons, including:\nNot all users want to use the feature. For example, you might want\nto make using Matplotlib or PyVista optional if you expect your PyAnsys library is\nto be used primarily for headless scripting rather than visualization.\nNot all users can install the optional package. For certain less popular\nor obscure environments, some binary wheels might not be available or compatible\nwith the user’s environment. For example, if a user of CentOS 6.9 needs to\nhave the manylinux1 package but CentOS 6.9 only supports manylinux2014 (CentOS\n7+ and later), the user’s environment wouldn’t be able to run the PyAnsys\nlibrary.\nReduce dependency bloat. Removing the package as a “required”\ndependency reduces the number of packages to install at installation time,\nspeeding up the installation and reducing the possibility of dependency\nconflicts. The trade-off here is that any user who wants to access features that\nrequire the optional package must install it separately.\nIf you choose to implement optional packages for your PyAnsys library, some helpful\nbest practices follow.\nOptional dependencies\nmanylinux1\nmanylinux2014"
    },
    {
        "objectID": "how-to/packaging",
        "href": "how-to/packaging.html#implement-optional-packages-in-the-build-system",
        "title": "Packaging > Implement optional packages in the build system",
        "section": "Implement optional packages in the build system",
        "text": "The following code snippets show how to implement and use optional requirements for\nthe three most popular build systems:\nInstall package-name with the optional qt packages with this command\nInstall package-name with the optional qt packages with this command:\nInstall package-name with the optional qt packages with this command:\nImplement optional packages in the build system\npackage-name\nqt\npackage-name\nqt\npackage-name\nqt"
    },
    {
        "objectID": "how-to/packaging",
        "href": "how-to/packaging.html#implement-optional-libraries-in-features",
        "title": "Packaging > Implement optional libraries in features",
        "section": "Implement optional libraries in features",
        "text": "One of the best ways to implement an optional dependency is to execute a lazy\nimport at runtime for the feature in question. For example, if your library\nhas an optional dependency on Matplotlib, you can implement it like this:\nNote that the import statement is within the method and not at the module\nlevel. Normally this is a bad practice because it can cause runtime errors. However,\nfor optional features where the user isn’t expected to have the library\ninstalled, this is one of the best ways of handling it. Otherwise, the PyAnsys\nlibrary might fail to import because the optional package might not be installed.\nAlso note how this code snippet adds a helpful ModuleNotFoundError rather\nthan simply allowing the error to be raised. This lets the user know that this\nerror is expected because the feature relies on an optional dependency.\nIf you have many methods that rely on an optional feature, you can implement a\ndecorator to make it\neasier to add these lazy imports and helpful error messages. Here is an example:\nYou use the decorator with a method like this:\nIn practice, if the user does not have Matplotlib installed, this is the\nbehavior that the user would expect:\nImplement optional libraries in features\nimport"
    },
    {
        "objectID": "how-to/packaging",
        "href": "how-to/packaging.html#dependabot",
        "title": "Packaging > Dependabot",
        "section": "Dependabot",
        "text": "Dependabot is a built-in tool for keeping project dependencies updated. It informs\nyou of the latest releases of the packages being used.\nDependabot"
    },
    {
        "objectID": "how-to/packaging",
        "href": "how-to/packaging.html#the-dependabotyml-file",
        "title": "Packaging > The dependabot.yml file",
        "section": "The dependabot.yml file",
        "text": "Dependabot version updates are performed by checking a dependabot.yml\nconfiguration file into your repository. In this file, one should specify the\nlocation of the project’s requirement files, so that Dependabot knows where to\nlook. On top of that, Dependabot is also capable of updating GitHub actions\nversions.\nThe following code snippets show the required configuration for Dependabot\naccording to the type of file in which the dependencies are specified:\nThis file should be located in the .github folder of your repository for\nGitHub to detect it automatically. There are several main options:\npackage-ecosystem: Lets Dependabot know what your package manager is.\nPyAnsys projects typically use pip. However, conda could also be used.\ndirectory: Lets Dependabot know where your requirement files are located.\nPyAnsys projects typically contain all their requirements inside a requirements\ndirectory. Other directories could be used.\nschedule: Lets Dependabot know the frequency to perform subroutines\nfor checking for updates.\nThe dependabot.yml file\ndependabot.yml\ndependabot.yml\n.github\npip\nconda\nrequirements"
    },
    {
        "objectID": "how-to/packaging",
        "href": "how-to/packaging.html#dependabot-updates",
        "title": "Packaging > Dependabot updates",
        "section": "Dependabot updates",
        "text": "Dependabot determines (using semantic versioning) whether a requirement should\nbe updated due to the existence of a newer version. When Dependabot identifies\nan outdated dependency, it raises a pull request to update these requirement\nfiles.\nDependabot allows for two different types of updates:\nDependabot security updates: Automated pull requests that help update\ndependencies with known vulnerabilities.\nDependabot version updates: Automated pull requests that keep dependencies updated,\neven when they don’t have any vulnerabilities. To check the status of version updates,\nnavigate to the Insights tab of your repository and then select Dependency Graph\nand Dependabot.\nDependabot only works for pinned-down versions of requirements (or, at most, versions\nwith an upper-limits requirement such as pyvista <= 0.34.0). However, this is not\na best practice for run-time dependencies (that is, the usage of a package should support\nthe oldest available version if possible). Thus, it is only recommended to fully pin\ndocumentation and testing requirements (that is, using ==). Having the latest\ndependencies available in your requirements testing files lets you test the\nlatest packages against your library.\nDependabot updates\npyvista <= 0.34.0\n=="
    },
    {
        "objectID": "how-to/packaging",
        "href": "how-to/packaging.html#dependabot-version-updates",
        "title": "Packaging > Dependabot version updates",
        "section": "Dependabot version updates",
        "text": "To enable version updates for your repository, see\nEnabling Dependabot version updates\nin the GitHub documentation.\nDependabot version updates"
    },
    {
        "objectID": "how-to/packaging",
        "href": "how-to/packaging.html#dependabot-security-updates",
        "title": "Packaging > Dependabot security updates",
        "section": "Dependabot security updates",
        "text": "Dependabot security updates make it easier for you to fix vulnerable dependencies in your\nrepository. If you enable this feature, when a Dependabot alert is raised for a vulnerable\ndependency in the dependency graph of your repository, Dependabot automatically tries to fix it.\nFor information on enabling security updates and notifications for your repository, see\nEnabling or disabling Dependabot security updates for an individual repository\nin the GitHub documentation.\nDependabot security updates"
    },
    {
        "objectID": "content-writing/py-files-writers/index",
        "href": "content-writing/py-files-writers/index.html#content-in-py-files",
        "title": "Content in PY files",
        "section": "Content in PY files",
        "text": "In Python, a module is a PY file that contains Python objects such\nas interfaces, functions, classes, methods, parameters, attributes,\nenums (enumerations), properties, and constants. These objects each\nhave a docstring that describes and explains how to use it to interact\nwith the library. When building documentation, Sphinx assembles\nthe content in these docstrings to generate API reference documentation.\nThis section covers the setup and processing of PY files by Sphinx,\nfundamental Python objects, and formatting rules for docstrings, code comments,\nand message strings.\nFor resources related to PY files, see style_format_resources.\nTo learn how PY files for PyAnsys libraries are formatted, see py_file_format.\nContent in PY files"
    },
    {
        "objectID": "content-writing/py-files-writers/index",
        "href": "content-writing/py-files-writers/index.html#py-file-setup",
        "title": "Content in PY files > PY file setup",
        "section": "PY file setup",
        "text": "A Python client library consists of PY files that are organized and\npackaged in a way that makes it easy for users to interact with the library\nand its underlying APIs and services.\nIn a PyAnsys client library, PY files are organized in the src directory.\nThe folder and file names in the src directory cannot contain spaces or hyphens.\nReplace these characters with an underscore (_).\nEach subpackage contains an __init__.py file, which contains any\nnecessary package-level initialization code.\nPY file setup\nsrc\nsrc\n_\n__init__.py"
    },
    {
        "objectID": "content-writing/py-files-writers/index",
        "href": "content-writing/py-files-writers/index.html#numpy-docstrings",
        "title": "Content in PY files > NumPy docstrings",
        "section": "NumPy docstrings",
        "text": "Properly documented code is crucial to code readability, maintainability, and\nusability. For information on the two most popular conventions for formatting\ndocstrings, see the numpydoc style guide and\nComments and Docstrings in the Google Python Style Guide.\nWhile Sphinx supports both the numpydoc extension for NumPy-style docstrings\nand the napoleon extension for Goggle Python-style docstrings, PyAnsys libraries\nuse NumPy-style docstrings because this convention is favored by the most well known\nPython scientific packages.\nFor information on how to format docstrings so that they render correctly in PyAnsys\nAPI reference documentation, see py_file_format.\nNumPy docstrings\nnumpydoc\nnapoleon"
    },
    {
        "objectID": "content-writing/py-files-writers/index",
        "href": "content-writing/py-files-writers/index.html#api-documentation-extensions",
        "title": "Content in PY files > API documentation extensions",
        "section": "API documentation extensions",
        "text": "Older PyAnsys libraries use the native (built-in) sphinx.ext.autodoc extension to\ngenerate API reference documentation. This extension requires developers to manually author\nRST files and import code, which means that they can overlook Python objects when\nsetting up their API reference documentation. For more information, see\nsphinx.ext.autodoc in the Sphinx documentation.\nNewer PyAnsys projects use the external Sphinx AutoAPI\nextension to generate API reference documentation. This extension finds and generates content\nby parsing the source code, ensuring that the API reference documentation includes\nevery Python client object in every PY file. Generally, API members in projects using\nSphinx Auto API are ordered as follows:\nSubpackages\nSubmodules\nExceptions\nClasses\nFunctions\nMethods\nThe PyAnsys core team has contributed to the Sphinx AutoAPI extension to make the API\nreference documentation that it generates much more navigable and attractive. The goal\nis to eventually transition all PyAnsys libraries to use Sphinx AutoAPI so that all API\nreference documentation looks like this sample page from the PyAnsys Geometry documentation:\nFor more information on adding Sphinx extensions to a project, see add_sphinx_extensions.\nAPI documentation extensions\nsphinx.ext.autodoc"
    },
    {
        "objectID": "content-writing/content-contrib-setup/content-dev-environment",
        "href": "content-writing/content-contrib-setup/content-dev-environment.html#content-development-environment",
        "title": "Content development environment",
        "section": "Content development environment",
        "text": "Before you can contribute to PyAnsys documentation, you must set up your\ncontent development environment. Setup consists of creating a few accounts and\ninstalling some apps and tools.\nIf you intend only to review PRs created by others, you do not\nhave to set up a full content development environment. Instead, you need only perform the\nsteps in these topics:\ndoc_for_create_github_account\ninstall_git\njoin_ansys_github_orgs\nIf you intend to create your own PRs, perform the steps in all\ntopics to set up a full content development environment.\nContent development environment"
    },
    {
        "objectID": "content-writing/content-contrib-setup/content-dev-environment",
        "href": "content-writing/content-contrib-setup/content-dev-environment.html#create-a-github-account",
        "title": "Content development environment > Create a GitHub account",
        "section": "Create a GitHub account",
        "text": "GitHub is a web-based platform with more than 100 million users\nand 330 million repositories. A repository contains everything about a project\nand is the root of the project.\nWhile GitHub uses the Git version control system for tracking changes\nin source code, it goes way beyond version control, offering a\ncollaborative environment for hosting, managing, and sharing Git repositories.\nTo view or contribute to a GitHub project, you must have a GitHub user account.\nGo to the Join GitHub page and complete the user account form.\nOnce your account is created, set it up to require two-factor authentication.\nFor more information, see Configuring two-factor authentication in the GitHub documentation.\nIn a GitHub project, you can perform many tasks, including these:\nBrowse the codebase to explore the project’s structure and architecture.\nRead the project-hosted documentation to understand how the project works,\nhow to use it effectively, and how to contribute.\nReview a PR created by a fellow contributor to ensure that the code being\nadded or changed meets established coding standards, follows best practices,\nand is of high quality before the PR can be approved for merging into the\ncodebase.\nCreate a local branch, make commits with suggested additions or changes, and\nthen create your own PR for project maintainers to review, approve, and merge.\nIntegrate GitHub CI/CD.\nAutomate testing and deployment processes.\nManage discussions and build a community around your project.\nWhile some of the preceding GitHub tasks are usually performed only by project\nmaintainers, many of them are regularly performed by all team members, including\ncontent contributors.\nFor more information on GitHub, see Contributing and the GitHub documentation.\nCreate a GitHub account"
    },
    {
        "objectID": "content-writing/content-contrib-setup/content-dev-environment",
        "href": "content-writing/content-contrib-setup/content-dev-environment.html#install-git",
        "title": "Content development environment > Install Git",
        "section": "Install Git",
        "text": "You must have Git or a graphical user interface (GUI) client for Git installed.\nBecause developers are accustomed to working in terminal environments, they\ntend to prefer using the Git command line, especially because it provides for\ngreater control, customization, and automation. However, non-developer team members, such\nas project managers, designers, and writers, tend to prefer using a GUI client for Git.\nWith a GUI client like GitHub Desktop or Git Extensions, rather than having to\nremember complex command sequences, non-developer team members can use the visual clues\nthat the GUI provides to better understand branching, PRs, merging, and history visualization.\nIf you do not yet have Git or a GUI client for Git installed, install your\npreferred tool from an official channel.\nInstall Git"
    },
    {
        "objectID": "content-writing/content-contrib-setup/content-dev-environment",
        "href": "content-writing/content-contrib-setup/content-dev-environment.html#join-ansys-github-organizations",
        "title": "Content development environment > Join Ansys GitHub organizations",
        "section": "Join Ansys GitHub organizations",
        "text": "Ansys has two GitHub organizations:\nansys: Contains Ansys public repositories\nansys-internal: Contains Ansys internal repositories\nAnsys employees can join these Ansys organizations by clicking the following\nlinks to sign in first with their Ansys credentials and then with their\nGitHub credentials.\nJoin the Ansys GitHub organization.\nJoin the Ansys Internal GitHub organization.\nWriters outside of Ansys who want to contribute to the documentation for a PyAnsys\nlibrary can contact the PyAnsy core team\nfor permission to access this project’s repository.\nJoin Ansys GitHub organizations\nansys\nansys-internal"
    },
    {
        "objectID": "content-writing/content-contrib-setup/content-dev-environment",
        "href": "content-writing/content-contrib-setup/content-dev-environment.html#install-and-use-the-ansys-python-manager",
        "title": "Content development environment > Install and use the Ansys Python Manager",
        "section": "Install and use the Ansys Python Manager",
        "text": "Because installing Python on Windows is a complex process for people new to\nthe Python ecosystem, various groups within Ansys have worked together to\ncreate the Ansys Python Manager, an open source Python QT app. This app streamlines\ninstalling Python on Windows, simplifies creating and managing virtual environments,\ncentralizes installing and using other development tools, and facilitates both\ninstalling PyAnsys packages and viewing PyAnsys documentation.\nWhile Ansys Python Manager documentation is available,\nyou can perform these steps to install and immediately begin using this app:\nGo to the Releases page in the python-installer-qt-gui\nrepository.\nTo download the executable (EXE) file for the latest release, under Assets,\nclick Ansys-Python-Manager-Setup-v[latest version].exe.\nIn your Downloads directory, double-click this EXE file to open the Ansys\nPython Manager Setup window, where you can accept the defaults on each page.\nWhen the installation completes, the Ansys Python Manager opens. If you close\nthis app, you can open it again at any time from the Windows Start menu by\nselecting Ansys Python Manager.\nOn the Install Python tab, install a selected Python version:\nFor Installation type, choose Standard to install the standard\ninstallation from the Python organization.\nFor Python version, choose Python 3.11 to install the latest available\nversion.\nAt the bottom of the page, click Install.\nA Setup Progress window opens while Python is being installed and\nthen closes when the installation finishes.\nOn the Create virtual environments tab, create and activate a virtual environment:\nFor Select Python version, choose the Python version that you want to\ncreate a virtual environment for.\nYou likely have only the one Python version that you just installed.\nHowever, developers might have several Python versions installed.\nA virtual environment makes your life easier because you do not have\nto worry about dependency conflicts in the same environment. For more information,\nsee Creation of virtual environments in the\nPython documentation.\nFor Virtual environment name, type the name to give your virtual environment.\nFor example, type my_venv.\nAt the bottom of the page, click Create.\nAfter a few seconds, an Information window opens, indicating that your\nvirtual environment has been successfully created. As indicated on the tab, virtual\nenvironments are created in an .ansys_python_venvs folder in your user directory.\nClick OK to close the Information window.\nOn the Manage Python Environments tab, use some of its many features to set up and\nexplore your development environment:\nFor Available virtual environments, choose the virtual environment\nthat you have just created.\nWriters generally find having one virtual environment sufficient. However, developers\nmight have several virtual environments for managing different packages and\nlaunching options.\nUnder General package management, click Install Python default packages to install\nall the open source packages that are commonly used by PyAnsys libraries in your virtual\nenvironment.\nThe Administrator window opens, showing the installation of the default packages.\nOnce these packages are installed, this window closes.\nTo see a list of all packages installed in your virtual environment, under\nGeneral package management in the app, click List installed packages.\nThe Administrator window opens, showing the names and versions of all installed packages.\nIf you create an issue on the Issues page for a project’s GitHub repository, you are\nasked for your Python version, which you selected on the Install Python tab, and\nyour installed packages. You can copy the package information from here in the Administrator\nwindow and paste it into the issue template.\nIf a new release of an installed package is available, in addition to listing the installed\npackages, the Administrator window displays notices, which include the command for updating\nto the new release. Thus, you should periodically click List installed packages to see if\nyou need to update an installed package.\nAfter copying the update command for a package, you can paste it in the command prompt,\nwhich is the final line in the window that shows the name of your virtual environment in\nparentheses. You then press the enter key to run the command. The window displays\ninformation about uninstalling the current package and installing the updated package.\nWhen finished, close the Administrator window.\nUnder PyAnsys package management, choose the PyAnsys package to install,\nthe desired version, and click Install.\nChoosing PyAnsys-Metapackge and an Ansys version installs all PyAnsys\npackages that are compatible for use with this Ansys version. While you can choose\nto install a metapackage for a particular Ansys version, if you are contributing\nto PyAnsys documentation for only one package, or only for a few packages, choosing\nto install only these individual packages saves disk space.\nOnce the package is installed, this window closes.\nFrom the app’s File menu, periodically select Check for Updates to check for and install\nany updates available for the Ansys Python Manager.\nFrom the app’s Help menu, select Online Documentation to see this app’s documentation.\nIf this app is not open, you can go directly to the latest stable version of the\nAnsys Python Manager documentation on GitHub.\nFrom the app’s Help menu, select PyAnsys Documentation to open the PyAnsys Documentation\nwindow, where you can choose the documentation that you want to view. Then, click Open Website\nto go directly to this documentation. When finished, you can close this window.\nWhile you can close and reopen the Ansys Python Manager as needed, leaving this app open while you work\nis helpful because you can click Launch console under Launch options to open a console to run commands\nin your virtual environment.\nIf you click any other button under Launch options, the Ansys Python Manager installs this tool\nif it is not yet installed and then opens it. However, most of these tools are for developers and project\nmaintainers who use them to write and test Python scripts. Because editing the reStructuredText\n(RST) and Python (PY) files for PyAnsys documentation is more easily accomplished using Visual Studio\nCode and the Python in Visual Studio Code extension, you likely do not need to use these other tools.\nInstall and use the Ansys Python Manager\npython-installer-qt-gui\nmy_venv\n.ansys_python_venvs"
    },
    {
        "objectID": "content-writing/content-contrib-setup/content-dev-environment",
        "href": "content-writing/content-contrib-setup/content-dev-environment.html#install-visual-studio-code",
        "title": "Content development environment > Install Visual Studio Code",
        "section": "Install Visual Studio Code",
        "text": "Visual Studio Code is a lightweight but powerful source code editor that\nruns on your desktop. As indicated in the Visual Studio Code documentation,\nit is available for Windows, macOS, and Linux. For information on installing and\nrunning Visual Studio Code on your operating system, see SETUP in the\nVisual Studio Code documentation.\nWhile you can use another source code editor to modify RST and PY files, Visual Studio Code is\none of the most commonly used source code editors worldwide. This guide assumes that you are\nusing Visual Studio Code with the Python in Visual Studio Code extension.\nInstall Visual Studio Code"
    },
    {
        "objectID": "content-writing/content-contrib-setup/content-dev-environment",
        "href": "content-writing/content-contrib-setup/content-dev-environment.html#install-the-python-in-visual-studio-code-extension",
        "title": "Content development environment > Install the Python in Visual Studio Code extension",
        "section": "Install the Python in Visual Studio Code extension",
        "text": "The Python in Visual Studio Code extension makes Visual Studio Code an excellent Python editor.\nThis extension, which works on any operating system with a variety of Python interpreters,\nleverages Visual Studio Code’s power to provide autocompletion and IntelliSense, linting,\ndebugging, and unit testing.\nInstall the Python in Visual Studio Code extension:\nFrom the View menu in Visual Studio Code, select Extensions.\nAt the top of the EXTENSIONS pane, type python in the search box to filter the\nlist of available extensions.\nSelect the Python extension published by Microsoft, which is described as IntelliSense\n(Pylance) and is usually the first one in the list.\nYou can view information about this extension in the main pane on the right.\nIn either the EXTENSIONS pane or the main pane, click Install.\nThe Install button changes to a settings (gear) icon in the EXTENSIONS pane or to\ntwo buttons, Disable and Uninstall, in the main pane. This lets you know that the\nPython extension for Windows has been installed successfully.\nInstall the Python in Visual Studio Code extension\npython"
    },
    {
        "objectID": "content-writing/content-contrib-setup/content-dev-environment",
        "href": "content-writing/content-contrib-setup/content-dev-environment.html#install-pre-commit",
        "title": "Content development environment > Install pre-commit",
        "section": "Install pre-commit",
        "text": "pre-commit is a tool for ensuring that all the changes that you make to\nfiles in a project successfully pass all checks run by the code style tools that are\nconfigured as part of the CI/CD process. For more information on the code style tools most\ncommonly used in PyAnsys projects, see code_style_tools.\nTo run pre-commit locally, you must install it in your development environment:\nIf the Ansys Python Manager and Administrator window are not still\nopen, open them.\nFrom the Administrator window’s command prompt, run this command:\nThe window displays installation information.\nInstall pre-commit\npre-commit"
    },
    {
        "objectID": "content-writing/content-contrib-setup/content-dev-environment",
        "href": "content-writing/content-contrib-setup/content-dev-environment.html#install-vale",
        "title": "Content development environment > Install Vale",
        "section": "Install Vale",
        "text": "Vale is a tool for maintaining a consistent style and voice in your\ndocumentation based on a given style guide. When the Ansys templates\ntool is used to create a PyAnsys project from the pyansys or pyansys-advanced template,\nVale is one of the many documentation style tools that is configured to run as part of the\nCI/CD process. For more information, see doc_style_tools.\nTo run Vale locally, you must install it in your development environment:\nIf the Ansys Python Manager and Administrator window are not still\nopen, open them.\nFrom the Administrator window’s command prompt, run this command to\ninstall Vale:\nThe window displays installation information.\nTalk to the PyAnsys team about this approach. They said that this pip package is not\nofficial. I have, however, been using it locally for a month or so.\nInstall Vale\nTodo\npyansys\npyansys-advanced\npip"
    },
    {
        "objectID": "content-writing/content-contrib-setup/content-dev-environment",
        "href": "content-writing/content-contrib-setup/content-dev-environment.html#install-notepad",
        "title": "Content development environment > Install Notepad++",
        "section": "Install Notepad++",
        "text": "Links for using various GitHub search functions are available in Search on GitHub documentation in the GitHub documentation. However, to find occurrences of a\nparticular word or phrase in a project, using Notepad++ is often easier. This app’s\nFind in Files option provides for quickly searching any given directory for a search string.\nFor installation and search information, see Downloads on the Notepad++ website and\nSearching in the Notepad++ User Manual.\nNotepad++ is also handy if you want to open one file in it to visually compare it to\nanother file that you have open in Visual Studio Code.\nInstall Notepad++"
    },
    {
        "objectID": "content-writing/content-contrib-setup/content-dev-environment",
        "href": "content-writing/content-contrib-setup/content-dev-environment.html#install-sphinx-and-the-ansys-sphinx-theme",
        "title": "Content development environment > Install Sphinx and the Ansys Sphinx Theme",
        "section": "Install Sphinx and the Ansys Sphinx Theme",
        "text": "Sphinx, which uses reStructuredText as its default plaintext markup language, is\na tool for generating documentation. While designed primarily for generating documentation\nfor Python projects, it can be used for generating documentation for other programming languages\nand projects.\nThe Ansys Sphinx Theme is an Ansys-branded extension of the popular\nPyData Sphinx Theme. It is used along with Sphinx to assemble PyAnsys\ndocumentation from a project’s RST files and the docstrings in its PY files.\nInstall both Sphinx and the Ansys Sphinx Theme so that you can build PyAnsys documentation\nlocally:\nIf the Ansys Python Manager and Administrator window are not still\nopen, open them.\nTo install Sphinx in your virtual environment from PyPI (Python Package Index),\nfrom the Administrator window’s command prompt, run this command:\nThe window displays installation information.\nTo see the version of the installed Sphinx package, run this command:\nTo install the Ansys-branded theme, run this command:\nThe window displays installation information. The Sphinx conf.py file\nin the doc directory of a PyAnsys repository is already configured to use\nthe Ansys Sphinx Theme.\nYour development environment is now set up. If you are new to contributing to\nPyAnsys documentation, see essentials_writers. For lists of resources related\nto contributing to PyAnsys documentation, see resources_writers.\nYou can customize the Ansys Sphinx Theme by making changes to the\nhtml_theme_options dictionary in the Sphinx configuration (conf.py)\nfile in the doc directory. For additional information, see\nHTML theme options\nin the Ansys Sphinx Theme documentation.\nInstall Sphinx and the Ansys Sphinx Theme\nconf.py\ndoc\nhtml_theme_options\nconf.py\ndoc"
    },
    {
        "objectID": "content-writing/rst-files-writers/tables",
        "href": "content-writing/rst-files-writers/tables.html#tables",
        "title": "Tables",
        "section": "Tables",
        "text": "You can create four different types of tables in RST files:\nGrid tables: Create by drawing them.\nSimple tables: Create with the table directive.\nCSV tables: Create with the csv_table directive.\nList table: Create with the list-table directive.\nThe data in a table is always left-aligned. For all table types, you can\nuse reStructuredText syntax to format content and add links within the\ntable cells.\nAll tables created with directives support a title and optional attributes.\nFor more information, see Tables in\nthe section on reStructuredText directives in the Docutils\ndocumentation.\nTables\ntable\ncsv_table\nlist-table"
    },
    {
        "objectID": "content-writing/rst-files-writers/tables",
        "href": "content-writing/rst-files-writers/tables.html#create-a-grid-table",
        "title": "Tables > Create a grid table",
        "section": "Create a grid table",
        "text": "You create a grid table by using plus (+), hyphen (-), and pipe or\nvertical bar (|) characters to “draw” the table. You can also use the\nequal sign (=) character to bold the column header text.\nHere is a grid table with bold column header text:\nHere is how this grid table is rendered in the documentation:\nColumn A\nColumn B\nColumn C\nData 1\nData 2\nData 3\nData 4\nData 5\nData 6\nThe length of the hyphen characters in each column determines the column width.\nFor example, assume that you want a table with a wider first column and, in the\nsecond row, you want the data for the second column to span the third column.\nYou would add more hyphen characters in the corresponding header cell and, in the\nsecond row, remove the pipe character indicating the column break between the second\nand third columns:\nHere is how this grid table is rendered in the documentation:\nColumn A for wider first column\nColumn B\nColumn C\nData 1\nData 2\nData 3\nData 4\nData 5 with spanning text\nData 6\nData 7\nData 8\nCreate a grid table\n+\n-\n|\n="
    },
    {
        "objectID": "content-writing/rst-files-writers/tables",
        "href": "content-writing/rst-files-writers/tables.html#create-a-simple-table",
        "title": "Tables > Create a simple table",
        "section": "Create a simple table",
        "text": "You create a simple table using the table directive.\nHere is a table directive for a simple table, where\ncolumn widths are automatically adjusted to fill the page based\non their content:\nHere is how this simple table is rendered in the documentation:\nColumn A\nColumn B\nColumn C\nData 1\nData 2\nData 3\nData 4\nData 5\nData 6\nCreate a simple table\nTable title\ntable\ntable"
    },
    {
        "objectID": "content-writing/rst-files-writers/tables",
        "href": "content-writing/rst-files-writers/tables.html#create-a-csv-table",
        "title": "Tables > Create a CSV table",
        "section": "Create a CSV table",
        "text": "You create a CSV table using the csv_table directive. When using this\ndirective, you must be careful with the placement of commas (,).\nWhen defining the column names and data, you must place a comma immediately\nafter the closing quotation marks. You must also place only one space between\neach value. This directive provides no control over the merging of cells.\nHowever, it does provide the widths attribute for specifying the width of\neach column as a percentage.\nHere is a csv_table directive that creates a table with three columns\nand two rows, where the second and third columns are three times the width of the\nfirst column:\nHere is how this CSV table is rendered in the documentation:\nColumn A\nColumn B\nColumn C\nData 1\nData 2\nData 3\nData 4\nData 5\nData 6\nCreate a CSV table\nTable title\ncsv_table\n,\nwidths\ncsv_table"
    },
    {
        "objectID": "content-writing/rst-files-writers/tables",
        "href": "content-writing/rst-files-writers/tables.html#create-a-list-table",
        "title": "Tables > Create a list table",
        "section": "Create a list table",
        "text": "You create a list table using the list-table directive. When using this directive,\nyou provide the table data in a uniform two-level bulleted list, where uniform means\nthat each sublist (second-level list) must contain the same number of items. This directive\nuses asterisk (*) and hyphen (-) characters to define the table structure. When no\nvalue is specified for the widths attribute, auto is the default.\nThis example of the list-table directive uses the widths attribute to specify\nthe widths of columns as percentage values:\nHere is how this list table is rendered in the documentation:\nColumn A\nColumn B\nColumn C\nData 1\nData 2\nData 3\nData 4\nData 5\nData 6\nCreate a list table\nTable title\nlist-table\n*\n-\nwidths\nauto\nlist-table\nwidths"
    },
    {
        "objectID": "how-to/dns-configuration",
        "href": "how-to/dns-configuration.html#dns-configuration",
        "title": "DNS configuration",
        "section": "DNS configuration",
        "text": "As explained in documenting_developers, documentation for PyAnsys libraries is published\nonline following canonical name (CNAME) convention:\nhttps://<product>.docs.pyansys.com\nTo request a CNAME for the pyansys.com domain, contact the\nPyAnsy core team. They handle the creation of all PyAnsys subdomains.\nOnce the CNAME is created, repository administrators can configure their published\ndocumentation in GitHub pages to be exposed through it. To configure the CNAME\nfor your documentation, see Managing a custom domain for your GitHub Pages site\nin the GitHub documentation.\nDNS configuration\nhttps://<product>.docs.pyansys.com\npyansys.com"
    },
    {
        "objectID": "how-to/dns-configuration",
        "href": "how-to/dns-configuration.html#dns-txt-verification",
        "title": "DNS configuration > DNS TXT verification",
        "section": "DNS TXT verification",
        "text": "Once a CNAME is registered under the pyansys.com domain, the next step is\nto perform a DNS TXT verification. All PyAnsys subdomains are required by the Ansys\nIT department to provide a DNS TXT verification. To verify a new CNAME for an\norganization, see Verifying a domain for your organization site in the GitHub\ndocumentation. This article shows how to create DNS TXT verification elements\nfor GitHub Pages sites.\nOnly users with privilege access to the pyansys.com DNS zone can\nperform a DNS TXT verification. If assistance is needed, contact the\nPyAnsy core team.\nDNS TXT verification\npyansys.com\npyansys.com"
    },
    {
        "objectID": "how-to/dns-configuration",
        "href": "how-to/dns-configuration.html#pyansys-verified-domains",
        "title": "DNS configuration > PyAnsys-verified domains",
        "section": "PyAnsys-verified domains",
        "text": "In the Ansys GitHub organization, these domains have been verified:\npyansys.com\ndocs.pyansys.com\nOnly CNAME requests with one subdomain before the previous verified\ndomains are allowed. For more information, see DNS protection measures.\nPyAnsys-verified domains\npyansys.com\ndocs.pyansys.com"
    },
    {
        "objectID": "how-to/dns-configuration",
        "href": "how-to/dns-configuration.html#dns-protection-measures",
        "title": "DNS configuration > DNS protection measures",
        "section": "DNS protection measures",
        "text": "The rationale behind choosing the previous CNAME convention is related to cybersecurity.\nAs the Verifying a domain for your organization site article explains, GitHub provides for\nverifying domains for users and organizations.\nHaving a verified domain prevents users external to the organization from\ntaking over existing direct subdomains.\nHowever, GitHub does not verify deeper subdomains.\nThis is better explained with the following examples:\nDNS protection measures"
    },
    {
        "objectID": "how-to/dns-configuration",
        "href": "how-to/dns-configuration.html#scenario-for-a-protected-subdomain",
        "title": "DNS configuration > Scenario for a protected subdomain",
        "section": "Scenario for a protected subdomain",
        "text": "The docs.pyansys.com domain has been verified for the Ansys GitHub organization.\nThis CNAME is requested: subdomain.docs.pyansys.com.\nThis CNAME can only be used by repositories inside the Ansys GitHub organization.\nAny attempt by an external user to take over this CNAME is identified and rejected by GitHub.\nScenario for a protected subdomain\ndocs.pyansys.com\nsubdomain.docs.pyansys.com"
    },
    {
        "objectID": "how-to/dns-configuration",
        "href": "how-to/dns-configuration.html#scenario-for-a-vulnerable-subdomain",
        "title": "DNS configuration > Scenario for a vulnerable subdomain",
        "section": "Scenario for a vulnerable subdomain",
        "text": "The domain docs.pyansys.com has been verified for the Ansys GitHub organization.\nThis CNAME is requested: subsubdomain.subdomain.docs.pyansys.com.\nThis CNAME can be used by external users for their repositories. For this reason,\nyou must avoid creating CNAME requests that are not verified by the Ansys GitHub organization.\nScenario for a vulnerable subdomain\ndocs.pyansys.com\nsubsubdomain.subdomain.docs.pyansys.com"
    },
    {
        "objectID": "how-to/dns-configuration",
        "href": "how-to/dns-configuration.html#cname-takeover-prevention",
        "title": "DNS configuration > CNAME takeover prevention",
        "section": "CNAME takeover prevention",
        "text": "CNAME values have been taken over in the past by external users, typically due to\nthese reasons:\nThe Ansys GitHub organizations had no domain verification set up.\nA CNAME created did not follow the recommended CNAME guidelines.\nMore than one level of subdomain depth under the verified domain had been requested.\nLong time lapses occurred between CNAME creation and assignment to GitHub pages.\nThus, it is important that you follow these guidelines:\nEnsure that your GitHub organization has verified domains for hosting GitHub pages.\nCheck that the CNAME that you request does not have a subdomain depth larger than one with respect to the verified domains.\nRequest a CNAME only when needed, which is just prior to publishing the site.\nRequest deletion of the CNAME once it is no longer used to prevent others from hosting\ntheir sites on it.\nCNAME takeover prevention"
    },
    {
        "objectID": "how-to/index",
        "href": "how-to/index.html#how-to",
        "title": "How-to",
        "section": "How-to",
        "text": "This section describes how to create effective and efficient Python libraries\nfor interfacing with Ansys products and services. It also explains how apps\nand complex services expose functionalities such as logging, data transfer,\nand app APIs.\nHow to set up a development environment.\nHow to know which Python versions to support.\nHow to contribute to a PyAnsys library.\nHow to package a PyAnsys library.\nHow to integrate gRPC in PyAnsys packages.\nHow to write to PyAnsys library logs.\nHow to write and style your documentation.\nHow to perform code testing and check coverage.\nHow to use GitHub Actions for continuous integration.\nHow to release and publish a PyAnsys package.\nHow to handle sensitive information.\nHow to configure and protect your DNS.\nHow to handle compatibility issues.\nHow-to"
    },
    {
        "objectID": "how-to/continuous-integration",
        "href": "how-to/continuous-integration.html#continuous-integration",
        "title": "Continuous integration",
        "section": "Continuous integration",
        "text": "Continuous integration (CI) is the process of merging new changes into the main\ncode base while ensuring that these changes are functional and do not break the existing\ncode.\nThis process is automated as much as possible to alleviate the developer’s workload\nand ensure a quick development workflow.\nBecause PyAnsys projects are hosted in GitHub, the\nGitHub Actions framework is used.\nContinuous integration"
    },
    {
        "objectID": "how-to/continuous-integration",
        "href": "how-to/continuous-integration.html#enable-github-actions",
        "title": "Continuous integration > Enable GitHub actions",
        "section": "Enable GitHub actions",
        "text": "By default, Actions are enabled in new repositories and can be accessed\nusing the associated GitHub repository sections.\nIf Actions are not enabled, you can enable them. For more information, see\nManaging GitHub Actions permissions for your repository\nin the GitHub documentation.\nEnable GitHub actions\nActions\nActions"
    },
    {
        "objectID": "how-to/continuous-integration",
        "href": "how-to/continuous-integration.html#use-github-actions",
        "title": "Continuous integration > Use GitHub Actions",
        "section": "Use GitHub Actions",
        "text": "You must declare the GitHub Actions to be executed in the CI process in a\ncommon ci.yml file in the .github/workflows directory. Although each\naction is different, they all have a common structure:\nA name identifying the action.\nA collection of triggering events that run the action when required.\nA collection of concurrent workflows conditions to, for example, avoid running\nseveral workflows for the same branch. (Multiple consecutive pushes could lead to\nmultiple ongoing workflows when you want only the last push to run).\nA collection of jobs with different steps to follow during the CI process.\nUse GitHub Actions\nci.yml\n.github/workflows"
    },
    {
        "objectID": "how-to/continuous-integration",
        "href": "how-to/continuous-integration.html#disable-concurrent-workflows",
        "title": "Continuous integration > Disable concurrent workflows",
        "section": "Disable concurrent workflows",
        "text": "Handling hardware resources is a big deal, especially when running with self-hosted agents.\nIf you are using public GitHub hardware for running your workflows, disabling concurrent\nCI workflows is a way to show that you care about the environment and sustainability.\nFor example, imagine the following situation:\nYou push some changes to your branch.\nThe CI workflow kicks in and starts executing the different stages.\nYou suddenly realize that there is a typo or a file missing.\nYou push the new commit to your PR.\nA new CI workflow kicks in and starts running.\nAt this moment, you probably have two parallel workflows running at the same time,\nthough you are only interested in the results from the last one.\nOne way to solve this is manually cancelling the oldest workflow. However, it is also possible to\nautomatically cancel pre-existing workflows for a PR. To do so, prior to the\njobs section in the ci.yml file, add the following lines to your workflow:\nDisable concurrent workflows\njobs\nci.yml"
    },
    {
        "objectID": "how-to/continuous-integration",
        "href": "how-to/continuous-integration.html#required-workflows",
        "title": "Continuous integration > Required workflows",
        "section": "Required workflows",
        "text": "PyAnsys projects require workflows for performing these types of checks:\nCode style\nDocumentation style\nDocumentation building\nDocumentation deployment\nTesting\nTest code coverage\nrelease_publish\nYou should collect all workflows in a common ci.yml file. For more information,\nsee Workflow examples.\nRequired workflows\nci.yml"
    },
    {
        "objectID": "how-to/continuous-integration",
        "href": "how-to/continuous-integration.html#parametrize-workflows",
        "title": "Continuous integration > Parametrize workflows",
        "section": "Parametrize workflows",
        "text": "It is important to test a PyAnsys library on different operating systems\nusing different Python versions:\nThe most common operating systems are Windows, macOS, and Linux/UNIX. For supported\nPython versions, see Python versions.\nBecause having a YML file for each workflow would be tedious, GitHub\nActions provides the matrix parameter inside the strategy. For more\ninformation, see Using a matrix for your Jobs\nin the GitHub documentation\nConsider this example of a parametrized workflow:\nParametrize workflows\nmatrix\nstrategy"
    },
    {
        "objectID": "how-to/continuous-integration",
        "href": "how-to/continuous-integration.html#workflow-examples",
        "title": "Continuous integration > Workflow examples",
        "section": "Workflow examples",
        "text": "Workflow examples are provided for various checks, such as code style,\ntests, documentation style,\ndocumentation building, and releasing.\nWorkflow examples"
    },
    {
        "objectID": "content-writing/rst-files-writers/images",
        "href": "content-writing/rst-files-writers/images.html#images",
        "title": "Images",
        "section": "Images",
        "text": "You use the image or figure directive to add an image to\na RST file.\nBoth directives support these types of image files: PNG, JPG, and GIF.\nWhile these two directives are similar, the figure directive supports adding\na label and a caption for the image, where the label can be used for cross-referencing\nin your documentation.\nFor either directive, the setup is the same:\nPlace the image file in your library’s doc/source/_static directory.\nPlace the directive in your RST file where you want the image to appear,\nusing the relative path to this image file in the image or figure\ndirective.\nImages\nimage\nfigure\nfigure\ndoc/source/_static\nimage\nfigure"
    },
    {
        "objectID": "content-writing/rst-files-writers/images",
        "href": "content-writing/rst-files-writers/images.html#add-an-image-using-the-image-directive",
        "title": "Images > Add an image using the image directive",
        "section": "Add an image using the image directive",
        "text": "When you use the image directive to add an image, you only need to specify the path\nto the image file. However, specifying the optional alt attribute, which provides text for\nthe image, is strongly recommended:\nThis next image directive provides all optional attributes:\nHere are descriptions of all optional attributes for the image directive:\nwidth and height: Control the width and height of the image in pixels.\nYou can specify one of these attributes to maintain the aspect ratio, or specify\nboth attributes to set specific dimensions.\nalt: Provides text for the image. This attribute is used for accessibility\nand should provide a concise description of the image content. It is also displays\nif the image fails to load.\nalign: Controls the horizontal alignment of the image. Options are  left,\nright, and center. If omitted, the image is left-aligned.\nAdd an image using the image directive\nimage\nimage\nalt\nimage\nimage\nwidth\nheight\nalt\nalign\nleft\nright\ncenter"
    },
    {
        "objectID": "content-writing/rst-files-writers/images",
        "href": "content-writing/rst-files-writers/images.html#add-an-image-using-the-figure-directive",
        "title": "Images > Add an image using the figure directive",
        "section": "Add an image using the figure directive",
        "text": "When you use the figure directive to add an image, you can specify the  name\nattribute and a caption. The name attribute is a custom label that can\nbe used to cross-reference the image in your documentation. The caption provides\ncontext or explanations.\nHere is an example of a figure directive:\nTo reference this image elsewhere in your documentation, use the custom label defined\nfor the name attribute:\nThis creates a reference to the image where the name attribute is defined as custom-label.\nSphinx displays this image with its caption and specified label in the rendered documentation.\nAdd an image using the figure directive\nfigure\nfigure\nname\nname\nfigure\nname\nname\ncustom-label"
    },
    {
        "objectID": "coding-style/deprecation",
        "href": "coding-style/deprecation.html#deprecation-best-practices",
        "title": "Deprecation best practices",
        "section": "Deprecation best practices",
        "text": "While deprecation best practices are outlined in\nthis deprecation documentation,\nthere is no official guidance on deprecating features within Python.\nThus, this topic provides deprecation best practices for PyAnsys\nlibraries.\nWhenever you deprecate a method, class, or function, you must take one of\nthese actions:\nHave the old method call the new method and raise a warning.\nRaise an AttributeError if you remove the method entirely.\nIn the docstring of the old method, use a Sphinx deprecated directive\nthat links to the new method. This way, you notify your users when you make\nan API change and give them a chance to change their code. Otherwise,\nusers stop upgrading, or worse, stop using your API, due to stability concerns.\nFor this reason, it is best to use a warning first and then use an error after\na minor release or two.\nIf you remove a method entirely, there is no reason to provide a link to the old\nmethod. Simply raise an AttributeError as part of the class or raise an Exception.\nBecause there is no built-in deprecation error within Python, an alternate\napproach is to create a custom DeprecationError.\nYou then use this custom DeprecationError in place of an Exception.\nDeprecation best practices\nAttributeError\nAttributeError\nException\nDeprecationError\nDeprecationError\nException"
    },
    {
        "objectID": "content-writing/content-contrib-setup/doc-resources",
        "href": "content-writing/content-contrib-setup/doc-resources.html#pyansys-documentation-resources",
        "title": "PyAnsys documentation resources",
        "section": "PyAnsys documentation resources",
        "text": "This page lists resources relevant to contributing to PyAnsys documentation.\nWhile other pages reference resources as needed, this page provides a central\nlocation from which you can easily view all referenced resources.\nPyAnsys documentation resources"
    },
    {
        "objectID": "content-writing/content-contrib-setup/doc-resources",
        "href": "content-writing/content-contrib-setup/doc-resources.html#style-and-formatting-resources",
        "title": "PyAnsys documentation resources > Style and formatting resources",
        "section": "Style and formatting resources",
        "text": "The following alphabetized list of resources provide guidelines for writing PyAnsys\ndocumentation. These resources provide you with an understanding of how to write\nand format PyAnsys documentation content correctly in various types of source files.\nGitHub Flavored Markdown Spec: Describes how GitHub\nFlavored Markdown is a dialect of Markdown supported for user content on GitHub.com and\nGitHub Enterprise. Generally, all manually written source files for PyAnsys documentation\nare RST files. However, some projects use a GitHub Flavored Markdown (MD) file for the\nproject’s README file. For more information, see readme_files.\nGoogle developer documentation style guide: Provides\nguidance on the writing style used for PyAnsys documentation.\nMaterial for Sphinx: Provides guidance on authoring in\nreStructuredText when using Sphinx to generate documentation and includes a\nreStructuredText cheat sheet.\nnumpydoc Style guide: Provides the syntax and best practices for\nwriting docstrings in PY files when the numpydoc extension for Sphinx is used. Docstrings\ndescribe Python modules, functions, classes, and methods. This guide’s overview also provides\nlinks to standard Python code conventions, additional PEPs (Python Enhancement Proposals) related\nto the documentation of code, and common code style checkers.\nPEP 8 - Style Guide for Python Code: Provides guidance on the Python coding\nstyle that PyAnsys libraries use.\nPython documentation: Provides the official documentation published by the\nPython organization for Python for developers at all levels.\nQuick reStructuredText: Provides a summary of reStructuredText\nsyntax with links to the reStructuredText Markup Specification\nfor more comprehensive information.\nreStructuredText Markup Specification: Provides a quick reStructuredText\nsyntax overview, which explains that the parser is a component of Docutils,\nan open source text-processing system for plaintext documentation. It then provides the complete\ntechnical specification.\nreStructuredText Markup Syntax and Parser Component of Docutils:\nProvides a description of reStructuredText and links to user, reference, and developer\ndocumentation, including a reStructuredText cheat sheet.\nreStructuredText Primer: Provides introductory documentation on reStructuredText\nconcepts and syntax as part of the documentation.\nStyle guide for Sphinx-based documentation: Provides coding\nstandards for Sphinx-based documentation.\nUsing Markdown and Liquid in GitHub Docs: Provides an\nexplanation of GitHub Flavored Markdown and explains how GitHub docs use Liquid\nsyntax to expand the functionality to provide accessible tables, maintainable links, versioning,\nvariables, and chunks of reusable content.\nStyle and formatting resources\nnumpydoc"
    },
    {
        "objectID": "content-writing/content-contrib-setup/doc-resources",
        "href": "content-writing/content-contrib-setup/doc-resources.html#tool-resources",
        "title": "PyAnsys documentation resources > Tool resources",
        "section": "Tool resources",
        "text": "The following alphabetized list of resources are for tools used in the writing and\nbuilding of PyAnsys documentation.\nAnsys Python Manager: Provides a cross-platform\nQT app for installing Python and managing your Python environment, including creating\nvirtual environments, launching a command console, installing PyAnsys packages, and\nviewing PyAnsys documentation.\nAnsys Sphinx Theme: Provides an Ansys-branded extension\nof the popular PyData Sphinx Theme that is used to generate\nPyAnsys documentation.\nAnsys templates: Provides templates for creating PyAnsys projects\nthat are compliant with PyAnsys guidelines.\nGit: Provides a distributed version control system for tracking changes\nin source code during software development.\nGit Extensions: Provides a GUI client for Git.\nGitHub:  Provides a web-based platform that uses Git as its underlying\nversion control system but goes way beyond version control, offering a\ncollaborative environment for hosting, managing, and sharing Git repositories.\nGitHub Desktop: Provides a GUI client for Git.\nNotepad++: Provides an open source code text and code editor for use\non Microsoft Windows, supporting around 80 programming languages with syntax\nhighlighting and code folding.\npip: Provides a package manager for installing Python packages from the\nPython Package Index (PyPI).\npre-commit: Provides for checking the conformance of your code\nagainst predefined code style conventions.\nPython: Provides a general-purpose programming language that runs on\nalmost all system architectures and is used for a wide range of applications\nin different fields.\nPython in Visual Studio Code: Provides an extension\nthat makes Visual Studio Code an excellent Python editor.\nSphinx: Provides a Python documentation generator for generating documentation\nfrom RST, MD, and PY files.\nVale: Provides for checking RST and MD files for consistent\nstyle and voice.\nVisual Studio Code: Provides a lightweight but powerful source\ncode editor that runs on your desktop and is available for WIndows , macOS, and Linux.\nFor information on documentation style tools that might be implemented in\na PyAnsys project but are not necessarily described on this page, see\ndoc_style_tools.\nTool resources"
    },
    {
        "objectID": "content-writing/content-how-tos/review-PR",
        "href": "content-writing/content-how-tos/review-PR.html#review-a-pr",
        "title": "Review a PR",
        "section": "Review a PR",
        "text": "This page describes how to review a PR created by someone else. The\nnext page describes how to create your own PR.\nWhen someone wants you to review a PR, this person generally tags you as a\nreviewer on the PR. In the GitHub notification you receive, you can click the\nlink to go to the PR.\nThe simplest way to learn how to contribute to a project is to begin reviewing\nPRs that other contributors have created. You can add suggestions in their PRs\nfor improving content, thereby build the confidence that you need to create your\nown PRs.\nBeing an effective reviewer assumes that you are familiar with all the\nresources listed in resources_writers.\nReview a PR"
    },
    {
        "objectID": "content-writing/content-how-tos/review-PR",
        "href": "content-writing/content-how-tos/review-PR.html#add-a-comment",
        "title": "Review a PR > Add a comment",
        "section": "Add a comment",
        "text": "On the Conversation page of the PR, you can add a general comment on the\noverall PR by scrolling to the bottom of the page, entering your comment in\nthe Write tab, and clicking Comment.\nOn the Files changed page of the PR, you can add a general comment on\na single changed line or multiple consecutive changed lines in a file. You can\nalso add a comment that suggests specific changes to a single line or multiple\nconsecutive lines.\nIf the PR includes changes to multiple files, the Files changed page has\ntwo panes. The left pane displays a list of all changed files, and the right pane\ndisplays the files themselves, with changes highlighted in green. To quickly\nsee the changes made to a particular file, click its entry in the left pane.\nAdd a comment"
    },
    {
        "objectID": "content-writing/content-how-tos/review-PR",
        "href": "content-writing/content-how-tos/review-PR.html#add-a-general-comment-on-a-line",
        "title": "Review a PR > Add a general comment on a line",
        "section": "Add a general comment on a line",
        "text": "You can add a general comment on a line:\nClick the plus sign to the right of the line number.\nIn the window that opens, type your comment.\nClick Add single comment to add your comment and close the window.\nAdd a general comment on a line"
    },
    {
        "objectID": "content-writing/content-how-tos/review-PR",
        "href": "content-writing/content-how-tos/review-PR.html#add-a-comment-suggesting-a-change-to-a-single-line",
        "title": "Review a PR > Add a comment suggesting a change to a single line",
        "section": "Add a comment suggesting a change to a single line",
        "text": "You can add a comment suggesting a change to a single line:\nClick the plus sign to the right of the line number.\nIn the window that opens, click the Add a suggestion icon:\nGitHub copies the content of line, placing it in a suggestion tag in the\nwindow’s Write tab. The suggestion tag begins and ends with triple\nbackticks (```) like this:\nWithin the suggestion tag, edit the content to reflect your suggested\nchanges.\nIf you want to provide any information about your suggested change, type it\ndirectly under the three backticks that close the suggestion tag:\nClick Add single comment to add your comment and close the window.\nAdd a comment suggesting a change to a single line\nsuggestion\n```\nsuggestion\nsuggestion"
    },
    {
        "objectID": "content-writing/content-how-tos/review-PR",
        "href": "content-writing/content-how-tos/review-PR.html#add-a-comment-suggesting-changes-to-multiple-lines",
        "title": "Review a PR > Add a comment suggesting changes to multiple lines",
        "section": "Add a comment suggesting changes to multiple lines",
        "text": "You can add a comment suggesting changes to multiple consecutive lines:\nClick the plus sign to the right of the first line number that you want to comment on\nand then drag the mouse to the last line number you want to comment on before\nreleasing the mouse button.\nThe selected lines are highlighted in yellow.\nIn the window that opens, click the Add a suggestion icon.\nGitHub places the content of the selected lines within a suggestion tag in\nthe window’s Write tab.\nWithin the suggestion tag, edit the content to reflect your suggested\nchanges.\nIf you want to provide a comment about your suggested changes, type it\ndirectly under the three backticks that close the suggestion tag.\nClick Add single comment to add your comment and close the window.\nAdd a comment suggesting changes to multiple lines\nsuggestion\nsuggestion\nsuggestion"
    },
    {
        "objectID": "content-writing/content-how-tos/review-PR",
        "href": "content-writing/content-how-tos/review-PR.html#submit-your-review",
        "title": "Review a PR > Submit your review",
        "section": "Submit your review",
        "text": "When you finish reviewing the PR, submit your review:\nIn the upper right corner of the GitHub window, click Review changes .\nThe Finish your review window opens.\nLeave a comment about your review.\nChoose the Comment, Approve, or Request changes option.\nClick Submit review.\nSubmit your review"
    },
    {
        "objectID": "content-writing/content-how-tos/index",
        "href": "content-writing/content-how-tos/index.html#content-how-tos",
        "title": "Content how-tos",
        "section": "Content how-tos",
        "text": "This section describes how to perform tasks related to contributing\nto PyAnsys documentation. As indicated in earlier sections, PRs suggest\nchanges to the main codebase. Each PR undergoes a thorough review before\nit can be approved and subsequently merged into the codebase.\nContent contributors typically contribute to the codebase for a PyAnsys\nlibrary in one of two ways:\nBy reviewing PRs created by fellow contributors\nBy creating PRs that add to or improve the documentation\nAdditionally, content contributors can create issues on the Issues\npage of a library’s GitHub repository. If the repository has a Discussions\npage, they can use it to start a discussion on this library. Otherwise, you\ncan post questions, share ideas, and get community feedback on the\nDiscussions on the Ansys Developer portal.\nContent how-tos"
    },
    {
        "objectID": "doc-style/index",
        "href": "doc-style/index.html#documentation-style",
        "title": "Documentation style",
        "section": "Documentation style",
        "text": "Good API documentation drives library adoption and usage and is the\nfoundation for a good developer experience. Even with the best\ninterfaces and the most functional product, an API is not adopted\nif users aren’t satisfied with a library’s documentation or examples.\nGood API documentation provides many benefits, including these:\nIncreased adoption and improved experiences for developers using\nthe API or library.\nReduction in the time spent on-boarding new contributors and users\nof the library.\nBetter maintenance of the code base. The time spent reading the\nsource is often orders of magnitude greater than the time spent\nwriting it. Well documented code (both in user-facing docstrings\nand developer comments) pays dividends in code maintenance.\nReduction in the amount of time spent understanding the API features.\nThe documentation for a PyAnsys library should contain:\nModule, function, class, and method docstrings. See\ndocstrings.\nA full gallery of examples. See PyMAPDL examples.\nGeneral content on installing, using, and contributing.\nLink to the library’s documentation from the repository’s README file.\nTo ensure clear and consistent documentation, all PyAnsys libraries are to\nfollow the guidelines in the Google developer documentation style guide. Key guidelines include using:\nSentence case for headings and titles\nActive voice\nPresent tense\nShort, clear sentences\nTo help you follow the Google guidelines and any custom rules\ndeveloped by Ansys, you can implement Vale.\nThis command-line tool brings code-like linting to prose.\nFinally, the documentation should be public and hosted using GitHub pages, either as\na branch named gh-pages within the library repository or within a\ngh-pages branch within <library-repository>-docs.\nFor guidelines on how to acquire a specific DNS for hosting your documentation,\nsee DNS configuration. You should ensure that you are compliant with\nthe naming convention for your CNAME.\nFor procedural information related to crafting, building, and deploying\ndocumentation, see documenting_developers. For comprehensive information\non writing content for PyAnsys developers, see content_writing.\nDocumentation style\ngh-pages\ngh-pages\n<library-repository>-docs"
    },
    {
        "objectID": "examples/sg_execution_times",
        "href": "examples/sg_execution_times.html#computation-times",
        "title": "Computation times",
        "section": "Computation times",
        "text": "00:01.596 total execution time for 1 file from examples:\nExample\nTime\nMem (MB)\nsphx_glr_examples_pyvista_example.py (pyvista_example.py)\n00:01.596\n0.0\nComputation times\npyvista_example.py"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#environment-setup",
        "title": "Environment setup",
        "section": "Environment setup",
        "text": "Before you can contribute to any PyAnsys project, you must set up\nyour developer environment.\nEnvironment setup"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#python",
        "title": "Environment setup > Python",
        "section": "Python",
        "text": "All PyAnsys projects require a Python interpreter for interacting\nwith PyAnsys libraries. Therefore, you must ensure that at least one Python\ninterpreter is installed on your local machine.\nPython"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#install-python",
        "title": "Environment setup > Install Python",
        "section": "Install Python",
        "text": "There are several ways to install a Python package on your local machine:\nUse an installer from the official Python download page.\nUse a package manager or “store” on your machine.\nEnsure that you install Python from an official channel. Do not trust\nthird-party websites or download executable content from them.\nTo install Python on a machine running Windows:\nDownload the latest stable Python version for Windows.\nExecute the installer, referring to  Using Python on\nWindows for\ndetailed installation instructions.\nTo install Python on a machine running macOS:\nDownload the latest stable Python version for macOS.\nExecute the installer, referring to Using Python on\na Mac for\ndetailed installation instructions.\nIt is likely that your macOS distribution already comes with some\nversion of Python installed. For more information, see Verify Python version.\nTo install Python on a machine running Linux/UNIX:\nDownload the latest stable Python version for Linux/UNIX.\nDecompress the source code and follow the installation instructions in the\nREADME.rst file. For more information, see Using Python on Unix Platforms.\nIt is likely that your Linux/UNIX distribution already comes with some\nversion of Python installed. For more information, see Verify Python version.\nInstall Python\nREADME.rst"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#verify-python-version",
        "title": "Environment setup > Verify Python version",
        "section": "Verify Python version",
        "text": "Once your Python installation is complete, verify it with this command:\nVerify Python version"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#virtual-environments",
        "title": "Environment setup > Virtual environments",
        "section": "Virtual environments",
        "text": "When working in multiple Python projects, it is likely each of these projects has its\nown requirements. Sometimes, requirements across projects can be incompatible.\nVirtual environments were devised to isolate Python environments, which guarantees\nthat you do not face dependency problems when working in multiple projects.\nFor information on the most fundamental commands for manipulating and\ninteracting with a Python virtual environment, see the official Python documentation on\nthe venv module.\nVirtual environments"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#check",
        "title": "Environment setup > Check",
        "section": "Check",
        "text": "Before creating a virtual environment, you must run the command for your OS to see if you are already\nusing one:\nThe command returns the path to the Python virtual environment that your system is currently using.\nEnsure that it points to your default installation and not to a virtual\nenvironment. If it points to a virtual environment, see Deactivate for\ninformation on deactivating the virtual environment.\nCheck"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#create",
        "title": "Environment setup > Create",
        "section": "Create",
        "text": "Usually, virtual environments are named venv or .venv.\nYou can create a virtual environment named <venv> with this command:\nCreate\nvenv\n.venv\n<venv>"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#activate",
        "title": "Environment setup > Activate",
        "section": "Activate",
        "text": "You would activate the preceding virtual environment with the command for your OS:\nActivate"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#deactivate",
        "title": "Environment setup > Deactivate",
        "section": "Deactivate",
        "text": "You can deactivate a virtual environment with the command for your OS:\nDeactivate"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#git",
        "title": "Environment setup > Git",
        "section": "Git",
        "text": "Git is an open source VCS (version control system). It\nis used to track changes and register new content in software-related projects. Git\nregisters the author and date of the changes so that accurate tracking of the\nsoftware’s evolution is available.\nGit"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#install-git",
        "title": "Environment setup > Install Git",
        "section": "Install Git",
        "text": "To install Git on a machine running Windows:\nDownload the latest stable standalone Git version for Windows.\nExecute the installer and follow the installation instructions.\nTo install Git on a machine running macOS:\nCheck the latest stable Git version for macOS.\nRun the installation command for your package manager.\nTo install Git on a machine running Linux/UNIX:\nCheck the latest stable Git version for Linux/UNIX.\nRun the installation command for your package manager.\nInstall Git"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#verify-git-version",
        "title": "Environment setup > Verify Git version",
        "section": "Verify Git version",
        "text": "Once your Git installation finishes, verify it with this command:\nVerify Git version"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#use-git",
        "title": "Environment setup > Use Git",
        "section": "Use Git",
        "text": "If you’re new to Git, see the Git documentation\nfor comprehensive usage information.\nFor an understanding of Git workflows and branching strategies,\nsee the Learn Git Branching tutorial.\nIf you’re unfamiliar with GitHub, see\nThe Official GitHub Training Manual\nfor guidance.\nUse Git"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#configure-git",
        "title": "Environment setup > Configure Git",
        "section": "Configure Git",
        "text": "It is very important to properly configure Git so that every modification that you make\nto the code points to you. There are two types of Git configuration:\nGlobal and Local. It is also possible to combine both to have\na Dynamic configuration.\nConfigure Git"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#global",
        "title": "Environment setup > Global",
        "section": "Global",
        "text": "A global configuration is automatically included in every Git repository on\nyour machine unless overridden by a Local configuration, which\nis located in C:\\Users\\<username>\\.gitconfig for Windows users or in\n/home/<username>/.gitconfig for macOS, Linux, or UNIX users.\nYou can set the value for any variable in a field with this command:\nSome examples of setting values follow.\nSet your name\nSet your email\nSet the default branch name\nGlobal\nC:\\Users\\<username>\\.gitconfig\n/home/<username>/.gitconfig"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#local",
        "title": "Environment setup > Local",
        "section": "Local",
        "text": "There might be a time when you want to declare a specific configuration to use only\nin a given project. To override the global configuration, you can declare a local\nconfiguration.\nIn a local configuration, the commands are the same as in the global configuration. The\none exception is that instead of using the --global flag, you use the --local flag.\nEnsure that you run the commands in the root directory of your project and that a .git\ndirectory exists.\nIf you would like to manually modify your local configuration, it is saved in\nthe .git/config file.\nLocal\n--global\n--local\n.git\n.git/config"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#dynamic",
        "title": "Environment setup > Dynamic",
        "section": "Dynamic",
        "text": "It is possible to configure Git such that it selects between multiple\nconfiguration profiles according to whether your project is located on your system.\nThis lets you define common configurations for working under\nAnsys or other open source projects from which Ansys benefits.\nAs an example, consider the following scenario for setting up two Git\nconfiguration profiles for working with Ansys projects and personal projects.\nCreate the two files, naming them so that they are easily distinguishable. For\nexample, name them .gitconfig-ansys and .gitconfig-personal. Then, use Git\nConditional includes\nto control which Git configuration is applied based on whether the project is located\non your system.\nHere are examples of what these files might look like:\nDynamic\nAnsys\n.gitconfig-ansys\n.gitconfig-personal"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#sign-commits",
        "title": "Environment setup > Sign commits",
        "section": "Sign commits",
        "text": "To verify which code changes were made by you, signing the commit\nis required. To sign a commit, you must generate a GPG key, associate it with\nGitHub, and specify it in your Git configuration.\nFor an explanation of the process, see Manage commit signature verification\nin the GitHub documentation.\nSign commits\nGPG"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#enable-ssh",
        "title": "Environment setup > Enable SSH",
        "section": "Enable SSH",
        "text": "Working with Secure Shell Protocol (SSH) is not only a good practice but\nalso required for contributing to PyAnsys projects. Without an SSH key,\nyou are not able to clone internal or private repositories or\nto push new changes.\nFor information on setting up SSH with GitHub, see Connecting to GitHub with SSH\nin the GitHub documentation.\nEnable SSH"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#handle-line-endings",
        "title": "Environment setup > Handle line endings",
        "section": "Handle line endings",
        "text": "Every time you introduce a new line by pressing the enter key, an invisible\ncharacter is introduced to represent a line ending. Each operating system manages\nthese end-of-line (EOL) characters in its own way. For Windows, the EOL is\nalso known as a CRLF, while in Linux it is known as a LF.\nTo avoid problems between developers working in the same repository but using\ndifferent operating systems, you can specify an EOL policy in a .gitattributes file.\nIn a .gitattributes file that you have committed to your repository, you can\ncustomize the type of EOL characters that you expect developers to use. Git\nthen automatically manages these EOL characters so that developers do not\nneed to worry about them. Consider this example in Configuring Git to handle line endings\nin the GitHub documentations:\nHandle line endings\n.gitattributes\n.gitattributes"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#remove-files-and-directories-untracked-by-git",
        "title": "Environment setup > Remove files and directories untracked by Git",
        "section": "Remove files and directories untracked by Git",
        "text": "To remove files and directories that are not tracked by Git from your working directory,\nyou want to periodically run this command:\ngit clean -fdx .\nDescriptions follow for each command option:\nf forces deletion of untracked files (and directories if d is also specified)\nwithout requiring additional confirmation.\nd deletes untracked directories. By default, the git clean\ncommand does not recurse into untracked directories to avoid deleting too much.\nx deletes ignored files, which are those specified in your .gitignore file.\nYou use this option when you want to clean up all untracked files, including build\nproducts.\nThe trailing . specifies the current directory as the starting point for the\ncleaning. For example, to clean all of the untracked files that are generated by\nbuilding documentation locally, you would run the git clean -fdx . command\nfrom the doc directory.\nRemove files and directories untracked by Git\ngit clean -fdx .\nf\nd\nd\ngit clean\nx\n.gitignore\n.\ngit clean -fdx .\ndoc"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#wsl2",
        "title": "Environment setup > WSL2",
        "section": "WSL2",
        "text": "Some developers prefer using Windows as the operating system for their machines.\nHowever, they might like to take advantage of some features provided by a Linux\noperating system. The Windows Subsystem for Linux (WSL) was devised to solve\nthis problem. For installation information, see How to install Linux on Windows with WSL in the Microsoft Windows\ndocumentation.\nWSL2"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#install-wsl2",
        "title": "Environment setup > Install WSL2",
        "section": "Install WSL2",
        "text": "Open a new PowerShell session and install WSL with this command:\nAfter installing WSL, ensure that you are running the WSL2 version with this\ncommand:\nInstall WSL2"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#verify-wsl-version",
        "title": "Environment setup > Verify WSL version",
        "section": "Verify WSL version",
        "text": "Verify your WSL version with this command:\nVerify WSL version"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#install-linux-distribution",
        "title": "Environment setup > Install Linux distribution",
        "section": "Install Linux distribution",
        "text": "After WSL2 is installed, install a Linux distribution.\nTo get a list of available distributions, run this command:\nMost developers choose Ubuntu because it is a\nwell maintained Linux distribution with a huge collection of packages.\nTo install the Linux distribution of your choice, run this command:\nYou can use the preceding command to install multiple Linux distributions. Indicate\nthe distributions that you would like to use with WSL2 with this command:\nInstall Linux distribution"
    },
    {
        "objectID": "how-to/setting-up",
        "href": "how-to/setting-up.html#windows-terminal",
        "title": "Environment setup > Windows terminal",
        "section": "Windows terminal",
        "text": "Windows Terminal is\nan app that integrates multiple shells into a single console. Windows\nships by default with two shells: CMD and PowerShell. If WSL2 is\ninstalled, a Linux shell is added. Hence, the goal of Windows Terminal\nis to collect and manage all shell sessions in a single program. You can install\nWindows Terminal from the Windows Terminal page\non the Microsoft Store.\nWindows terminal\nCMD\nPowerShell"
    }
]